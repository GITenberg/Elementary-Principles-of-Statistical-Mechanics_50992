% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %
%                                                                         %
% The Project Gutenberg EBook of Elementary Principles of Statistical     %
% Mechanics, by Josiah Willard Gibbs                                      %
%                                                                         %
% This eBook is for the use of anyone anywhere in the United States and most
% other parts of the world at no cost and with almost no restrictions     %
% whatsoever.  You may copy it, give it away or re-use it under the terms of
% the Project Gutenberg License included with this eBook or online at     %
% www.gutenberg.org.  If you are not located in the United States, you'll have
% to check the laws of the country where you are located before using this ebook.
%                                                                         %
% Title: Elementary Principles of Statistical Mechanics                   %
%                                                                         %
% Author: Josiah Willard Gibbs                                            %
%                                                                         %
% Release Date: January 22, 2016 [EBook #50992]                           %
%                                                                         %
% Language: English                                                       %
%                                                                         %
% Character set encoding: ISO-8859-1                                      %
%                                                                         %
% *** START OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPLES STATISTICAL MECHANICS ***
%                                                                         %
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %

\def\ebook{50992}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                  %%
%% Packages and substitutions:                                      %%
%%                                                                  %%
%% book:     Required.                                              %%
%% inputenc: Latin-1 text encoding. Required.                       %%
%%                                                                  %%
%% ifthen:   Logical conditionals. Required.                        %%
%%                                                                  %%
%% amsmath:  AMS mathematics enhancements. Required.                %%
%% amssymb:  Additional mathematical symbols. Required.             %%
%%                                                                  %%
%% alltt:    Fixed-width font environment. Required.                %%
%%                                                                  %%
%% footmisc: Start footnote numbering on each page. Required.       %%
%%                                                                  %%
%% calc:     Length calculations. Required.                         %%
%%                                                                  %%
%% fancyhdr: Enhanced running headers and footers. Required.        %%
%%                                                                  %%
%% geometry: Enhanced page layout package. Required.                %%
%% hyperref: Hypertext embellishments for pdf output. Required.     %%
%%                                                                  %%
%%                                                                  %%
%% Producer's Comments:                                             %%
%%                                                                  %%
%%   OCR text for this ebook was obtained on Dec 28, 2015, from     %%
%%   https://www.archive.org/details/elementaryprinci00gibbrich.    %%
%%                                                                  %%
%%   Minor changes to the original are noted in this file in three  %%
%%   ways:                                                          %%
%%     1. \Typo{}{} for typographical corrections, showing original %%
%%        and replacement text side-by-side.                        %%
%%     2. \Chg{}{} and \Add{}, for inconsistent/missing punctuation,%%
%%        italicization, and capitalization.                        %%
%%     3. [** TN: Note]s for lengthier or stylistic comments.       %%
%%                                                                  %%
%%                                                                  %%
%% Compilation Flags:                                               %%
%%                                                                  %%
%%   The following behavior may be controlled by boolean flags.     %%
%%                                                                  %%
%%   ForPrinting (false by default):                                %%
%%   If false, compile a screen optimized file (one-sided layout,   %%
%%   blue hyperlinks). If true, print-optimized PDF file: Larger    %%
%%   text block, two-sided layout, black hyperlinks.                %%
%%                                                                  %%
%%                                                                  %%
%% PDF pages: 273 (if ForPrinting set to false)                     %%
%% PDF page size: 4.75 x 6.125" (non-standard)                      %%
%%                                                                  %%
%% Summary of log file:                                             %%
%% * Four overfull hboxes (harmless).                               %%
%%                                                                  %%
%% Compile History:                                                 %%
%%                                                                  %%
%% January, 2016: (Andrew D. Hwang)                                 %%
%%             texlive2012, GNU/Linux                               %%
%%                                                                  %%
%% Command block:                                                   %%
%%                                                                  %%
%%     pdflatex x2                                                  %%
%%                                                                  %%
%%                                                                  %%
%% January 2016: pglatex.                                           %%
%%   Compile this project with:                                     %%
%%   pdflatex 50992-t.tex ..... TWO times                           %%
%%                                                                  %%
%%   pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013/Debian)   %%
%%                                                                  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{book}[2005/09/16]
\listfiles

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[latin1]{inputenc}[2006/05/05]

\usepackage{ifthen}[2001/05/26]  %% Logical conditionals

\usepackage{amsmath}[2000/07/18] %% Displayed equations
\usepackage{amssymb}[2002/01/22] %% and additional symbols

\usepackage{alltt}[1997/06/16]   %% boilerplate, credits, license

\usepackage[perpage,symbol]{footmisc}[2005/03/17]

\usepackage{calc}[2005/08/06]

\usepackage{fancyhdr} %% For running heads

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Interlude:  Set up PRINTING (default) or SCREEN VIEWING %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ForPrinting=true                     false (default)
% Asymmetric margins                   Symmetric margins
% 1 : 1.6 text block aspect ratio      3 : 4 text block aspect ratio
% Black hyperlinks                     Blue hyperlinks
% Start major marker pages recto       No blank verso pages
%
\newboolean{ForPrinting}

%% UNCOMMENT the next line for a PRINT-OPTIMIZED VERSION of the text %%
%\setboolean{ForPrinting}{true}

%% Initialize values to ForPrinting=false
\newcommand{\Margins}{hmarginratio=1:1}     % Symmetric margins
\newcommand{\HLinkColor}{blue}              % Hyperlink color
\newcommand{\PDFPageLayout}{SinglePage}
\newcommand{\TransNote}{Transcriber's Note}
\newcommand{\TransNoteCommon}{%
  The camera-quality files for this public-domain ebook may be
  downloaded \textit{gratis} at
  \begin{center}
    \texttt{www.gutenberg.org/ebooks/\ebook}.
  \end{center}

  This ebook was produced using scanned images and OCR text generously
  provided by the University of California, Berkeley, through the
  Internet Archive.
  \bigskip

  Minor typographical corrections and presentational changes have been
  made without comment.
  \bigskip
}

\newcommand{\TransNoteText}{%
  \TransNoteCommon

  This PDF file is optimized for screen viewing, but may be recompiled
  for printing. Please consult the preamble of the \LaTeX\ source file
  for instructions and other particulars.
}
%% Re-set if ForPrinting=true
\ifthenelse{\boolean{ForPrinting}}{%
  \renewcommand{\Margins}{hmarginratio=2:3} % Asymmetric margins
  \renewcommand{\HLinkColor}{black}         % Hyperlink color
  \renewcommand{\PDFPageLayout}{TwoPageRight}
  \renewcommand{\TransNote}{Transcriber's Note}
  \renewcommand{\TransNoteText}{%
    \TransNoteCommon

    This PDF file is optimized for printing, but may be recompiled for
    screen viewing. Please consult the preamble of the \LaTeX\ source
    file for instructions and other particulars.
  }
}{% If ForPrinting=false, don't skip to recto
  \renewcommand{\cleardoublepage}{\clearpage}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%  End of PRINTING/SCREEN VIEWING code; back to packages  %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifthenelse{\boolean{ForPrinting}}{%
  \setlength{\paperwidth}{8.5in}%
  \setlength{\paperheight}{11in}%
% 1:1.6
  \usepackage[body={4.75in,9in},\Margins]{geometry}[2002/07/08]
}{%
  \setlength{\paperwidth}{5in}%
  \setlength{\paperheight}{7in}%
  \raggedbottom
% 3:4
  \usepackage[body={4.75in,6.125in},\Margins,includeheadfoot]{geometry}[2002/07/08]
}

\providecommand{\ebook}{00000}    % Overridden during white-washing
\usepackage[pdftex,
  hyperfootnotes=false,
  pdftitle={The Project Gutenberg eBook \#\ebook: Elementray Principles in Statistical Mechanics.},
  pdfauthor={Josiah Willard Gibbs},
  pdfkeywords={University of California Berkeley, The Internet Archive, Andrew D. Hwang},
  pdfstartview=Fit,    % default value
  pdfstartpage=1,      % default value
  pdfpagemode=UseNone, % default value
  bookmarks=true,      % default value
  linktocpage=false,   % default value
  pdfpagelayout=\PDFPageLayout,
  pdfdisplaydoctitle,
  pdfpagelabels=true,
  bookmarksopen=true,
  bookmarksopenlevel=0,
  colorlinks=true,
  linkcolor=\HLinkColor]{hyperref}[2007/02/07]


%% Fixed-width environment to format PG boilerplate %%
\newenvironment{PGtext}{%
\begin{alltt}
\fontsize{8.1}{10}\ttfamily\selectfont}%
{\end{alltt}}

% Errors found during digitization
\newcommand{\Typo}[2]{#2}

% Stylistic changes made for consistency
\newcommand{\Chg}[2]{#2}
%\newcommand{\Chg}[2]{#1} % Use this to revert inconsistencies in the original
\newcommand{\Add}[1]{\Chg{}{#1}}

%% Miscellaneous global parameters %%
% No hrule in page header
\renewcommand{\headrulewidth}{0pt}

% Loosen spacing
\setlength{\emergencystretch}{1em}
\newcommand{\Loosen}{\spaceskip 0.375em plus 0.75em minus 0.25em}

% Scratch pad for length calculations
\newlength{\TmpLen}

%% Running heads %%
\newcommand{\RHeads}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\SetRunningHeads}[2][C]{\fancyhead[#1]{\RHeads{#2}}}

\newcommand{\FlushRunningHeads}{\cleardoublepage\fancyhf{}}
\newcommand{\InitRunningHeads}{%
  \setlength{\headheight}{15pt}
  \pagestyle{fancy}
  \thispagestyle{empty}
  \ifthenelse{\boolean{ForPrinting}}{%
    \fancyhead[RO,LE]{\thepage}%
    \SetRunningHeads[CE]{Elementary Principles in}%
    \SetRunningHeads[CO]{Statistical Mechanics}%
  }{%
    \fancyhead[R]{\thepage}%
    \SetRunningHeads[C]{Statistical Mechanics}%
  }
}

\newcommand{\BookMark}[2]{\phantomsection\pdfbookmark[#1]{#2}{#2}}

%% Major document divisions %%
\newcommand{\PGBoilerPlate}{%
  \pagenumbering{Alph}
  \pagestyle{empty}
  \BookMark{0}{PG Boilerplate.}
}
\newcommand{\FrontMatter}{%
  \FlushRunningHeads
  \frontmatter
  \BookMark{-1}{Front Matter.}
}
\newcommand{\MainMatter}{%
  \FlushRunningHeads
  \InitRunningHeads
  \mainmatter
  \BookMark{-1}{Main Matter.}
}

\newcommand{\PGLicense}{%
  \FlushRunningHeads
  \pagenumbering{Roman}
  \InitRunningHeads
  \BookMark{-1}{PG License.}
  \SetRunningHeads{License.}
}

%% ToC formatting %%
\newcommand{\ToCChap}[2]{%
  \SubsectTitle{CHAPTER #1}
  \subsubsection*{\hangindent3em\normalfont\raggedright\MakeUppercase{#2}}
  \medskip
}

% \ToC{Title}{start page}{range}{end page}
\newcommand{\ToC}[4]{%
  \settowidth{\TmpLen}{\ToCPg{#2}{#3}{#4}}%
  \noindent\strut\parbox[b]{\textwidth-\TmpLen}{\raggedright\small%
     \hangindent2em\hspace*{0.5em}#1\dotfill}\ToCPg{#2}{#3}{#4}%
  \smallskip
}

\newcommand{\ToCPg}[3]{%
  \ifthenelse{\equal{#1}{}}{%
    \Pageref{#3}%
  }{%
    \Pageref{#1}#2\Pageref{#3}%
  }%
}

%% Sectional units %%
\newcommand{\SectTitle}[2][\large]{%
  \section*{\centering#1\normalfont #2}
}
\newcommand{\SubsectTitle}[2][\normalsize]{%
  \subsection*{\centering#1\normalfont #2}
}

% \Chapter[Alt title]{Number}{Title}
\newcommand{\Chapter}[3][]{%
  \FlushRunningHeads
  \InitRunningHeads
  \ifthenelse{\equal{#1}{}}{%
    \BookMark{0}{#2 #3}
  }{%
    \BookMark{0}{#2 #1}
  }
  \ifthenelse{\equal{#2}{I.}}{% Chapter I. has a global heading
    \section*{\centering\normalfont ELEMENTARY PRINCIPLES IN STATISTICAL MECHANICS}
  }{}%
  \SectTitle{\MakeUppercase{#2}}
  \SubsectTitle{\MakeUppercase{#3}}
}

% Unnumbered Chapter-like unit (Preface, Contents)
\newcommand{\Chap}[1]{%
  \FlushRunningHeads
  \InitRunningHeads
  \SetRunningHeads{#1}
  \BookMark{0}{#1}
  \SectTitle{\MakeUppercase{#1}}
}

\newcommand{\Theorem}[1]{\medskip\par\emph{Theorem~#1}}

\newcommand{\Signature}[3]{%
  \medskip
  \null\hfill#1\hspace{\parindent}
  \bigskip

  \small%
  \textsc{#2} #3
  \normalsize
}

% Page separators
\newcommand{\PageSep}[1]{\phantomsection\label{page:#1}\ignorespaces}
\newcommand{\Pageref}[1]{\pageref{page:#1}}

% Miscellaneous textual conveniences (N.B. \emph, not \textit)
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\First}[1]{\textsc{#1}}

%% Miscellaneous mathematical formatting %%
\newcommand{\Avg}[2][\eps]{%
  \overline{#2\phantom{\rule{2pt}{8pt}}}\!\!\left.\vphantom{#2}\right|_{#1}%
}

\renewcommand{\Bar}[1]{\overline{#1}}
\renewcommand{\bar}[1]{\overline{#1}}

% Hack to lengthen fraction bar, for use with fractions containing averages
\newcommand{\Fsp}[1][0.5em]{\hspace*{#1}}

\newcommand{\dsp}{\displaystyle}
\newcommand{\eps}{\epsilon}
\newcommand{\Eta}{\mathrm{H}}
\newcommand{\Fact}[2][*]{\ifthenelse{\equal{#1}{}}{#2!}{(#2)!}}
\newcommand{\const}{\text{const}}
\newcommand{\constant}{\text{constant}}

\newcommand{\efrac}[2]{%
  \ifthenelse{\equal{#1}{1}}{\frac{#1}{#2}}{\tfrac{#1}{#2}}}

\DeclareMathOperator{\func}{func.}
\DeclareInputMath{176}{{}^{\circ}}
\DeclareInputMath{183}{\ifmmode\cdot\else\textperiodcentered\fi}
\newcommand{\Op}{\phantom{<}}

%% Integral signs
\newcommand{\ints}{\int\!\! \cdots\!\! \int}
\newcommand{\Int}[2]{%
  \int\!\! \overset{\text{#1}}{\underset{\text{#2}}{\cdots}} \int%
}
\newcommand{\intac}{\Int{all}{config.}}
\newcommand{\intap}{\Int{all}{phases}}
\newcommand{\intav}{\Int{all}{veloc.}}

% over all of space
\newcommand{\intsall}{\int_{-\infty}^{\infty}\ \llap{$\cdots$}\!\! \int_{-\infty}^{\infty}}

% specified limits
\newcommand{\intslim}[2][]{\int_{#1}^{#2} \llap{\!\!$\cdots$}\!\! \int}

% Cross-ref-able equation tags
\newcommand{\Tag}[1]{%
  \phantomsection\label{eqn:#1}\tag*{\ensuremath{#1}}%
}
\newcommand{\Eq}[1]{%
  \hyperref[eqn:#1]{\ensuremath{#1}}%
}

%%%%%%%%%%%%%%%%%%%%%%%% START OF DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%% PG BOILERPLATE %%
\PGBoilerPlate
\begin{center}
\begin{minipage}{\textwidth}
\small
\begin{PGtext}
The Project Gutenberg EBook of Elementary Principles of Statistical
Mechanics, by Josiah Willard Gibbs

This eBook is for the use of anyone anywhere in the United States and most
other parts of the world at no cost and with almost no restrictions
whatsoever.  You may copy it, give it away or re-use it under the terms of
the Project Gutenberg License included with this eBook or online at
www.gutenberg.org.  If you are not located in the United States, you'll have
to check the laws of the country where you are located before using this ebook.

Title: Elementary Principles of Statistical Mechanics

Author: Josiah Willard Gibbs

Release Date: January 22, 2016 [EBook #50992]

Language: English

Character set encoding: ISO-8859-1

*** START OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPLES STATISTICAL MECHANICS ***
\end{PGtext}
\end{minipage}
\end{center}
\newpage
%% Credits and transcriber's note %%
\begin{center}
\begin{minipage}{\textwidth}
\begin{PGtext}
Produced by Andrew D. Hwang
\end{PGtext}
\end{minipage}
\vfill
\end{center}

\begin{minipage}{0.85\textwidth}
\small
\BookMark{0}{Transcriber's Note.}
\subsection*{\centering\normalfont\scshape%
\normalsize\MakeLowercase{\TransNote}}%

\raggedright
\TransNoteText
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%% FRONT MATTER %%%%%%%%%%%%%%%%%%%%%%%%%%
\PageSep{v}
\FrontMatter
\begin{center}
\large ELEMENTARY PRINCIPLES \\[12pt]
\normalsize IN \\[12pt]
\LARGE STATISTICAL MECHANICS\\[12pt]
\footnotesize DEVELOPED WITH ESPECIAL REFERENCE TO\\[12pt]
\normalsize THE RATIONAL FOUNDATION OF \\
THERMODYNAMICS\\[24pt]
\footnotesize BY \\
\normalsize J. WILLARD GIBBS \\
\textit{\footnotesize Professor of Mathematical Physics in Yale University}
\vfill

NEW YORK : CHARLES SCRIBNER'S SONS \\
LONDON: EDWARD ARNOLD \\
1902
\end{center}
\PageSep{vi}
\clearpage
\null
\vfill
\begin{center}
\scriptsize
\textit{Copyright, 1902,} \\
\textsc{By Charles Scribner's Sons} \\
\textit{Published, March, 1902.}
\vfill

\rule{2in}{1pt} \\
UNIVERSITY PRESS · JOHN WILSON \\
AND SON · CAMBRIDGE, U.S.A.
\end{center}
\PageSep{vii}

\Chap{Preface.}

\First{The} usual point of view in the study of mechanics is that
where the attention is mainly directed to the changes which
take place in the course of time in a given system. The principal
problem is the determination of the condition of the
system with respect to configuration and velocities at any
required time, when its condition in these respects has been
given for some one time, and the fundamental equations are
those which express the changes continually taking place in
the system. Inquiries of this kind are often simplified by
taking into consideration conditions of the system other than
those through which it actually passes or is supposed to pass,
but our attention is not usually carried beyond conditions
differing infinitesimally from those which are regarded as
actual.

For some purposes, however, it is desirable to take a broader
view of the subject. We may imagine a great number of
systems of the same nature, but differing in the configurations
and velocities which they have at a given instant, and
differing not merely infinitesimally, but it may be so as to
embrace every conceivable combination of configuration and
velocities. And here we may set the problem, not to follow
a particular system through its succession of configurations,
but to determine how the whole number of systems will be
distributed among the various conceivable configurations and
velocities at any required time, when the distribution has
been given for some one time. The fundamental equation
for this inquiry is that which gives the rate of change of the
number of systems which fall within any infinitesimal limits
of configuration and velocity.
\PageSep{viii}

Such inquiries have been called by Maxwell \emph{statistical}.
They belong to a branch of mechanics which owes its origin to
the desire to explain the laws of thermodynamics on mechanical
principles, and of which Clausius, Maxwell, and Boltzmann
are to be regarded as the principal founders. The first
inquiries in this field were indeed somewhat narrower in their
scope than that which has been mentioned, being applied to
the particles of a system, rather than to independent systems.
Statistical inquiries were next directed to the phases (or conditions
with respect to configuration and velocity) which
succeed one another in a given system in the course of time.
The explicit consideration of a great number of systems and
their distribution in phase, and of the permanence or alteration
of this distribution in the course of time is perhaps first found
in Boltzmann's paper on the ``Zusammenhang zwischen den
Sätzen über das Verhalten mehratomiger Gasmoleküle mit
Jacobi's Princip des letzten Multiplicators'' (1871).

But although, as a matter of history, statistical mechanics
owes its origin to investigations in thermodynamics, it seems
eminently worthy of an independent development, both on
account of the elegance and simplicity of its principles, and
because it yields new results and places old truths in a new
light in departments quite outside of thermodynamics. Moreover,
the separate study of this branch of mechanics seems to
afford the best foundation for the study of rational thermodynamics
and molecular mechanics.

The laws of thermodynamics, as empirically determined,
express the approximate and probable behavior of systems of
a great number of particles, or, more precisely, they express
the laws of mechanics for such systems as they appear to
beings who have not the fineness of perception to enable
them to appreciate quantities of the order of magnitude of
those which relate to single particles, and who cannot repeat
their experiments often enough to obtain any but the most
probable results. The laws of statistical mechanics apply to
conservative systems of any number of degrees of freedom,
\PageSep{ix}
and are exact. This does not make them more difficult to
establish than the approximate laws for systems of a great
many degrees of freedom, or for limited classes of such
systems. The reverse is rather the case, for our attention is
not diverted from what is essential by the peculiarities of the
system considered, and we are not obliged to satisfy ourselves
that the effect of the quantities and circumstances neglected
will be negligible in the result. The laws of thermodynamics
may be easily obtained from the principles of statistical mechanics,
of which they are the incomplete expression, but
they make a somewhat blind guide in our search for those
laws. This is perhaps the principal cause of the slow progress
of rational thermodynamics, as contrasted with the rapid deduction
of the consequences of its laws as empirically established.
To this must be added that the rational foundation
of thermodynamics lay in a branch of mechanics of which
the fundamental notions and principles, and the characteristic
operations, were alike unfamiliar to students of mechanics.

We may therefore confidently believe that nothing will
more conduce to the clear apprehension of the relation of
thermodynamics to rational mechanics, and to the interpretation
of observed phenomena with reference to their evidence
respecting the molecular constitution of bodies, than the
study of the fundamental notions and principles of that department
of mechanics to which thermodynamics is especially
related.

Moreover, we avoid the gravest difficulties when, giving up
the attempt to frame hypotheses concerning the constitution
of material bodies, we pursue statistical inquiries as a branch
of rational mechanics. In the present state of science, it
seems hardly possible to frame a dynamic theory of molecular
action which shall embrace the phenomena of thermodynamics,
of radiation, and of the electrical manifestations
which accompany the union of atoms. Yet any theory is
obviously inadequate which does not take account of all
these phenomena. Even if we confine our attention to the
\PageSep{x}
phenomena distinctively thermodynamic, we do not escape
difficulties in as simple a matter as the number of degrees
of freedom of a diatomic gas. It is well known that while
theory would assign to the gas six degrees of freedom per
molecule, in our experiments on specific heat we cannot account
for more than five. Certainly, one is building on an
insecure foundation, who rests his work on hypotheses concerning
the constitution of matter.

Difficulties of this kind have deterred the author from attempting
to explain the mysteries of nature, and have forced
him to be contented with the more modest aim of deducing
some of the more obvious propositions relating to the statistical
branch of mechanics. Here, there can be no mistake in
regard to the agreement of the hypotheses with the facts of
nature, for nothing is assumed in that respect. The only
error into which one can fall, is the want of agreement between
the premises and the conclusions, and this, with care,
one may hope, in the main, to avoid.

The matter of the present volume consists in large measure
of results which have been obtained by the investigators
mentioned above, although the point of view and the arrangement
may be different. These results, given to the public
one by one in the order of their discovery, have necessarily,
in their original presentation, not been arranged in the most
logical manner.

In the first chapter we consider the general problem which
has been mentioned, and find what may be called the fundamental
equation of statistical mechanics. A particular case
of this equation will give the condition of statistical equilibrium,
\ie, the condition which the distribution of the
systems in phase must satisfy in order that the distribution
shall be permanent. In the general case, the fundamental
equation admits an integration, which gives a principle which
may be variously expressed, according to the point of view
from which it is regarded, as the conservation of density-in-phase,
or of extension-in-phase, or of probability of phase.
\PageSep{xi}

In the second chapter, we apply this principle of conservation
of probability of phase to the theory of errors in the
calculated phases of a system, when the determination of the
arbitrary constants of the integral equations are subject to
error. In this application, we do not go beyond the usual
approximations. In other words, we combine the principle
of conservation of probability of phase, which is exact, with
those approximate relations, which it is customary to assume
in the ``theory of errors.''

In the third chapter we apply the principle of conservation
of extension-in-phase to the integration of the differential
equations of motion. This gives Jacobi's ``last multiplier,''
as has been shown by Boltzmann.

In the fourth and following chapters we return to the consideration
of statistical equilibrium, and confine our attention
to conservative systems. We consider especially ensembles
of systems in which the index (or logarithm) of probability of
phase is a linear function of the energy. This distribution,
on account of its unique importance in the theory of statistical
equilibrium, I have ventured to call \emph{canonical}, and the
divisor of the energy, the \emph{modulus} of distribution. The
moduli of ensembles have properties analogous to temperature,
in that equality of the moduli is a condition of equilibrium
with respect to exchange of energy, when such exchange is
made possible.

We find a differential equation relating to average values
in the ensemble which is identical in form with the fundamental
differential equation of thermodynamics, the average
index of probability of phase, with change of sign, corresponding
to entropy, and the modulus to temperature.

For the average square of the anomalies of the energy, we
find an expression which vanishes in comparison with the
square of the average energy, when the number of degrees
of freedom is indefinitely increased. An ensemble of systems
in which the number of degrees of freedom is of the same
order of magnitude as the number of molecules in the bodies
\PageSep{xii}
with which we experiment, if distributed canonically, would
therefore appear to human observation as an ensemble of
systems in which all have the same energy.

We meet with other quantities, in the development of the
subject, which, when the number of degrees of freedom is
very great, coincide sensibly with the modulus, and with the
average index of probability, taken negatively, in a canonical
ensemble, and which, therefore, may also be regarded as corresponding
to temperature and entropy. The correspondence
is however imperfect, when the number of degrees of freedom
is not very great, and there is nothing to recommend these
quantities except that in definition they may be regarded as
more simple than those which have been mentioned. In
Chapter~XIV, this subject of thermodynamic analogies is
discussed somewhat at length.

Finally, in Chapter~XV, we consider the modification of
the preceding results which is necessary when we consider
systems composed of a number of entirely similar particles,
or, it may be, of a number of particles of several kinds, all of
each kind being entirely similar to each other, and when one
of the variations to be considered is that of the numbers of
the particles of the various kinds which are contained in a
system. This supposition would naturally have been introduced
earlier, if our object had been simply the expression of
the laws of nature. It seemed desirable, however, to separate
sharply the purely thermodynamic laws from those special
modifications which belong rather to the theory of the properties
of matter.

\Signature{J. W. G.}{New Haven,}{December, 1901.}
\PageSep{xiii}
% [** TN: Setting ToC manually]

\Chap{Contents.}

\ToCChap{I.}{General Notions. The Principle of Conservation
of Extension-in-Phase.}

%Page

\ToC{Hamilton's equations of motion}{3}{--}{5}

\ToC{Ensemble of systems distributed in phase}{}{}{5}

\ToC{Extension-in-phase, density-in-phase}{}{}{6}

\ToC{Fundamental equation of statistical mechanics}{6}{--}{8}

\ToC{Condition of statistical equilibrium}{}{}{8}

\ToC{Principle of conservation of density-in-phase}{}{}{9}

\ToC{Principle of conservation of extension-in-phase}{}{}{10}

\ToC{Analogy in hydrodynamics}{}{}{11}

\ToC{Extension-in-phase is an invariant}{11}{--}{13}

\ToC{Dimensions of extension-in-phase}{}{}{13}

\ToC{Various analytical expressions of the principle}{13}{--}{15}

\ToC{Coefficient and index of probability of phase}{}{}{16}

\ToC{Principle of conservation of probability of phase}{17}{, }{18}

\ToC{Dimensions of coefficient of probability of phase}{}{}{19}

\ToCChap{II.}{Application of the Principle of Conservation of
Extension-in-Phase to the Theory of Errors.}

\ToC{Approximate expression for the index of probability of phase}{20}{, }{21}

\ToC{Application of the principle of conservation of probability of phase
to the constants of this expression}{21}{--}{25}

\ToCChap{III.}{Application of the Principle of Conservation of
Extension-in-Phase to the Integration of the
Differential Equations of Motion.}

\ToC{Case in which the forces are \Typo{function}{functions} of the coördinates alone}{26}{--}{29}

\ToC{Case in which the forces are functions of the coördinates with the
time}{30}{, }{31}
\PageSep{xiv}

\ToCChap{IV.}{On the Distribution-in-Phase called Canonical, in
which the Index of Probability is a Linear
Function of the Energy.}

%Page
\ToC{Condition of statistical equilibrium}{}{}{32}

\ToC{Other conditions which the coefficient of probability must satisfy}{}{}{33}

\ToC{Canonical distribution Modulus of distribution}{}{}{34}

\ToC{$\psi$ must be finite}{}{}{35}

\ToC{The modulus of the canonical distribution has properties analogous
to temperature}{35}{--}{37}

\ToC{Other distributions have similar properties}{}{}{37}

\ToC{Distribution in which the index of probability is a linear function of
the energy and of the moments of momentum about three axes}{38}{, }{39}

\ToC{Case in which the forces are linear functions of the displacements,
and the index is a linear function of the separate energies relating
to the normal types of motion}{39}{--}{41}

\ToC{Differential equation relating to average values in a canonical
ensemble}{42}{--}{44}

\ToC{This is identical in form with the fundamental differential equation
of thermodynamics}{44}{, }{45}

\ToCChap{V.}{Average Values in a Canonical Ensemble of Systems.}

\ToC{Case of $\nu$ material points. Average value of kinetic energy of a
single point for a given configuration or for the whole ensemble
$= \frac{3}{2} \Theta$}{46}{, }{47}

\ToC{Average value of total kinetic energy for any given configuration
or for the whole ensemble $= \frac{3}{2} \nu \Theta$}{}{}{47}

\ToC{System of $n$~degrees of freedom. Average value of kinetic energy,
for any given configuration or for the whole ensemble $= \frac{n}{2} \Theta$}{48}{--}{50}

\ToC{Second proof of the same proposition}{50}{--}{52}

\ToC{Distribution of canonical ensemble in configuration}{52}{--}{54}

\ToC{Ensembles canonically distributed in configuration}{}{}{55}

\ToC{Ensembles canonically distributed in velocity}{}{}{56}

\ToCChap{VI.}{Extension-in-Configuration and Extension-in-Vel\-o\-city.}

\ToC{Extension-in-configuration and extension-in-velocity are invariants}{57}{--}{59}
\PageSep{xv}
%Page
\ToC{Dimensions of these quantities}{}{}{60}

\ToC{Index and coefficient of probability of configuration}{}{}{61}

\ToC{Index and coefficient of probability of velocity}{}{}{62}

\ToC{Dimensions of these coefficients}{}{}{63}

\ToC{Relation between extension-in-configuration and extension-in-velocity}{}{}{64}

\ToC{Definitions of extension-in-phase, extension-in-configuration, and extension-in-velocity,
without explicit mention of coördinates}{65}{--}{67}

\ToCChap{VII.}{Farther Discussion of Averages in a Canonical
Ensemble of Systems.}

\ToC{Second and third differential equations relating to average values
in a canonical ensemble}{68}{, }{69}

\ToC{These are identical in form with thermodynamic equations enunciated
by Clausius}{}{}{69}

\ToC{Average square of the anomaly---of the energy of the kinetic energy---of
the potential energy}{70}{--}{72}

\ToC{These anomalies are insensible to human observation and experience
when the number of degrees of freedom of the system is very
great}{73}{, }{74}

\ToC{Average values of powers of the energies}{75}{--}{77}

\ToC{Average values of powers of the anomalies of the energies}{77}{--}{80}

\ToC{Average values relating to forces exerted on external bodies}{80}{--}{83}

\ToC{General formulae relating to averages in a canonical ensemble}{83}{--}{86}

\ToCChap{VIII.}{On Certain Important Functions of the Energies
of a System.}

\ToC{Definitions. $V =$ extension-in-phase below a limiting energy~($\eps$).
$\phi = \log dV/d\eps$}{87}{, }{88}

\ToC{$V_{q} =$ extension-in-configuration below a limiting value of the potential
energy~($e_{p}$). $\phi_{q} = \log dV_{q}/d\eps_{q}$}{89}{, }{90}

\ToC{$V_{p} =$ extension-in-velocity below a limiting value of the kinetic energy~($\eps_{p}$).
$\phi_{p} = \log dV_{p}/d\eps_{p}$}{90}{, }{91}

\ToC{Evaluation of $V_{p}$ and $\phi_{p}$}{91}{--}{93}

\ToC{Average values of functions of the kinetic energy}{94}{, }{95}

\ToC{Calculation of~$V$ from~$V_{q}$}{95}{, }{96}

\ToC{Approximate formulae for large values of~$n$}{97}{, }{98}

\ToC{Calculation of~$V$ or~$\phi$ for whole system when given for parts}{}{}{98}

\ToC{Geometrical illustration}{}{}{99}
\PageSep{xvi}

\ToCChap{IX.}{The Function~$\phi$ and the Canonical Distribution.}

\ToC{When $n > 2$, the most probable value of the energy in a canonical
ensemble is determined by $d\phi/d\eps = 1/\Theta$}{100}{, }{101}

\ToC{When $n > 2$, the average value of $d\phi/d\eps$ in a canonical ensemble
is~$1/\Theta$}{}{}{101}

\ToC{When $n$~is large, the value of~$\phi$ corresponding to $d\phi/d\eps = 1\Theta$
($\phi_{0}$) is nearly equivalent (except for an additive constant) to
the average index of probability taken negatively~($-\bar{\eta}$)}{101}{--}{104}

\ToC{Approximate formulae for $\phi_{0} + \bar{\eta}$ when $n$~is large}{104}{--}{106}

\ToC{When $n$~is large, the distribution of a canonical ensemble in energy
follows approximately the law of errors}{}{}{105}

\ToC{This is not peculiar to the canonical distribution}{107}{, }{108}

\ToC{Averages in a canonical ensemble}{108}{--}{114}

\ToCChap{X.}{On a Distribution in Phase Called Microcanonical
in which all the Systems have the same
Energy.}

\ToC{The microcanonical distribution defined as the limiting distribution
obtained by various processes}{115}{, }{116}

\ToC{Average values in the microcanonical ensemble of functions of the
kinetic and potential energies}{117}{--}{120}

\ToC{If two quantities have the same average values in every microcanonical
ensemble, they have the same average value in every canonical
ensemble}{}{}{120}

\ToC{Average values in the microcanonical ensemble of functions of the
energies of parts of the system}{121}{--}{123}

\ToC{Average values of functions of the kinetic energy of a part of the
system}{123}{, }{124}

\ToC{Average values of the external forces in a microcanonical ensemble.
Differential equation relating to these averages, having the form
of the fundamental differential equation of thermodynamics}{124}{--}{128}

\ToCChap{XI.}{Maximum and Minimum Properties of Various Distributions
in Phase.}

\ToC{Theorems I--VI\@. Minimum properties of certain distributions}{129}{--}{133}

\ToC{Theorem VII\@. The average index of the whole system compared
with the sum of the average indices of the parts}{133}{--}{135}
\PageSep{xvii}
% Page
\ToC{Theorem VIII\@. The average index of the whole ensemble compared
with the average indices of parts of the ensemble}{135}{--}{137}

\ToC{Theorem IX\@. Effect on the average index of making the distribution-in-phase
uniform within any limits}{137}{--}{138}

\ToCChap{XII.}{On the Motion of Systems and Ensembles of Systems
through Long Periods of Time.}

\ToC{Under what conditions, and with what limitations, may we assume
that a system will return in the course of time to its original
phase, at least to any required degree of approximation?}{139}{--}{142}

\ToC{Tendency in an ensemble of isolated systems toward a state of statistical
equilibrium}{143}{--}{151}

\ToCChap{XIII.}{Effect of Various Processes on an Ensemble of
Systems.}

\ToC{Variation of the external coördinates can only cause a decrease in
the average index of probability}{152}{--}{154}

\ToC{This decrease may in general be diminished by diminishing the
rapidity of the change in the external coördinates}{154}{--}{157}

\ToC{The mutual action of two ensembles can only diminish the sum of
their average indices of probability}{158}{, }{159}

\ToC{In the mutual action of two ensembles which are canonically distributed,
that which has the greater modulus will lose energy}{}{}{160}

\ToC{Repeated action between any ensemble and others which are canonically
distributed with the same modulus will tend to distribute
the first-mentioned ensemble canonically with the same modulus}{}{}{161}

\ToC{Process analogous to a Carnot's cycle}{162}{, }{163}

\ToC{Analogous processes in thermodynamics}{163}{, }{164}

\ToCChap{XIV.}{Discussion of Thermodynamic Analogies.}

\ToC{The finding in rational mechanics an \textit{\Chg{à}{a}~priori} foundation for thermodynamics
requires mechanical definitions of temperature and
entropy. Conditions which the quantities thus defined must
satisfy}{165}{--}{167}

\ToC{The modulus of a canonical ensemble~($\Theta$), and the average index of
probability taken negatively~($\bar{\eta}$), as analogues of temperature
and entropy}{167}{--}{169}
\PageSep{xviii}
% Page
\ToC{The functions of the energy $d\eps/d \log V$ and $\log V$ as analogues of
temperature and entropy}{169}{--}{172}

\ToC{The functions of the energy $d\eps/d\phi$ and $\phi$ as analogues of temperature
and entropy}{172}{--}{178}

\ToC{Merits of the different systems}{178}{--}{183}

\ToC{If a system of a great number of degrees of freedom is microcanonically
distributed in phase, any very small part of it may be regarded
as canonically distributed}{}{}{183}

\ToC{Units of $\Theta$ and $\bar{\eta}$ compared with those of temperature and
entropy}{183}{--}{186}

\ToCChap{XV.}{Systems Composed of Molecules.}

\ToC{Generic and specific definitions of a phase}{187}{--}{189}

\ToC{Statistical equilibrium with respect to phases generically defined
and with respect to phases specifically defined}{}{}{189}

\ToC{Grand ensembles, petit ensembles}{189}{, }{190}

\ToC{Grand ensemble canonically distributed}{190}{--}{193}

\ToC{$\Omega$~must be finite}{}{}{193}

\ToC{Equilibrium with respect to gain or loss of molecules}{194}{--}{197}

\ToC{Average value of any quantity in grand ensemble canonically distributed}{}{}{198}

\ToC{Differential equation identical in form with fundamental differential
equation in thermodynamics}{199}{, }{200}

\ToC{Average value of number of any kind of molecules~($\nu$)}{}{}{201}

\ToC{Average value of $(\nu - \bar{\nu})^{2}$}{201}{, }{202}

\ToC{Comparison of indices}{203}{--}{206}

\ToC{When the number of particles in a system is to be treated as
variable, the average index of probability for phases generically
defined corresponds to entropy}{}{}{206}
%\PageSep{1}
% ELEMENTARY PRINCIPLES IN STATISTICAL MECHANICS
%\PageSep{2}
% [Blank page]
\MainMatter
\PageSep{3}


%[** TN: \Chapter prints ELEMENTARY PRINCIPLES IN STATISTICAL MECHANICS]

\Chapter{I.}{General Notions. The Principle of Conservation
of Extension-in-Phase.}

\First{We} shall use Hamilton's form of the equations of motion for
a system of $n$~degrees of freedom, writing $q_{1}$,~\dots $q_{n}$ for the
(generalized) coördinates, $\dot{q}_{1}$,~\dots $\dot{q}_{n}$ for the (generalized) velocities,
and
\[
F_{1}\, dq_{1} + F_{2}\, dq_{2} \Add{+} \cdots + F_{n}\, dq_{n}
\Tag{(1)}
\]
for the moment of the forces. We shall call the quantities
$F_{1}$,~\dots $F_{n}$ the (generalized) forces, and the quantities $p_{1}$,~\dots $p_{n}$,
defined by the equations
\[
p_{1} = \frac{d\eps_{p}}{d\Dot{q}_{1}},\quad
p_{2} = \frac{d\eps_{p}}{d\Dot{q}_{2}},\quad \text{etc.},
\Tag{(2)}
\]
where $\eps_{p}$~denotes the kinetic energy of the system, the (generalized)
momenta. The kinetic energy is here regarded as
a function of the velocities and coördinates. We shall usually
regard it as a function of the momenta and coördinates,\footnote
  {The use of the momenta instead of the velocities as independent variables
  is the characteristic of Hamilton's method which gives his equations of motion
  their remarkable degree of simplicity. We shall find that the fundamental
  notions of statistical mechanics are most easily defined, and are expressed in
  the most simple form, when the momenta with the coördinates are used to
  describe the state of a system.}
and on this account we denote it by~$\eps_{p}$. This will not prevent
us from occasionally using \Chg{formulæ}{formulae} like~\Eq{(2)}, where it is
sufficiently evident the kinetic energy is regarded as function
of the $\dot{q}$'s~and~$q$'s. But in expressions like $d\eps_{p}/dq_{1}$, where the
denominator does not determine the question, the kinetic
\PageSep{4}
energy is always to be treated in the differentiation as function
of the $p$'s and~$q$'s.

We have then
\[
\dot{q}_{1} = \frac{d\eps_{p}}{dp_{1}},\quad
\dot{p}_{1} = -\frac{d\eps_{p}}{dq_{1}} + F_{1},\quad \text{etc.}
\Tag{(3)}
\]

These equations will hold for any forces whatever. If the
forces are conservative, in other words, if the expression~\Eq{(1)}
is an exact differential, we may set
\[
F_{1} = -\frac{d\eps_{q}}{dq_{1}},\quad
F_{2} = -\frac{d\eps_{q}}{dq_{2}},\quad \text{etc.,}
\Tag{(4)}
\]
where $\eps_{q}$~is a function of the coördinates which we shall call
the potential energy of the system. If we write~$\eps$ for the
total energy, we shall have
\[
\eps = \eps_{p} + \eps_{q},
\Tag{(5)}
\]
and equations~\Eq{(3)} may be written
\[
\dot{q}_{1} = \frac{d\eps}{dp_{1}},\quad
\dot{p}_{1} = -\frac{d\eps}{dq_{1}},\quad \text{etc.}
\Tag{(6)}
\]

The potential energy~($\eps_{q}$) may depend on other variables
beside the coördinates $q_{1}$\Add{,}~\dots $q_{n}$. We shall often suppose it to
depend in part on coördinates of external bodies, which we
shall denote by $a_{1}$, $a_{2}$, etc. We shall then have for the complete
value of the differential of the potential energy\footnote
  {It will be observed, that although we call $\eps_{q}$ the potential energy of the
  system which we are considering, it is really so defined as to include that
  energy which might be described as mutual to that system and external
  bodies.}%
\[
d\eps_{q} = -F_{1}\, dq_{1} \Add{-} \cdots - F_{n}\, dq_{n} - A_{1}\, da_{1} - A_{2}\, da_{2} - \text{etc.,}
\Tag{(7)}
\]
where $A_{1}$, $A_{2}$, etc., represent forces (in the generalized sense)
exerted by the system on external bodies. For the total energy~($\eps$)
we shall have
\begin{multline*}
d\eps = \dot{q}_{1}\, dp_{1} \Add{+} \cdots + \dot{q}_{n}\, dp_{n} - \dot{p}_{1}\, dq_{1} \Add{-} \cdots \\
  - \dot{p}_{n}\, dq_{n} - A_{1}\, da_{1} - A_{2}\, da_{2} - \text{etc.,}
\Tag{(8)}
\end{multline*}

It will be observed that the kinetic energy~($\eps_{p}$) in the
most general case is a quadratic function of the~$p$'s (or~$\dot{q}$'s)
\PageSep{5}
involving also the~$q$'s but not the~$a$'s; that the potential energy,
when it exists, is function of the $q$'s and~$a$'s; and that the
total energy, when it exists, is function of the~$p$'s (or~$\dot{q}$'s), the~$q$'s,
and the~$a$'s. In expressions like $d\eps/dq_{1}$, the~$p$'s, and not
the~$\dot{q}$'s, are to be taken as independent variables, as has already
been stated with respect to the kinetic energy.

Let us imagine a great number of independent systems,
identical in nature, but differing in phase, that is, in their
condition with respect to configuration and velocity. The
forces are supposed to be determined for every system by the
same law, being functions of the coördinates of the system
$q_{1}$,~\dots $q_{n}$, either alone or with the coördinates $a_{1}$, $a_{2}$, etc.\ of
certain external bodies. It is not necessary that they should
be derivable from a force-function. The external coördinates
$a_{1}$, $a_{2}$, etc.\ may vary with the time, but at any given time
have fixed values. In this they differ from the internal
coördinates $q_{1}$,~\dots $q_{n}$, which at the same time have different
values in the different systems considered.

Let us especially consider the number of systems which at a
given instant fall within specified limits of phase, viz., those
for which
\[
\left.
\begin{alignedat}{2}
p_{1}' &< p_{1} &&< p_{1}'', \\
p_{2}' &< p_{2} &&< p_{2}'', \\
\cdots &\Op \cdots &&\Op \cdots  \\
p_{n}' &< p_{n} &&< p_{n}'', \\
\end{alignedat}
\qquad\qquad
\begin{alignedat}{2}
q_{1}' &< q_{1} &&< q_{1}'', \\
q_{2}' &< q_{2} &&< q_{2}'', \\
\cdots &\Op \cdots &&\Op \cdots  \\
q_{n}' &< q_{n} &&< q_{n}'', \\
\end{alignedat}
\right\}
\Tag{(9)}
\]
the accented letters denoting constants. We shall suppose
the differences $p_{1}'' - p_{1}'$, $q_{1}'' - q_{1}'$, etc.\ to be infinitesimal, and
that the systems are distributed in phase in some continuous
manner,\footnote
  {In strictness, a finite number of systems cannot be distributed continuously
  in phase. But by increasing indefinitely the number of systems, we
  may approximate to a continuous law of distribution, such as is here
  described. To avoid tedious circumlocution, language like the above may
  be allowed, although wanting in precision of expression, when the sense in
  which it is to be taken appears sufficiently clear.}
so that the number having phases within the limits
specified may be represented by
\[
D(p_{1}'' - p_{1}') \cdots (p_{n}'' - p_{n}')(q_{1}'' - q_{1}') \cdots (q_{n}'' - q_{n}'),
\Tag{(10)}
\]
\PageSep{6}
or more briefly by
\[
D\, dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n},
\Tag{(11)}
\]
where $D$~is a function of the $p$'s and~$q$'s and in general of $t$~also,
for as time goes on, and the individual systems change their
phases, the distribution of the ensemble in phase will in general
vary. In special cases, the distribution in phase will
remain unchanged. These are cases of \emph{statistical equilibrium}.

If we regard all possible phases as forming a sort of extension
of $2n$~dimensions, we may regard the product of differentials
in~\Eq{(11)} as expressing an element of this extension, and
$D$~as expressing the density of the systems in that element.
We shall call the product
\[
dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}
\Tag{(12)}
\]
an element of \emph{extension-in-phase}, and $D$~the \emph{density-in-phase}
of the systems.

It is evident that the changes which take place in the density
of the systems in any given element of extension-in-phase
will depend on the dynamical nature of the systems
and their distribution in phase at the time considered.

In the case of conservative systems, with which we shall be
principally concerned, their dynamical nature is completely
determined by the function which expresses the energy~($\eps$) in
terms of the $p$'s,~$q$'s, and~$a$'s (a function supposed identical
for all the systems); in the more general case which we are
considering, the dynamical nature of the systems is determined
by the functions which express the kinetic energy~($\eps_{p}$)
in terms of the $p$'s and~$q$'s, and the forces in terms of the
$q$'s and~$a$'s. The distribution in phase is expressed for the
time considered by~$D$ as function of the $p$'s and~$q$'s. To find
the value of~$dD/dt$ for the specified element of extension-in-phase,
we observe that the number of systems within the
limits can only be varied by systems passing the limits, which
may take place in $4n$~different ways, viz., by the $p_{1}$ of a system
passing the limit~$p_{1}'$, or the limit~$p_{1}''$, or by the $q_{1}$ of a
system passing the limit~$q_{1}'$ or the limit~$q_{1}''$, etc. Let us
consider these cases separately.
\PageSep{7}

In the first place, let us consider the number of systems
which in the time~$dt$ pass into or out of the specified element
by $p_{1}$~passing the limit~$p_{1}'$. It will be convenient, and it is
evidently allowable, to suppose $dt$~so small that the quantities
$\dot{p}_{1}\, dt$, $\dot{q}_{1}\, dt$, etc., which represent the increments of $p_{1}$, $q_{1}$, etc.,
in the time~$dt$ shall be infinitely small in comparison with
the infinitesimal differences $p_{1}'' - p_{1}'$, $q_{1}'' - q_{1}'$, etc., which determine
the magnitude of the element of extension-in-phase.
The systems for which $p_{1}$~passes the limit~$p_{1}'$ in the interval~$dt$
are those for which at the commencement of this interval
the value of~$p_{1}$ lies between $p_{1}'$ and $p_{1}' - \dot{p}_{1}\, dt$, as is evident
if we consider separately the cases in which $p_{1}$~is positive and
negative. Those systems for which $p_{1}$~lies between these
limits, and the other $p$'s and~$q$'s between the limits specified in~\Eq{(9)},
will therefore pass into or out of the element considered
according as $\dot{p}_{1}$~is positive or negative, unless indeed they also
pass some other limit specified in~\Eq{(9)} during the same interval
of time. But the number which pass any two of these
limits will be represented by an expression containing the
square of~$dt$ as a factor, and is evidently negligible, when $dt$~is
sufficiently small, compared with the number which we are
seeking to evaluate, and which (with neglect of terms containing~$dt^{2}$)
may be found by substituting $\dot{p}_{1}\, dt$ for $p_{1}'' - p_{1}'$ in~\Eq{(10)}
or for~$dp_{1}$ in~\Eq{(11)}.

The expression
\[
D\, \dot{p}_{1}\, dt\, dp_{2} \cdots dp_{n}\, dq_{1} \cdots dq_{n}
\Tag{(13)}
\]
will therefore represent, according as it is positive or negative,
the increase or decrease of the number of systems within the
given limits which is due to systems passing the limit~$p_{1}'$. A
similar expression, in which however $D$~and $\dot{p}$ will have
slightly different values (being determined for~$p_{1}''$ instead of~$p_{1}'$),
will represent the decrease or increase of the number of
systems due to the passing of the limit~$p_{1}''$. The difference
of the two expressions, or
\[
\frac{d(D \dot{p}_{1})}{dp_{1}}\, dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}\, dt
\Tag{(14)}
\]
\PageSep{8}
will represent algebraically the decrease of the number of
systems within the limits due to systems passing the limits $p_{1}'$
and~$p_{1}''$.

The decrease in the number of systems within the limits
due to systems passing the limits $q_{1}'$ and~$q_{1}''$ may be found in
the same way. This will give
\[
\left(\frac{d(D \dot{p}_{1})}{dp_{1}} + \frac{d(D \dot{q}_{1})}{dq_{1}}\right) dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}\, dt
\Tag{(15)}
\]
for the decrease due to passing the four limits $p_{1}'$,~$p_{1}''$, $q_{1}'$,~$q_{1}''$.
But since the equations of motion~\Eq{(3)} give
\[
\frac{d\Dot{p}_{1}}{dp_{1}} + \frac{d\Dot{q}_{1}}{dq_{1}} = 0,
\Tag{(16)}
\]
the expression reduces to
\[
\left(\frac{dD}{dp_{1}}\, \dot{p}_{1} + \frac{dD}{dq_{1}}\, \dot{q}_{1}\right) dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}\, dt.
\Tag{(17)}
\]

If we prefix~$\sum$ to denote summation relative to the suffixes
$1$\Add{,}~\dots $n$, we get the total decrease in the number of systems
within the limits in the time~$dt$. That is,
\begin{multline*}
\sum \left(\frac{dD}{dp_{1}}\, \dot{p}_{1} + \frac{dD}{dq_{1}}\, \dot{q}_{1}\right) dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}\, dt \\
  = -dD\, dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n},
\Tag{(18)}
\end{multline*}
or
\[
\left(\frac{dD}{dt}\right)_{p,q} = -\sum \left(\frac{dD}{dp_{1}}\, \dot{p}_{1} + \frac{dD}{dq_{1}}\, \dot{q}_{1}\right),
\Tag{(19)}
\]
where the suffix applied to the differential coefficient indicates
that the $p$'s and~$q$'s are to be regarded as constant in the differentiation.
The condition of statistical equilibrium is therefore
\[
\sum \left(\frac{dD}{dp_{1}}\, \dot{p}_{1} + \frac{dD}{dq_{1}}\, \dot{q}_{1}\right) = 0.
\Tag{(20)}
\]
If at any instant this condition is fulfilled for all values of the
$p$'s and~$q$'s, $(dD/dt)_{p,q}$ vanishes, and therefore the condition
will continue to hold, and the distribution in phase will be
permanent, so long as the external coördinates remain constant.
But the statistical equilibrium would in general be disturbed
by a change in the values of the external coördinates, which
\PageSep{9}
would alter the values of the~$\dot{p}$'s as determined by equations~\Eq{(3)},
and thus disturb the relation expressed in the last equation.

If we write equation~\Eq{(19)} in the form
\[
\left(\frac{dD}{dt}\right)_{p,q} dt + \sum \left(\frac{dD}{dp_{1}}\, \dot{p}_{1}\, dt + \frac{dD}{dq_{1}}\, \dot{q}_{1}\, dt\right) = 0,
\Tag{(21)}
\]
it will be seen to express a theorem of remarkable simplicity.
Since $D$~is a function of $t$, $p_{1}$,~\dots $p_{n}$, $q_{1}$,~\dots $q_{n}$, its complete
differential will consist of parts due to the variations of all
these quantities. Now the first term of the equation represents
the increment of~$D$ due to an increment of~$t$ (with constant
values of the $p$'s and~$q$'s), and the rest of the first member
represents the increments of~$D$ due to increments of the $p$'s
and~$q$'s, expressed by $\dot{p}_{1}\, dt$, $\dot{q}_{1}\, dt$, etc. But these are precisely
the increments which the $p$'s and~$q$'s receive in the movement
of a system in the time~$dt$. The whole expression represents
the total increment of~$D$ for the varying phase of a moving
system. We have therefore the theorem:---

\emph{In an ensemble of mechanical systems identical in nature and
subject to forces determined by identical laws, but distributed
in phase in any continuous manner, the density-in-phase is
constant in time for the varying phases of a moving system;
provided, that the forces of a system are functions of its coördinates,
either alone or with the time.}\footnote
  {The condition that the forces $F_{1}$,~\dots $F_{n}$ are functions of $q_{1}$,~\dots $q_{n}$ and
  $a_{1}$, $a_{2}$, etc., which last are functions of the time, is analytically equivalent
  to the condition that $F_{1}$,~\dots $F_{n}$ are functions of $q_{1}$,~\dots $q_{n}$ and the time.
  Explicit mention of the external coördinates, $a_{1}$, $a_{2}$, etc., has been made in
  the preceding pages, because our purpose will require us hereafter to consider
  these coördinates and the connected forces, $A_{1}$, $A_{2}$, etc., which represent
  the action of the systems on external bodies.}%

This may be called the principle of \emph{conservation of density-in-phase}.
It may also be written
\[
\left(\frac{dD}{dt}\right)_{a, \dots h} = 0,
\Tag{(22)}
\]
where $a$,~\dots $h$ represent the arbitrary constants of the integral
equations of motion, and are suffixed to the differential coefficient
\PageSep{10}
to indicate that they are to be regarded as constant
in the differentiation.

We may give to this principle a slightly different expression.
Let us call the value of the integral
\[
\ints dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}
\Tag{(23)}
\]
taken within any limits the \emph{extension-in-phase} within those
limits.

\emph{When the phases bounding an extension-in-phase vary in
the course of time according to the dynamical laws of a system
subject to forces which are functions of the coördinates either
alone or with the time, the value of the extension-in-phase thus
bounded remains constant.} In this form the principle may be
called the principle of \emph{conservation of extension-in-phase}. In
some respects this may be regarded as the most simple statement
of the principle, since it contains no explicit reference
to an ensemble of systems.

Since any extension-in-phase may be divided into infinitesimal
portions, it is only necessary to prove the principle for
an infinitely small extension. The number of systems of an
ensemble which fall within the extension will be represented
by the integral
\[
\ints D\, dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}.
\]
If the extension is infinitely small, we may regard $D$ as constant
in the extension and write
\[
D \ints dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n}
\]
for the number of systems. The value of this expression must
be constant in time, since no systems are supposed to be
created or destroyed, and none can pass the limits, because
the motion of the limits is identical with that of the systems.
But we have seen that $D$~is constant in time, and therefore
the integral
\[
\ints dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n},
\]
\PageSep{11}
which we have called the extension-in-phase, is also constant
in time.\footnote
  {If we regard a phase as represented by a point in space of $2n$~dimensions,
  the changes which take place in the course of time in our ensemble of
  systems will be represented by a current in such space. This current will
  be steady so long as the external coördinates are not varied. In any case
  the current will satisfy a law which in its various expressions is analogous
  to the hydrodynamic law which may be expressed by the phrases \emph{conservation
  of volumes} or \emph{conservation of density about a moving point}, or by the equation
  \[
  \frac{d\dot{x}}{dx} + \frac{d\dot{y}}{dy} + \frac{d\dot{z}}{dz} = 0.
  \]

  The analogue in statistical mechanics of this equation, viz.,
  \[
  \frac{d\dot{p}_{1}}{dp_{1}} + \frac{d\dot{q}_{1}}{dq_{1}} +
  \frac{d\dot{p}_{2}}{dp_{2}} + \frac{d\dot{q}_{2}}{dq_{2}} + \text{etc.} = 0,
  \]
  may be derived directly from equations \Eq{(3)} or~\Eq{(6)}, and may suggest such
  theorems as have been enunciated, if indeed it is not regarded as making
  them intuitively evident. The somewhat lengthy demonstrations given
  above will at least serve to give precision to the notions involved, and
  familiarity with their use.}%

Since the system of coördinates employed in the foregoing
discussion is entirely arbitrary, the values of the coördinates
relating to any configuration and its immediate vicinity do
not impose any restriction upon the values relating to other
configurations. The fact that the quantity which we have
called density-in-phase is constant in time for any given system,
implies therefore that its value is independent of the
coördinates which are used in its evaluation. For let the
density-in-phase as evaluated for the same time and phase by
one system of coördinates be~$D_{1}'$, and by another system~$D_{2}'$.
A system which at that time has that phase will at another
time have another phase. Let the density as calculated for
this second time and phase by a third system of coördinates
be~$D_{3}''$. Now we may imagine a system of coördinates which
at and near the first configuration will coincide with the first
system of coördinates, and at and near the second configuration
will coincide with the third system of coördinates. This will
give $D_{1}' = D_{3}''$. Again we may imagine a system of coördinates
which at and near the first configuration will coincide
with the second system of coördinates, and at and near the
\PageSep{12}
second configuration will coincide with the third system of
coördinates. This will give $D_{2}' = D_{3}''$. We have therefore
$D_{1}' = D_{2}'$.

It follows, or it may be proved in the same way, that the
value of an extension-in-phase is independent of the system
of coördinates which is used in its evaluation. This may
easily be verified directly. If $q_{1}$,~\dots $q_{n}$, $Q_{1}$,~\dots $Q_{n}$ are two
systems of coördinates, and $p_{1}$,~\dots $p_{n}$, $P_{1}$,~\dots $P_{n}$ the corresponding
momenta, we have to prove that
\[
\ints dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n} = \ints dP_{1} \cdots dP_{n}\, dQ_{1} \cdots dQ_{n},
\Tag{(24)}
\]
when the multiple integrals are taken within limits consisting
of the same phases. And this will be evident from the principle
on which we change the variables in a multiple integral,
if we prove that
\[
\frac{d(P_{1}, \dots P_{n}, Q_{1}, \dots Q_{n})}{d(p_{1}, \dots p_{n}, q_{1}, \dots q_{n})} = 1,
\Tag{(25)}
\]
where the first member of the equation represents a Jacobian
or functional determinant. Since all its elements of the form
$dQ/dp$ are equal to zero, the determinant reduces to a product
of two, and we have to prove that
\[
\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}\,
\frac{d(Q_{1}, \dots Q_{n})}{d(q_{1}, \dots q_{n})} = 1.
\Tag{(26)}
\]
We may transform any element of the first of these determinants
as follows. By equations \Eq{(2)} and~\Eq{(3)}, and in
view of the fact that the~$\dot{Q}$'s are linear functions of the~$\dot{q}$'s
and therefore of the~$p$'s, with coefficients involving the~$q$'s,
so that a differential coefficient of the form $d\dot{Q}_{r}/dp_{y}$ is function
of the $q$'s~alone, we get\footnote
  {The form of the equation
  \[
  \frac{d}{dp_{y}}\, \frac{d\eps_{p}}{d\dot{Q}_{x}} = \frac{d}{d\dot{Q}_{x}}\, \frac{d\eps_{p}}{dp_{y}}
  \]
  in~\Eq{(27)} reminds us of the fundamental identity in the differential calculus
  relating to the order of differentiation with respect to independent variables.
  But it will be observed that here the variables $\dot{Q}_{x}$ and $p_{y}$ are not independent
  and that the proof depends on the \emph{linear} relation between the~$\dot{Q}$'s and the~$p$'s.}%
\PageSep{13}
\begin{multline*}
\frac{dP_{x}}{dp_{y}} = \frac{d}{dp_{y}}\, \frac{d\eps_{p}}{d\dot{Q}_{x}}
  = \sum_{r=1}^{r=n} \left(\frac{d^{2}\eps_{p}}{d\dot{Q}_{r}\, d\dot{Q}_{x}}\, \frac{d\dot{Q}_{r}}{dp_{y}}\right) \\
  = \frac{d}{d\dot{Q}_{x}} \sum_{r=1}^{r=n} \left(\frac{d\eps_{p}}{d\dot{Q}_{r}}\, \frac{d\dot{Q}_{r}}{dp_{y}}\right)
  = \frac{d}{d\dot{Q}_{x}}\, \frac{d\eps_{p}}{dp_{y}}
  = \frac{d\dot{q}_{y}}{d\dot{Q}_{x}}.
\Tag{(27)}
\end{multline*}
But since
\begin{gather*}
\dot{q}_{y} = \sum_{r=1}^{r=n} \left(\frac{dq_{y}}{dQ_{r}}\, \dot{Q}_{r}\right), \\
\frac{d\dot{q}_{y}}{d\dot{Q}_{x}} = \frac{dq_{y}}{dQ_{x}}.
\Tag{(28)}
\end{gather*}
Therefore,
\[
\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}
  = \frac{d(\dot{q}_{1}, \dots \dot{q}_{n})}{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}
  = \frac{d(q_{1}, \dots q_{n})}{d(Q_{1}, \dots Q_{n})}.
\Tag{(29)}
\]
The equation to be proved is thus reduced to
\[
\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}\,
\frac{d(Q_{1}, \dots Q_{n})}{d(q_{1}, \dots q_{n})} = 1,
\Tag{(30)}
\]
which is easily proved by the ordinary rule for the multiplication
of determinants.

The numerical value of an extension-in-phase will however
depend on the units in which we measure energy and time.
For a product of the form $dp\, dq$ has the dimensions of energy
multiplied by time, as appears from equation~\Eq{(2)}, by which
the momenta are defined. Hence an extension-in-phase has
the dimensions of the $n$th~power of the product of energy
and time. In other words, it has the dimensions of the $n$th~power
of \emph{action}, as the term is used in the `principle of \emph{Least
Action}.'

If we distinguish by accents the values of the momenta
and coördinates which belong to a time~$t'$, the unaccented
letters relating to the time~$t$, the principle of the conservation
of extension-in-phase may be written
\[
\ints dp_{1} \cdots dp_{n}\, dq_{1} \cdots dq_{n} = \ints dp_{1}' \cdots dp_{n}'\, dq_{1}' \cdots dq_{n}',
\Tag{(31)}
\]
or more briefly
\[
\ints dp_{1} \cdots dq_{n} = \ints dp_{1}' \cdots dq_{n}',
\Tag{(32)}
\]
\PageSep{14}
the limiting phases being those which belong to the same
systems at the times $t$~and $t'$~respectively. But we have
identically
\[
\ints dp_{1} \cdots dq_{n}
  = \ints \frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')}\, dp_{1}' \cdots dq_{n}'
\]
for such limits. The principle of conservation of extension-in-phase
may therefore be expressed in the form
\[
\frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')} = 1.
\Tag{(33)}
\]
This equation is easily proved directly. For we have
identically
\[
\frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')}
  = \frac{d(p_{1}, \dots q_{n})}{d(p_{1}'', \dots q_{n}'')}
  \, \frac{d(p_{1}'', \dots q_{n}'')}{d(p_{1}', \dots q_{n}')},
\]
where the double accents distinguish the values of the momenta
and coördinates for a time~$t''$. If we vary~$t$, while $t'$~and $t''$
remain constant, we have
\[
\frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')}
  = \frac{d(p_{1}'', \dots q_{n}'')}{d(p_{1}', \dots q_{n}')}\,
  \frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(p_{1}'', \dots q_{n}'')}.
\Tag{(34)}
\]
Now since the time~$t''$ is entirely arbitrary, nothing prevents
us from making $t''$~identical with~$t$ at the moment considered.
Then the determinant
\[
\frac{d(p_{1}, \dots q_{n})}{d(p_{1}'', \dots q_{n}'')}
\]
will have unity for each of the elements on the principal
diagonal, and zero for all the other elements. Since every
term of the determinant except the product of the elements
on the principal diagonal will have two zero factors, the differential
of the determinant will reduce to that of the product of
these elements, \ie, to the sum of the differentials of these
elements. This gives the equation
\[
\frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(p_{1}'', \dots q_{n}'')}
  = \frac{d\dot{p}_{1}}{dp_{1}''} \Add{+} \cdots + \frac{d\dot{p}_{n}}{dp_{n}''}
  + \frac{d\dot{q}_{1}}{dq_{1}''} \Add{+} \cdots + \frac{d\dot{q}_{n}}{dq_{n}''}.
\]
Now since $t = t''$, the double accents in the second member
of this equation may evidently be neglected. This will give,
in virtue of such relations as~\Eq{(16)},
\PageSep{15}
\[
\frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(p_{1}'', \dots q_{n}'')} = 0,
\]
which substituted in~\Eq{(34)} will give
\[
\frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')} = 0.
\]
The determinant in this equation is therefore a constant, the
value of which may be determined at the instant when $t = t'$,
when it is evidently unity. Equation~\Eq{(33)} is therefore
demonstrated.

Again, if we write $a$,~\dots $h$ for a system of $2n$~arbitrary constants
of the integral equations of motion, $p_{1}$, $q_{1}$, etc.\ will be
functions of $a$,~\dots $h$, and~$t$, and we may express an extension-in-phase
in the form
\[
\ints \frac{d(p_{1}, \dots q_{n})}{d(a, \dots, h)}\, da \cdots dh.
\Tag{(35)}
\]
If we suppose the limits specified by values of $a$,~\dots $h$, a
system initially at the limits will remain at the limits.
The principle of conservation of extension-in-phase requires
that an extension thus bounded shall have a constant value.
This requires that the determinant under the integral sign
shall be constant, which may be written
\[
\frac{d}{dt}\, \frac{d(p_{1}, \dots q_{n})}{d(a, \dots, h)} = 0.
\Tag{(36)}
\]
This equation, which may be regarded as expressing the principle
of conservation of extension-in-phase, may be derived
directly from the identity
\[
\frac{d(p_{1}, \dots q_{n})}{d(a, \dots, h)}
  = \frac{d(p_{1}, \dots q_{n})}{d(p_{1}', \dots q_{n}')}
  \, \frac{d(p_{1}', \dots q_{n}')}{d(a, \dots, h)}
\]
in connection with equation~\Eq{(33)}.

Since the coördinates and momenta are functions of $a$,~\dots $h$,
and~$t$, the determinant in~\Eq{(36)} must be a function of the same
variables, and since it does not vary with the time, it must
be a function of $a$,~\dots $h$ alone. We have therefore
\[
\frac{d(p_{1}, \dots q_{n})}{d(a, \dots, h)} = \func(a, \dots h).
\Tag{(37)}
\]
\PageSep{16}

It is the relative numbers of systems which fall within different
limits, rather than the absolute numbers, with which we
are most concerned. It is indeed only with regard to relative
numbers that such discussions as the preceding will apply
with literal precision, since the nature of our reasoning implies
that the number of systems in the smallest element of space
which we consider is very great. This is evidently inconsistent
with a finite value of the total number of systems, or of
the density-in-phase. Now if the value of~$D$ is infinite, we
cannot speak of any definite number of systems within any
finite limits, since all such numbers are infinite. But the
ratios of these infinite numbers may be perfectly definite. If
we write~$N$ for the total number of systems, and set
\[
P = \frac{D}{N},
\Tag{(38)}
\]
$P$~may remain finite, when $N$~and $D$ become infinite. The
integral
\[
\ints P\, dp_{1} \dots dq_{n}
\Tag{(39)}
\]
taken within any given limits, will evidently express the ratio
of the number of systems falling within those limits to the
whole number of systems. This is the same thing as the
\emph{probability} that an unspecified system of the ensemble (\ie\
one of which we only know that it belongs to the ensemble)
will lie within the given limits. The product
\[
P\, dp_{1} \dots dq_{n}
\Tag{(40)}
\]
expresses the probability that an unspecified system of the
ensemble will be found in the element of extension-in-phase
$dp_{1} \dots dq_{n}$. We shall call~$P$ the \emph{coefficient of probability} of the
phase considered. Its natural logarithm we shall call the
\emph{index of probability} of the phase, and denote it by the letter~$\eta$.

If we substitute $NP$ and~$Ne^{\eta}$ for~$D$ in equation~\Eq{(19)}, we get
\[
\left(\frac{dP}{dt}\right)_{p,q} = -\sum \left(\frac{dP}{dp_{1}}\, \dot{p}_{1} + \frac{dP}{dq_{1}}\, \dot{q}_{1}\right),
\Tag{(41)}
\]
and
\[
\left(\frac{d\eta}{dt}\right)_{p,q} = -\sum \left(\frac{d\eta}{dp_{1}}\, \dot{p}_{1} + \frac{d\eta}{dq_{1}}\, \dot{q}_{1}\right).
\Tag{(42)}
\]
\PageSep{17}
The condition of statistical equilibrium may be expressed
by equating to zero the second member of either of these
equations.

The same substitutions in~\Eq{(22)} give
\[
\left(\frac{dP}{dt}\right)_{a, \dots h} = 0,
\Tag{(43)}
\]
and
\[
\left(\frac{d\eta}{dt}\right)_{a, \dots h} = 0.
\Tag{(44)}
\]
That is, the values of $P$ and~$\eta$, like those of~$D$, are constant
in time for moving systems of the ensemble. From this point
of view, the principle which otherwise regarded has been
called the principle of conservation of density-in-phase or
conservation of extension-in-phase, may be called the principle
of conservation of the coefficient (or index) of probability
of a phase varying according to dynamical laws, or
more briefly, the principle of \emph{conservation of probability of
phase}. It is subject to the limitation that the forces must be
functions of the coördinates of the system either alone or with
the time.

The application of this principle is not limited to cases in
which there is a formal and explicit reference to an ensemble of
systems. Yet the conception of such an ensemble may serve
to give precision to notions of probability. It is in fact customary
in the discussion of probabilities to describe anything
which is imperfectly known as something taken at random
from a great number of things which are completely described.
But if we prefer to avoid any reference to an ensemble
of systems, we may observe that the probability that the
phase of a system falls within certain limits at a certain time,
is equal to the probability that at some other time the phase
will fall within the limits formed by phases corresponding to
the first. For either occurrence necessitates the other. That
is, if we write~$P'$ for the coefficient of probability of the
phase $p_{1}'$,~\dots $q_{n}'$ at the time~$t'$, and $P''$~for that of the phase
$p_{1}''$,~\dots $q_{n}''$ at the time~$t''$,
\PageSep{18}
\[
\ints P'\, dp_{1}' \cdots dq_{n}'
  = \ints P''\, dp_{1}'' \cdots dq_{n}'',
\Tag{(45)}
\]
where the limits in the two cases are formed by corresponding
phases. When the integrations cover infinitely small variations
of the momenta and coördinates, we may regard $P'$ and
$P''$ as constant in the integrations and write
\[
P' \ints dp_{1}' \cdots \Typo{dq_{n}''}{dq_{n}'}
  = P'' \ints dp_{1}'' \cdots dq_{n}''.
\]
Now the principle of the conservation of extension-in-phase,
which has been proved (viz., in the second demonstration given
above) independently of any reference to an ensemble of
systems, requires that the values of the multiple integrals in
this equation shall be equal. This gives
\[
P'' = P'.
\]

With reference to an important class of cases this principle
may be enunciated as follows.

\emph{When the differential equations of motion are exactly known,
but the constants of the integral equations imperfectly determined,
the coefficient of probability of any phase at any time is
equal to the coefficient of probability of the corresponding phase
at any other time.} By corresponding phases are meant those
which are calculated for different times from the same values
of the arbitrary constants of the integral equations.

Since the sum of the probabilities of all possible cases is
necessarily unity, it is evident that we must have
\[
\intap P\, dp_{1} \cdots dq_{n} = 1,
\Tag{(46)}
\]
where the integration extends over all phases. This is indeed
only a different form of the equation
\[
N = \intap D\, dp_{1} \cdots dq_{n},
\]
which we may regard as defining~$N$.
\PageSep{19}

The values of the coefficient and index of probability of
phase, like that of the density-in-phase, are independent of the
system of coördinates which is employed to express the distribution
in phase of a given ensemble.

In dimensions, the coefficient of probability is the reciprocal
of an extension-in-phase, that is, the reciprocal of the $n$th~power
of the product of time and energy. The index of probability
is therefore affected by an additive constant when we
change our units of time and energy. If the unit of time is
multiplied by~$c_{t}$ and the unit of energy is multiplied by~$c_{\eps}$, all
indices of probability relating to systems of $n$~degrees of
freedom will be increased by the addition of
\[
n \log c_{t} + n \log c_{\eps}.
\Tag{(47)}
\]
\PageSep{20}


\Chapter{II.}{Application of the Principle of Conservation
of Extension-in-Phase to the Theory
of Errors.}

\First{Let} us now proceed to combine the principle which has been
demonstrated in the preceding chapter and which in its different
applications and regarded from different points of view
has been variously designated as the conservation of density-in-phase,
or of extension-in-phase, or of probability of phase,
with those approximate relations which are generally used in
the `theory of errors.'

We suppose that the differential equations of the motion of
a system are exactly known, but that the constants of the
integral equations are only approximately determined. It is
evident that the probability that the momenta and coördinates
at the time~$t'$ fall between the limits~$p_{1}'$ and $p_{1}' + dp_{1}'$, $q_{1}'$~and
$q_{1}' + dq_{1}'$, etc., may be expressed by the formula
\[
e^{\eta'}\, dp_{1}' \dots dq_{n}',
\Tag{(48)}
\]
where $\eta'$ (the index of probability for the phase in question) is
a function of the coördinates and momenta and of the time.

Let $Q_{1}'$, $P_{1}'$, etc.\ be the values of the coördinates and momenta
which give the maximum value to~$\eta'$, and let the general
value of~$\eta'$ be developed by Taylor's theorem according to
ascending powers and products of the differences $p_{1}' - P_{1}'$,
$q_{1}' - Q_{1}'$, etc., and let us suppose that we have a sufficient
approximation without going beyond terms of the second
degree in these differences. We may therefore set
\[
\eta' = c - F',
\Tag{(49)}
\]
where $c$~is independent of the differences $p_{1}' - P_{1}'$, $q_{1}' - Q_{1}'$,
etc., and $F'$~is a homogeneous quadratic function of these
\PageSep{21}
differences. The terms of the first degree vanish in virtue
of the maximum condition, which also requires that $F'$~must
have a positive value except when all the differences mentioned
vanish. If we set
\[
C = e^{c},
\Tag{(50)}
\]
we may write for the probability that the phase lies within
the limits considered
\[
C e^{-F'}\, dp_{1}' \dots dq_{n}'.
\Tag{(51)}
\]
$C$~is evidently the maximum value of the coefficient of probability
at the time considered.

In regard to the degree of approximation represented by
these \Chg{formulæ}{formulae}, it is to be observed that we suppose, as is
usual in the `theory of errors,' that the determination (explicit
or implicit) of the constants of motion is of such
precision that the coefficient of probability~$e^{\eta'}$ or $Ce^{-F'}$ is
practically zero except for very small values of the differences
$p_{1}' - P_{1}'$, $q_{1}' - Q_{1}'$, etc. For very small values of these
differences the approximation is evidently in general sufficient,
for larger values of these differences the value of $Ce^{-F'}$ will
be sensibly zero, as it should be, and in this sense the formula
will represent the facts.

We shall suppose that the forces to which the system is
subject are functions of the coördinates either alone or with
the time. The principle of conservation of probability of
phase will therefore apply, which requires that at any other
time~($t''$) the maximum value of the coefficient of probability
shall be the same as at the time~$t'$, and that the phase
($P_{1}''$, $Q_{1}''$, etc.)\ which has this greatest probability-coefficient,
shall be that which corresponds to the phase ($P_{1}'$, $Q_{1}'$, etc.),
\ie, which is calculated from the same values of the constants
of the integral equations of motion.

We may therefore write for the probability that the phase
at the time~$t''$ falls within the limits~$p_{1}''$ and $p_{1}'' + dp_{1}''$, $q_{1}''$~and
$q_{1}'' + dq_{1}''$, etc.,
\[
C e^{-F'}\, dp_{1}'' \dots dq_{n}'',
\Tag{(52)}
\]
\PageSep{22}
where $C$~represents the same value as in the preceding
formula, viz., the constant value of the maximum coefficient
of probability, and $F''$~is a quadratic function of the differences
$p_{1}'' - P_{1}''$, $q_{1} - Q_{1}''$, etc., the phase ($P_{1}''$, $Q_{1}''$, etc.)\ being that
which at the time~$t''$ corresponds to the phase ($P_{1}'$, $Q_{1}'$, etc.)\
at the time~$t'$.

Now we have necessarily
\[
\ints C e^{-F'}\, dp_{1}' \dots dq_{n}' = \ints C e^{-F''}\, dp_{1}'' \dots dq_{n}'' = 1,
\Tag{(53)}
\]
when the integration is extended over all possible phases.
It will be allowable to set~$\pm\infty$ for the limits of all the coördinates
and momenta, not because these values represent the
actual limits of possible phases, but because the portions of
the integrals lying outside of the limits of all possible phases
will have sensibly the value zero. With $\pm\infty$ for limits, the
equation gives
\[
\frac{C \pi^{n}}{\sqrt{f'}} = \frac{C \pi^{n}}{\sqrt{f''}} = 1,
\Tag{(54)}
\]
where $f'$~is the discriminant\footnote
  {This term is used to denote the determinant having for elements on the
  principal diagonal the coefficients of the squares in the quadratic function~$F'$,
  and for its other elements the halves of the coefficients of the products
  in~$F'$.}
of~$F'$, and $f''$~that of~$F''$. This
discriminant is therefore constant in time, and like~$C$ an absolute
invariant in respect to the system of coördinates which
may be employed. In dimensions, like~$C^{2}$, it is the reciprocal
of the \Typo{2n}{$2n$}th~power of the product of energy and time.

Let us see precisely how the functions $F''$~and $F'$ are related.
The principle of the conservation of the probability-coefficient
requires that any values of the coördinates and momenta at the
time~$t'$ shall give the function~$F'$ the same value as the corresponding
coördinates and momenta at the time~$t''$ give to~$F''$.
Therefore $F''$~may be derived from~$F'$ by substituting for
$p_{1}'$,~\dots $q_{n}'$ their values in terms of $p_{1}''$,~\dots $q_{n}''$. Now we
have approximately
\PageSep{23}
\[
\left.
\begin{aligned}
p_{1}' - P_{1}' &= \frac{dP_{1}'}{dP_{1}''} (p_{1}'' - P_{1}'') \Add{+} \cdots + \frac{dP_{1}'}{dQ_{n}''} (q_{n}'' - Q_{n}'')\Add{,} \\
 \dots & \dots \\
q_{n}' - Q_{n}' &= \frac{dQ_{n}'}{dP_{1}''} (p_{1}'' - P_{1}'') \Add{+} \cdots + \frac{dQ_{n}'}{dQ_{n}''} (q_{n}'' - Q_{n}''),
\end{aligned}
\right\}
\Tag{(55)}
\]
and as in~$F''$ terms of higher degree than the second are to be
neglected, these equations may be considered accurate for the
purpose of the transformation required. Since by equation~\Eq{(33)}
the eliminant of these equations has the value unity,
the discriminant of~$F''$ will be equal to that of~$F'$, as has
already appeared from the consideration of the principle of
conservation of probability of phase, which is, in fact, essentially
the same as that expressed by equation~\Eq{(33)}.

At the time~$t'$, the phases satisfying the equation
\[
F' = k,
\Tag{(56)}
\]
where $k$~is any positive constant, have the probability-coefficient
$C e^{-k}$. At the time~$t''$, the corresponding phases satisfy
the equation
\[
F'' = k,
\Tag{(57)}
\]
and have the same probability-coefficient. So also the phases
within the limits given by one or the other of these equations
are corresponding phases, and have probability-coefficients
greater than $Ce^{-k}$, while phases without these limits have less
probability-coefficients. The probability that the phase at
the time~$t'$ falls within the limits $F'' = k$ is the same as the
probability that it falls within the limits $F'' = k$ at the time~$t''$,
since either event necessitates the other. This probability
may be evaluated as follows. We may omit the accents, as
we need only consider a single time. Let us denote the extension-in-phase
within the limits $F = k$ by~$U$, and the probability
that the phase falls within these limits by~$R$, also the
extension-in-phase within the limits $F = 1$ by~$U_{1}$. We have
then by definition
\[
U = \intslim{F = k} dp_{1} \dots dq_{n},
\Tag{(58)}
\]
\PageSep{24}
\begin{gather*}
R = \intslim{F = k} C e^{-F}\, dp_{1} \dots dq_{n},
\Tag{(59)} \displaybreak[0] \\
U_{1} = \intslim{F = 1} dp_{1} \dots dq_{n}.
\Tag{(60)}
\end{gather*}
But since $F$~is a homogeneous quadratic function of the
differences
\[
p_{1} - P_{1},\quad
p_{2} - P_{2},\quad \dots\Add{,\quad}
q_{n} - Q_{n},
\]
we have identically
\begin{multline*}
\intslim{F = k} d(p_{1} - P_{1}) \dots d(q_{n} - Q_{n}) \\
\begin{aligned}
  &= \intslim{kF = k} k^{n}\, d(p_{1} - P_{1}) \dots d(q_{n} - Q_{n}) \\
  &= k^{n} \intslim{F = 1} d(p_{1} - P_{1}) \dots d(q_{n} - Q_{n}).
\end{aligned}
\end{multline*}
That is
\[
U = k^{n} U_{1},
\Tag{(61)}
\]
whence
\[
dU = U_{1} n k^{n-1}\, dk.
\Tag{(62)}
\]
But if $k$~varies, equations \Eq{(58)} and~\Eq{(59)} give
\begin{align*}
dU &= \intslim[F=k]{F = k+dk} dp_{1} \dots dq_{n}\Add{,}
\Tag{(63)} \\
dR &= \intslim[F=k]{F = k+dk} C e^{-F}\, dp_{1} \dots dq_{n}\Add{.}
\Tag{(64)}
\end{align*}

Since the factor $Ce^{-F}$ has the constant value~$Ce^{-k}$ in the
last multiple integral, we have
\begin{gather*}
dR = C e^{-k}\, dU = C U_{1} n e^{-k} k^{n-1}\, dk,
\Tag{(65)} \displaybreak[0] \\
R = -C U_{1} \Fact[]{n} e^{-k} \left(1 + k + \frac{k^{2}}{2} + \cdots + \frac{k^{n-1}}{\Fact{n - 1}}\right) + \const.
\Tag{(66)}
\end{gather*}
We may determine the constant of integration by the condition
that $R$~vanishes with~$k$. This gives
\PageSep{25}
\[
R = CU_{1} \Fact[]{n} - CU_{1} \Fact[]{n} e^{-k} \left(1 + k + \frac{k^{2}}{2} + \cdots + \frac{k^{n-1}}{\Fact{n - 1}}\right).
\Tag{(67)}
\]
We may determine the value of the constant~$U_{1}$ by the condition
that $R = 1$ for $k = \infty$. This gives $CU_{1} \Fact[]{n} = 1$, and
\begin{gather*}
R = 1 - e^{-k} \left(1 + k + \frac{k^{2}}{2} + \cdots + \frac{k^{n-1}}{\Fact{n - 1}}\right),
\Tag{(68)} \\
U = \frac{k^{n}}{C\Fact[]{n}}.
\Tag{(69)}
\end{gather*}

It is worthy of notice that the form of these equations depends
only on the number of degrees of freedom of the system,
being in other respects independent of its dynamical nature,
except that the forces must be functions of the coördinates
either alone or with the time.

If we write
\[
k_{E = \frac{1}{2}}
\]
for the value of~$k$ which substituted in equation~\Eq{(68)} will give
$R = \frac{1}{2}$, the phases determined by the equation
\[
F = k_{E = \frac{1}{2}}
\Tag{(70)}
\]
will have the following properties.

The probability that the phase falls within the limits formed
by these phases is greater than the probability that it falls
within any other limits enclosing an equal extension-in-phase.
It is equal to the probability that the phase falls without the
same limits.

These properties are analogous to those which in the theory
of errors in the determination of a single quantity belong to
values expressed by $A \pm a$, when $A$~is the most probable
value, and $a$~the `probable error.'
\PageSep{26}


\Chapter[Application of the Principle of Conservation of Extension-in-Phase to the Integration of the Differential Equations of Motion.]{III.}{Application of the Principle of Conservation of
Extension-in-Phase to the Integration of the
Differential Equations of Motion.\protect\footnotemark}

\footnotetext{See Boltzmann: ``Zusammenhang zwischen den Sätzen über das Verhalten
  mehratomiger \Typo{Gasmolecüle}{Gasmoleküle} mit Jacobi's Princip des letzten Multiplicators.\Typo{}{''}
  Sitzb.\ der Wiener Akad., Bd.~LXIII, Abth.~II., S.~679, (1871).}%

\First{We} have seen that the principle of conservation of extension-in-phase
may be expressed as a differential relation between
the coördinates and momenta and the arbitrary constants
of the integral equations of motion. Now the integration of
the differential equations of motion consists in the determination
of these constants as functions of the coördinates and
momenta with the time, and the relation afforded by the principle
of conservation of extension-in-phase may assist us in
this determination.

It will be convenient to have a notation which shall not distinguish
between the coördinates and momenta. If we write
$r_{1}$\Add{,}~\dots $r_{2n}$ for the coördinates and momenta, and $a$\Add{,}~\dots $h$ as before
for the arbitrary constants, the principle of which we
wish to avail ourselves, and which is expressed by equation~\Eq{(37)},
may be written
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)} = \func (a, \dots h).
\Tag{(71)}
\]

Let us first consider the case in which the forces are determined
by the coördinates alone. Whether the forces are
`conservative' or not is immaterial. Since the differential
equations of motion do not contain the time~($t$) in the finite
form, if we eliminate~$dt$ from these equations, we obtain $2n - 1$
equations in $r_{1}$,~\dots $r_{2n}$ and their differentials, the integration
of which will introduce $2n - 1$ arbitrary constants which we
shall call $b$\Add{,}~\dots $h$. If we can effect these integrations, the
\PageSep{27}
remaining constant~($a$) will then be introduced in the final
integration, (viz., that of an equation containing~$dt$,) and will
be added to or subtracted from~$t$ in the integral equation.
Let us have it subtracted from~$t$. It is evident then that
\[
\frac{dr_{1}}{da} = -\dot{r}_{1},\quad
\frac{dr_{2}}{da} = -\dot{r}_{2},\quad \text{etc.}
\Tag{(72)}
\]

Moreover, since $b$,~\dots $h$ and $t - a$ are independent functions
of $r_{1}$,~\dots $r_{2n}$, the latter variables are functions of the former.
The Jacobian in~\Eq{(71)} is therefore function of $b$,~\dots $h$, and
$t - a$, and since it does not vary with~$t$ it cannot vary with~$a$.
We have therefore in the case considered, viz., where the
forces are functions of the coördinates alone,
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)} = \func (b, \dots h).
\Tag{(73)}
\]

Now let us suppose that of the first $2n - 1$ integrations we
have accomplished all but one, determining $2n - 2$ arbitrary
constants (say $c$,~\dots $h$) as functions of $r_{1}$,~\dots $r_{2n}$, leaving $b$ as
well as~$a$ to be determined. Our $2n - 2$ finite equations enable
us to regard all the variables $r_{1}$,~\dots $r_{2n}$, and all functions
of these variables as functions of two of them, (say $r_{1}$ and~$r_{2}$,)
with the arbitrary constants $c$,~\dots $h$. To determine~$b$, we
have the following equations for constant values of $c$,~\dots $h$.
\begin{align*}
dr_{1} &= \frac{dr_{1}}{da}\, da + \frac{dr_{1}}{db}\, db, \\
dr_{2} &= \frac{dr_{2}}{da}\, da + \frac{dr_{2}}{db}\, db, \\
\intertext{whence}
\frac{d(r_{1}, r_{2})}{d(a, b)} &= -\frac{dr_{2}}{da}\, dr_{1} + \frac{dr_{1}}{da}\, dr_{2}.
\Tag{(74)}
\end{align*}
Now, by the ordinary formula for the change of variables,
\begin{multline*}
\ints \frac{d(r_{1}, r_{2})}{d(a, b)}\, da\, db\, dr_{3} \dots dr_{2n}
  = \ints dr_{1} \dots dr_{2n} \\
  \begin{aligned}
    &= \ints \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, da \dots dh \\
    &= \ints \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, \frac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}\, da\, db\, dr_{3} \dots dr_{2n},
\end{aligned}
\end{multline*}
\PageSep{28}
where the limits of the multiple integrals are formed by the
same phases. Hence
\[
\frac{d(r_{1}, r_{2})}{d(a, b)} = \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, \frac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}.
\Tag{(75)}
\]
With the aid of this equation, which is an identity, and~\Eq{(72)},
we may write equation~\Eq{(74)} in the form
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, \frac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}\, db = \dot{r}_{2}\, dr_{1} - \dot{r}_{1}\, dr_{2}.
\Tag{(76)}
\]

The separation of the variables is now easy. The differential
equations of motion give $\dot{r}_{1}$~and $\dot{r}_{2}$ in terms of $r_{1}$,~\dots $r_{2n}$.
The integral equations already obtained give $c$,~\dots $h$ and
therefore the Jacobian $d(c, \dots h)/d(r_{3}, \dots r_{2n})$, in terms of
the same variables. But in virtue of these same integral
equations, we may regard functions of $r_{1}$,~\dots $r_{2n}$ as functions
of $r_{1}$~and $r_{2}$ with the constants $c$,~\dots $h$. If therefore we write
the equation in the form
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, db
  = \frac{\dot{r}_{2}}{\dfrac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}}\, dr_{1}
  - \frac{\dot{r}_{1}}{\dfrac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}}\, dr_{2},
\Tag{(77)}
\]
the coefficients of $dr_{1}$~and $dr_{2}$ may be regarded as known functions
of $r_{1}$~and $r_{2}$ with the constants $c$,~\dots $h$. The coefficient
of~$db$ is by~\Eq{(73)} a function of $b$,~\dots $h$. It is not indeed a
known function of these quantities, but since $c$,~\dots $h$ are
regarded as constant in the equation, we know that the first
member must represent the differential of some function of
$b$,~\dots $h$, for which we may write~$b'$. We have thus
\[
db' = \frac{\dot{r}_{2}}{\dfrac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}}\, dr_{1}
    - \frac{\dot{r}_{1}}{\dfrac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}}\, dr_{2},
\Tag{(78)}
\]
which may be integrated by quadratures and gives $b''$~as functions
of $r_{1}$, $r_{2}$,~\dots $c$,~\dots $h$, and thus as function of $r_{1}$,~\dots $r_{2n}$.

This integration gives us the last of the arbitrary constants
which are functions of the coördinates and momenta without
the time. The final integration, which introduces the remaining
\PageSep{29}
constant~($a$), is also a quadrature, since the equation to
be integrated may be expressed in the form
\[
dt = F(r_{1})\, dr_{1}.
\]

Now, apart from any such considerations as have been adduced,
if we limit ourselves to the changes which take place
in time, we have identically
\[
\dot{r}_{2}\, dr_{1} - \dot{r}_{1}\, dr_{2} = 0,
\]
and $\dot{r}_{1}$~and $\dot{r}_{2}$ are given in terms of $r_{1}$,~\dots $r_{2n}$ by the differential
equations of motion. When we have obtained $2n - 2$ integral
equations, we may regard $\dot{r}_{2}$~and $\dot{r}_{1}$ as known functions of $r_{1}$~and
$r_{2}$. The only remaining difficulty is in integrating this
equation. If the case is so simple as to present no difficulty,
or if we have the skill or the good fortune to perceive that the
multiplier
\[
\frac{1}{\dfrac{d(c, \dots h)}{d(r_{3}, \dots r_{2n})}},
\Tag{(79)}
\]
or any other, will make the first member of the equation an
exact differential, we have no need of the rather lengthy considerations
which have been adduced. The utility of the
principle of conservation of extension-in-phase is that it supplies
a `multiplier' which renders the equation integrable, and
which it might be difficult or impossible to find otherwise.

It will be observed that the function represented by~$b'$ is a
particular case of that represented by~$b$. The system of arbitrary
constants $a$, $b'$, $c$,~\dots $h$ has certain properties notable for
simplicity. If we write $b'$~for $b$ in~\Eq{(77)}, and compare the
result with~\Eq{(78)}, we get
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, b', c, \dots h)} = 1.
\Tag{(80)}
\]
Therefore the multiple integral
\[
\ints da\, db'\, dc \dots dh
\Tag{(81)}
\]
\PageSep{30}
taken within limits formed by phases regarded as contemporaneous
represents the extension-in-phase within those limits.

The case is somewhat different when the forces are not determined
by the coördinates alone, but are functions of the
coördinates with the time. All the arbitrary constants of the
integral equations must then be regarded in the general case
as functions of $r_{1}$,~\dots $r_{2n}$, and~$t$. We cannot use the principle
of conservation of extension-in-phase until we have made
$2n - 1$ integrations. Let us suppose that the constants $b$,~\dots $h$
have been determined by integration in terms of $r_{1}$,~\dots $r_{2n}$, and~$t$,
leaving a single constant~($a$) to be thus determined. Our
$2n - 1$ finite equations enable us to regard all the variables
$r_{1}$,~\dots $r_{2n}$ as functions of a single one, say~$r_{1}$.

For constant values of $b$,~\dots $h$, we have
\[
dr_{1} = \frac{dr_{1}}{da}\, da + \dot{r}_{1}\, dt.
\Tag{(82)}
\]
Now
\begin{multline*}
\ints \frac{dr_{1}}{da}\, da\, dr_{2} \dots dr_{2n} = \ints dr_{1} \dots dr_{2n} \\
\begin{aligned}
  &= \ints \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, da \dots dh \\
  &= \ints \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, \frac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}\, da\, dr_{2} \dots dr_{2n},
\end{aligned}
\end{multline*}
where the limits of the integrals are formed by the same
phases. We have therefore
\[
\frac{dr_{1}}{da}
  = \frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, \frac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})},
\Tag{(83)}
\]
by which equation~\Eq{(82)} may be reduced to the form
\[
\frac{d(r_{1}, \dots r_{2n})}{d(a, \dots h)}\, da
  = \frac{1}{\dfrac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}}\, dr_{1}
  - \frac{\dot{r}_{1}}{\dfrac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}}\, dt.
\Tag{(84)}
\]

Now we know by~\Eq{(71)} that the coefficient of~$da$ is a function
of $a$,~\dots $h$. Therefore, as $b$,~\dots $h$ are regarded as constant
in the equation, the first number represents the differential
\PageSep{31}
of a function of $a$,~\dots $h$, which we may denote by~$a'$. We
have then
\[
da' = \frac{1}{\dfrac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}}\, dr_{1}
    - \frac{\dot{r}_{1}}{\dfrac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}}\, dt,
\Tag{(85)}
\]
which may be integrated by quadratures. In this case we
may say that the principle of conservation of extension-in-phase
has supplied the `multiplier'
\[
\frac{1}{\dfrac{d(b, \dots h)}{d(r_{2}, \dots r_{2n})}}\, dr_{1}
\Tag{(86)}
\]
for the integration of the equation
\[
dr_{1} - \dot{r}_{1}\, dt = 0.
\Tag{(87)}
\]

The system of arbitrary constants $a'$, $b$,~\dots $h$ has evidently
the same properties which were noticed in regard to the
system $a$, $b'$,~\dots $h$.
\PageSep{32}


\Chapter{IV.}{On the Distribution in Phase Called Canonical,
in which the Index of Probability is a Linear
Function of the Energy.}

\First{Let} us now give our attention to the statistical equilibrium
of ensembles of conservation systems, especially to those cases
and properties which promise to throw light on the phenomena
of thermodynamics.

The condition of statistical equilibrium may be expressed
in the form\footnote
  {See equations \Eq{(20)}, \Eq{(41)}, \Eq{(42)}, also the paragraph following equation~\Eq{(20)}.
  The positions of any external bodies which can affect the systems are here
  supposed uniform for all the systems and constant in time.}%
\[
\sum \left(\frac{dP}{dp_{1}}\, \dot{p}_{1} + \frac{dP}{dq_{1}}\, \dot{q}_{1}\right) = 0,
\Tag{(88)}
\]
where $P$~is the coefficient of probability, or the quotient of
the density-in-phase by the whole number of systems. To
satisfy this condition, it is necessary and sufficient that $P$~should
be a function of the $p$'s and~$q$'s (the momenta and
coördinates) which does not vary with the time in a moving
system. In all cases which we are now considering, the
energy, or any function of the energy, is such a function.
\[
P = \func(\eps)
\]
will therefore satisfy the equation, as indeed appears identically
if we write it in the form
\[
\sum \left(\frac{dP}{dp_{1}}\, \frac{d\eps}{dp_{1}} - \frac{dP}{dq_{1}}\, \frac{d\eps}{dq_{1}}\right) = 0.
\]

There are, however, other conditions to which $P$~is subject,
which are not so much conditions of statistical equilibrium, as
conditions implicitly involved in the definition of the coefficient
\PageSep{33}
of probability, whether the case is one of equilibrium
or not. These are: that $P$~should be single-valued, and
neither negative nor imaginary for any phase, and that expressed
by equation~\Eq{(46)}, viz.,
\[
\intap P\, dp_{1} \dots dq_{n} =1.
\Tag{(89)}
\]
These considerations exclude
\[
P = \eps \times \constant,
\]
as well as
\[
P = \constant,
\]
as cases to be considered.

The distribution represented by
\[
\eta = \log P = \frac{\psi - \eps}{\Theta},
\Tag{(90)}
\]
or
\[
P = e^{\efrac{\psi - \eps}{\Theta}},
\Tag{(91)}
\]
where $\Theta$~and $\psi$ are constants, and $\Theta$~positive, seems to represent
the most simple case conceivable, since it has the property
that when the system consists of parts with separate energies,
the laws of the distribution in phase of the separate parts are
of the same nature,---a property which enormously simplifies
the discussion, and is the foundation of extremely important
relations to thermodynamics. The case is not rendered less
simple by the divisor~$\Theta$, (a quantity of the same dimensions as~$\eps$,)
but the reverse, since it makes the distribution independent
of the units employed. The negative sign of~$\eps$ is required by~\Eq{(89)},
which determines also the value of~$\psi$ for any given~$\Theta$,
viz.,
\[
e^{-\efrac{\psi}{\Theta}} = \intap e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n}.
\Tag{(92)}
\]

When an ensemble of systems is distributed in phase in the
manner described, \ie, when the index of probability is a
\PageSep{34}
linear function of the energy, we shall say that the ensemble is
\emph{canonically distributed}, and shall call the divisor of the energy~($\Theta$)
the \emph{modulus} of distribution.

The fractional part of an ensemble canonically distributed
which lies within any given limits of phase is therefore represented
by the multiple integral
\[
\ints e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}
\Tag{(93)}
\]
taken within those limits. We may express the same thing
by saying that the multiple integral expresses the probability
that an unspecified system of the ensemble (\ie, one of
which we only know that it belongs to the ensemble) falls
within the given limits.

Since the value of a multiple integral of the form~\Eq{(23)}
(which we have called an extension-in-phase) bounded by any
given phases is independent of the system of coördinates by
which it is evaluated, the same must be true of the multiple
integral in~\Eq{(92)}, as appears at once if we divide up this
integral into parts so small that the exponential factor may be
regarded as constant in each. The value of~$\psi$ is therefore independent
of the system of coördinates employed.

It is evident that $\psi$~might be defined as the energy for
which the coefficient of probability of phase has the value
unity. Since however this coefficient has the dimensions of
the inverse $n$th~power of the product of energy and time,\footnote
  {See Chapter~I, p.~\Pageref{19}.}
the energy represented by~$\psi$ is not independent of the units
of energy and time. But when these units have been chosen,
the definition of~$\psi$ will involve the same arbitrary constant as~$\eps$,
so that, while in any given case the numerical values of~$\psi$
or~$\eps$ will be entirely indefinite until the zero of energy has
also been fixed for the system considered, the difference $\psi - \eps$
will represent a perfectly definite amount of energy, which is
entirely independent of the zero of energy which we may
choose to adopt.
\PageSep{35}

It is evident that the canonical distribution is entirely determined
by the modulus (considered as a quantity of energy)
and the nature of the system considered, since when equation~\Eq{(92)}
is satisfied the value of the multiple integral~\Eq{(93)} is
independent of the units and of the coördinates employed, and
of the zero chosen for the energy of the system.

In treating of the canonical distribution, we shall always
suppose the multiple integral in equation~\Eq{(92)} to have a
finite value, as otherwise the coefficient of probability vanishes,
and the law of distribution becomes illusory. This will
exclude certain cases, but not such apparently, as will affect
the value of our results with respect to their bearing on thermodynamics.
It will exclude, for instance, cases in which the
system or parts of it can be distributed in unlimited space
(or in a space which has limits, but is still infinite in volume),
while the energy remains beneath a finite limit. It also
excludes many cases in which the energy can decrease without
limit, as when the system contains material points which
attract one another inversely as the squares of their distances.
Cases of material points attracting each other inversely as the
distances would be excluded for some values of~$\Theta$, and not
for others. The investigation of such points is best left to
the particular cases. For the purposes of a general discussion,
it is sufficient to call attention to the assumption implicitly
involved in the formula~\Eq{(92)}.\footnote
  {It will be observed that similar limitations exist in thermodynamics. In
  order that a mass of gas can be in thermodynamic equilibrium, it is necessary
  that it be enclosed. There is no thermodynamic equilibrium of a (finite) mass
  of gas in an infinite space. Again, that two attracting particles should be
  able to do an infinite amount of work in passing from one configuration
  (which is regarded as possible) to another, is a notion which, although perfectly
  intelligible in a mathematical formula, is quite foreign to our ordinary
conceptions of matter.}%

The modulus~$\Theta$ has properties analogous to those of temperature
in thermodynamics. Let the system~$A$ be defined as
one of an ensemble of systems of $m$~degrees of freedom
distributed in phase with a probability-coefficient
\[
e^{\efrac{\psi_{A} - \eps_{A}}{\Theta}},
\]
\PageSep{36}
and the system~$B$ as one of an ensemble of systems of $n$~degrees
of freedom distributed in phase with a probability-coefficient
\[
e^{\efrac{\psi_{B} - \eps_{B}}{\Theta}},
\]
which has the same modulus. Let $q_{1}$,~\dots $q_{m}$, $p_{1}$,~\dots $p_{m}$ be the
coördinates and momenta of~$A$, and $q_{m+1}$,~\dots $q_{m+n}$, $p_{m+1}$,~\dots $p_{m+n}$
those of~$B$. Now we may regard the systems $A$~and $B$ as
together forming a system~$C$, having $m + n$~degrees of freedom,
and the coördinates and momenta $q_{1}$,~\dots $q_{m+n}$, $p_{1}$,~\dots $p_{m+n}$.
The probability that the phase of the system~$C$, as thus defined,
will fall within the limits
\[
dp_{1},\ \dots\ dp_{m+n},\ dq_{1},\ \dots\ dq_{m+n}
\]
is evidently the product of the probabilities that the systems
$A$~and $B$ will each fall within the specified limits, viz.,
\[
e^{\efrac{\psi_{A} + \psi_{B} - \eps_{A} - \eps_{B}}{\Theta}}\, dp_{1} \dots dp_{m+n}\, dq_{1} \dots dq_{m+n}.
\Tag{(94)}
\]
We may therefore regard~$C$ as an undetermined system of an
ensemble distributed with the probability-coefficient
\[
e^{\efrac{\psi_{A} + \psi_{B} - (\eps_{A} + \eps_{B})}{\Theta}},
\Tag{(95)}
\]
an ensemble which might be defined as formed by combining
each system of the first ensemble with each of the second.
But since $\eps_{A} + \eps_{B}$ is the energy of the whole system, and
$\psi_{A}$~and $\psi_{B}$ are constants, the probability-coefficient is of the
general form which we are considering, and the ensemble to
which it relates is in statistical equilibrium and is canonically
distributed.

This result, however, so far as statistical equilibrium is
concerned, is rather nugatory, since conceiving of separate
systems as forming a single system does not create any interaction
between them, and if the systems combined belong to
ensembles in statistical equilibrium, to say that the ensemble
formed by such combinations as we have supposed is in statistical
equilibrium, is only to repeat the data in different
\PageSep{37}
words. Let us therefore suppose that in forming the system~$C$
we add certain forces acting between $A$ and~$B$, and having
the force-function~$-\eps_{AB}$. The energy of the system~$C$ is now
$\eps_{A} + \eps_{B} + \eps_{AB}$, and an ensemble of such systems distributed
with a density proportional to
\[
e^{\efrac{-(\eps_{A} + \eps_{B} + \eps_{AB})}{\Theta}}
\Tag{(96)}
\]
would be in statistical equilibrium. Comparing this with the
probability-coefficient of~$C$ given above~\Eq{(95)}, we see that if
we suppose~$\eps_{AB}$ (or rather the variable part of this term when
we consider all possible configurations of the systems $A$ and~$B$)
to be infinitely small, the actual distribution in phase of~$C$
will differ infinitely little from one of statistical equilibrium,
which is equivalent to saying that its distribution in phase
will vary infinitely little even in a time indefinitely prolonged.\footnote
  {It will be observed that the above condition relating to the forces which
  act between the different systems is entirely analogous to that which must
  hold in the corresponding case in thermodynamics. The most simple test
  of the equality of temperature of two bodies is that they remain in equilibrium
  when brought into thermal contact. Direct thermal contact implies
  molecular forces acting between the bodies. Now the test will fail unless
  the energy of these forces can be neglected in comparison with the other
  energies of the bodies. Thus, in the case of energetic chemical action between
  the bodies, or when the number of particles affected by the forces
  acting between the bodies is not negligible in comparison with the whole
  number of particles (as when the bodies have the form of exceedingly thin
  sheets), the contact of bodies of the same temperature may produce considerable
  thermal disturbance, and thus fail to afford a reliable criterion of
  the equality of temperature.}
The case would be entirely different if $A$~and $B$ belonged to
ensembles having different moduli, say $\Theta_{A}$ and~$\Theta_{B}$. The probability-coefficient
of~$C$ would then be
\[
e^{\efrac{\psi_{A} - \eps_{A}}{\Theta_{A}} + \efrac{\psi_{B} - \eps_{B}}{\Theta_{B}}},
\Tag{(97)}
\]
which is not approximately proportional to any expression of
the form~\Eq{(96)}.

Before proceeding farther in the investigation of the distribution
in phase which we have called canonical, it will be
interesting to see whether the properties with respect to
\PageSep{38}
statistical equilibrium which have been described are peculiar
to it, or whether other distributions may have analogous
properties.

Let $\eta'$~and $\eta''$ be the indices of probability in two independent
ensembles which are each in statistical equilibrium, then
$\eta' + \eta''$ will be the index in the ensemble obtained by combining
each system of the first ensemble with each system of the
second. This third ensemble will of course be in statistical
equilibrium, and the function of phase $\eta' + \eta''$ will be a constant
of motion. Now when infinitesimal forces are added to
the compound systems, if $\eta' + \eta''$ or a function differing
infinitesimally from this is still a constant of motion, it must
be on account of the nature of the forces added, or if their action
is not entirely specified, on account of conditions to which
they are subject. Thus, in the case already considered,
$\eta' + \eta''$ is a function of the energy of the compound system,
and the infinitesimal forces added are subject to the law of
conservation of energy.

Another natural supposition in regard to the added forces
is that they should be such as not to affect the moments of
momentum of the compound system. To get a case in which
moments of momentum of the compound system shall be
constants of motion, we may imagine material particles contained
in two concentric spherical shells, being prevented from
passing the surfaces bounding the shells by repulsions acting
always in lines passing through the common centre of the
shells. Then, if there are no forces acting between particles in
different shells, the mass of particles in each shell will have,
besides its energy, the moments of momentum about three
axes through the centre as constants of motion.

Now let us imagine an ensemble formed by distributing in
phase the system of particles in one shell according to the
index of probability
\[
A - \frac{\eps}{\Theta}
  + \frac{\omega_{1}}{\Omega_{1}}
  + \frac{\omega_{2}}{\Omega_{2}}
  + \frac{\omega_{3}}{\Omega_{3}},
\Tag{(98)}
\]
where $\eps$~denotes the energy of the system, and $\omega_{1}$, $\omega_{2}$, $\omega_{3}$, its
three moments of momentum, and the other letters constants.
\PageSep{39}
In like manner let us imagine a second ensemble formed by
distributing in phase the system of particles in the other shell
according to the index
\[
A' - \frac{\eps'}{\Theta}
  + \frac{\omega_{1}'}{\Omega_{1}}
  + \frac{\omega_{2}'}{\Omega_{2}}
  + \frac{\omega_{3}'}{\Omega_{3}},
\Tag{(99)}
\]
where the letters have similar significations, and $\Theta$,~$\Omega_{1}$, $\Omega_{2}$,~$\Omega_{3}$
the same values as in the preceding formula. Each of the
two ensembles will evidently be in statistical equilibrium, and
therefore also the ensemble of compound systems obtained by
combining each system of the first ensemble with each of the
second. In this third ensemble the index of probability will be
\[
A + A' - \frac{\eps + \eps'}{\Theta}
  + \frac{\omega_{1} + \omega_{1}'}{\Omega_{1}}
  + \frac{\omega_{2} + \omega_{2}'}{\Omega_{2}}
  + \frac{\omega_{3} + \omega_{3}'}{\Omega_{3}},
\Tag{(100)}
\]
where the four numerators represent functions of phase which
are constants of motion for the compound systems.

Now if we add in each system of this third ensemble infinitesimal
conservative forces of attraction or repulsion between
particles in different shells, determined by the same law for
all the systems, the functions $\omega_{1} + \omega_{1}'$, $\omega_{2} + \omega_{2}'$, and $\omega_{3} + \omega_{3}'$
will remain constants of motion, and a function differing infinitely
little from $\eps_{1} + \eps'$ will be a constant of motion. It
would therefore require only an infinitesimal change in the
distribution in phase of the ensemble of compound systems to
make it a case of statistical equilibrium. These properties are
entirely analogous to those of canonical ensembles.\footnote
  {It would not be possible to omit the term relating to energy in the above
  indices, since without this term the condition expressed by equation~\Eq{(89)}
  cannot be satisfied.

  The consideration of the above case of statistical equilibrium may be
  made the foundation of the theory of the thermodynamic equilibrium of
  rotating bodies,---a subject which has been treated by Maxwell in his memoir
  ``On Boltzmann's theorem on the average distribution of energy in a system
  of material points.'' Cambr.\ Phil.\ Trans., vol.~XII, p.~547, (1878).}%

Again, if the relations between the forces and the coördinates
can be expressed by linear equations, there will be certain
``normal'' types of vibration of which the actual motion may
be regarded as composed, and the whole energy may be divided
\PageSep{40}
into parts relating separately to vibrations of these different
types. These partial energies will be constants of motion,
and if such a system is distributed according to an index
which is any function of the partial energies, the ensemble will
be in statistical equilibrium. Let the index be a linear function
of the partial energies, say
\[
A - \frac{\eps_{1}}{\Theta_{1}} \Add{-} \cdots - \frac{\eps_{n}}{\Theta_{n}}.
\Tag{(101)}
\]
Let us suppose that we have also a second ensemble composed
of systems in which the forces are linear functions of
the coördinates, and distributed in phase according to an index
which is a linear function of the partial energies relating to
the normal types of vibration, say
\[
A' - \frac{\eps_{1}'}{\Theta_{1}'} \Add{-} \cdots - \frac{\eps_{m}'}{\Theta_{m}'}.
\Tag{(102)}
\]

Since the two ensembles are both in statistical equilibrium,
the ensemble formed by combining each system of the first
with each system of the second will also be in statistical
equilibrium. Its distribution in phase will be represented by
the index
\[
A + A' - \frac{\eps_{1}}{\Theta_{1}} \Add{-} \cdots - \frac{\eps_{n}}{\Theta_{n}}
       - \frac{\eps_{1}'}{\Theta_{1}'} \Add{-} \cdots - \frac{\eps_{m}'}{\Theta_{m}'},
\Tag{(103)}
\]
and the partial energies represented by the numerators in the
formula will be constants of motion of the compound systems
which form this third ensemble.

Now if we add to these compound systems infinitesimal
forces acting between the component systems and subject to
the same general law as those already existing, viz., that they
are conservative and linear functions of the coördinates, there
will still be $n + m$ types of normal vibration, and $n + m$
partial energies which are independent constants of motion.
If all the original $n + m$ normal types of vibration have different
periods, the new types of normal vibration will differ infinitesimally
from the old, and the new partial energies, which are
constants of motion, will be nearly the same functions of
phase as the old. Therefore the distribution in phase of the
\PageSep{41}
ensemble of compound systems after the addition of the supposed
infinitesimal forces will differ infinitesimally from one
which would be in statistical equilibrium.

The case is not so simple when some of the normal types of
motion have the same periods. In this case the addition of
infinitesimal forces may completely change the normal types
of motion. But the sum of the partial energies for all the
original types of vibration which have any same period, will
be nearly identical (as a function of phase, \ie, of the coördinates
and momenta,) with the sum of the partial energies for
the normal types of vibration which have the same, or nearly
the same, period after the addition of the new forces. If,
therefore, the partial energies in the indices of the first two
ensembles \Eq{(101)} and \Eq{(102)} which relate to types of vibration
having the same periods, have the same divisors, the same will
be true of the index~\Eq{(103)} of the ensemble of compound systems,
and the distribution represented will differ infinitesimally
from one which would be in statistical equilibrium after the
addition of the new forces.\footnote
  {It is interesting to compare the above relations with the laws respecting
  the exchange of energy between bodies by radiation, although the phenomena
  of radiations lie entirely without the scope of the present treatise, in which
  the discussion is limited to systems of a finite number of degrees of freedom.}%

The same would be true if in the indices of each of the
original ensembles we should substitute for the term or terms
relating to any period which does not occur in the other ensemble,
any function of the total energy related to that period,
subject only to the general limitation expressed by equation\Eq{(89)}.
But in order that the ensemble of compound systems
(with the added forces) shall always be approximately in
statistical equilibrium, it is necessary that the indices of the
original ensembles should be linear functions of those partial
energies which relate to vibrations of periods common to the
two ensembles, and that the coefficients of such partial energies
should be the same in the two indices.\footnote
  {The above may perhaps be sufficiently illustrated by the simple case
  where $n = 1$ in each system. If the periods are different in the two systems,
  they may be distributed according to any functions of the energies: but if
  the periods are the same they must be distributed canonically with same
  modulus in order that the compound ensemble with additional forces may
  be in statistical equilibrium.}%
\PageSep{42}

The properties of canonically distributed ensembles of
systems with respect to the equilibrium of the new ensembles
which may be formed by combining each system of one ensemble
with each system of another, are therefore not peculiar
to them in the sense that analogous properties do not belong
to some other distributions under special limitations in regard
to the systems and forces considered. Yet the canonical
distribution evidently constitutes the most simple case of the
kind, and that for which the relations described hold with the
least restrictions.

Returning to the case of the canonical distribution, we
shall find other analogies with thermodynamic systems, if we
suppose, as in the preceding chapters,\footnote
  {See especially Chapter~I, p.~\Pageref{4}.}
that the potential
energy~($\eps_{q}$) depends not only upon the coördinates $q_{1}$\Add{,}~\dots $q_{n}$
which determine the configuration of the system, but also
upon certain coördinates $a_{1}$, $a_{2}$, etc.\ of bodies which we call
\emph{external}, meaning by this simply that they are not to be regarded
as forming any part of the system, although their
positions affect the forces which act on the system. The
forces exerted by the system upon these external bodies will
be represented by $-d\eps_{q}/da_{1}$, $-d\eps_{q}/da_{2}$, etc., while $-d\eps_{q}/dq_{1}$,~\dots
$-d\eps_{q}/dq_{n}$ represent all the forces acting upon the bodies
of the system, including those which depend upon the position
of the external bodies, as well as those which depend only
upon the configuration of the system itself. It will be understood
that $\eps_{p}$~depends only upon $q_{1}$,~\dots $q_{n}$ $p_{1}$,~\dots $p_{n}$, in other
words, that the kinetic energy of the bodies which we call
external forms no part of the kinetic energy of the system.
It follows that we may write
\[
\frac{d\eps}{da_{1}} = \frac{d\eps_{q}}{da_{1}} = -A_{1},
\Tag{(104)}
\]
although a similar equation would not hold for differentiations
relative to the internal coördinates.
\PageSep{43}

We always suppose these external coördinates to have the
same values for all systems of any ensemble. In the case of
a canonical distribution, \ie, when the index of probability
of phase is a linear function of the energy, it is evident that
the values of the external coördinates will affect the distribution,
since they affect the energy. In the equation
\[
e^{-\efrac{\psi}{\Theta}} = \intap e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n},
\Tag{(105)}
\]
by which $\psi$~may be determined, the external coördinates, $a_{1}$,
$a_{2}$, etc., contained implicitly in~$\eps$, as well as~$\Theta$, are to be regarded
as constant in the integrations indicated. The equation
indicates that $\psi$~is a function of these constants. If we
imagine their values varied, and the ensemble distributed
canonically according to their new values, we have by
differentiation of the equation\Add{,}
\begin{multline*}
e^{-\efrac{\psi}{\Theta}} \left(-\frac{1}{\Theta}\, d\psi + \frac{\psi}{\Theta^{2}}\, d\Theta\right)
  = \frac{1}{\Theta^{2}}\, d\Theta \intap \eps e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n} \\
  - \frac{1}{\Theta}\, da_{1} \intap \frac{d\eps}{da_{1}}\, e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n} \\
  - \frac{1}{\Theta}\, da_{2} \intap \frac{d\eps}{da_{2}}\, e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n} - \text{etc.},
\Tag{(106)}
\end{multline*}
or, multiplying by $\Theta\, e^{\efrac{\psi}{\Theta}}$, and setting
\[
-\frac{d\eps}{da_{1}} = A_{1},\quad
-\frac{d\eps}{da_{2}} = A_{2},\quad \text{etc.},
\]
\begin{multline*}
-d\psi + \frac{\psi}{\Theta}\, d\Theta
  = \frac{1}{\Theta}\, d\Theta \intap \eps e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} \\
  + da_{1} \intap A_{1}\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} \\
  + da_{2} \intap A_{2}\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} + \text{etc.}
\Tag{(107)}
\end{multline*}
\PageSep{44}
Now the average value in the ensemble of any quantity
(which we shall denote in general by a horizontal line above
the proper symbol) is determined by the equation
\[
\bar{u} = \intap u\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}.
\Tag{(108)}
\]
Comparing this with the preceding equation, we have
\[
d\psi = \frac{\psi}{\Theta}\, d\Theta - \frac{\bar{\eps}}{\Theta}\, d\Theta
  - \bar{A}_{1}\, da_{1} - \bar{A}_{2}\, da_{2} - \text{etc.}
\Tag{(109)}
\]
Or, since
\[
\frac{\psi - \eps}{\Theta} = \eta,
\Tag{(110)}
\]
and
\begin{gather*}
\frac{\psi - \bar{\eps}}{\Theta} = \bar{\eta},
\Tag{(111)} \\
d\psi = \bar{\eta}\, d\Theta - \bar{A}_{1}\, da_{1} - \bar{A}_{2}\, da_{2} - \text{etc.}
\Tag{(112)}
\end{gather*}
Moreover, since \Eq{(111)} gives
\[
d\psi - d\bar{\eps} = \Theta\, d\bar{\eta} + \bar{\eta}\, d\Theta,
\Tag{(113)}
\]
we have also
\[
d\bar{\eps} = -\Theta\, d\bar{\eta} - \bar{A}_{1}\, da_{1} - \bar{A}_{2}\, da_{2} - \text{etc.}
\Tag{(114)}
\]

This equation, if we neglect the sign of averages, is identical
in form with the thermodynamic equation
\[
d\eta = \frac{d\eps + A_{1}\, da_{1} + A_{2}\, da_{2} + \text{etc.}}{T},
\Tag{(115)}
\]
or
\[
d\eps = T\, d\eta - A_{1}\, da_{1} - A_{2}\, da_{2} - \text{etc.}
\Tag{(116)}
\]
which expresses the relation between the energy, temperature,
and entropy of a body in thermodynamic equilibrium,
and the forces which it exerts on external bodies,---a relation
which is the mathematical expression of the second law of
thermodynamics for reversible changes. The modulus in the
statistical equation corresponds to temperature in the thermodynamic
equation, and the average index of probability \emph{with
its sign reversed} corresponds to entropy. But in the thermodynamic
equation the entropy~($\eta$) is a quantity which is
\PageSep{45}
only defined by the equation itself, and incompletely defined
in that the equation only determines its differential, and the
constant of integration is arbitrary. On the other hand, the
$\bar{\eta}$ in the statistical equation has been completely defined as
the average value in a canonical ensemble of systems of
the logarithm of the coefficient of probability of phase.

We may also compare equation~\Eq{(112)} with the thermodynamic
equation
\[
\Typo{\psi}{d\psi} = -\eta\, dT - A_{1}\, da_{1} - A_{2}\, da_{2} - \text{etc.},
\Tag{(117)}
\]
where $\psi$~represents the function obtained by subtracting the
product of the temperature and entropy from the energy.

How far, or in what sense, the similarity of these equations
constitutes any demonstration of the thermodynamic equations,
or accounts for the behavior of material systems, as
described in the theorems of thermodynamics, is a question
of which we shall postpone the consideration until we have
further investigated the properties of an ensemble of systems
distributed in phase according to the law which we are considering.
The analogies which have been pointed out will at
least supply the motive for this investigation, which will
naturally commence with the determination of the average
values in the ensemble of the most important quantities relating
to the systems, and to the distribution of the ensemble with
respect to the different values of these quantities.
\PageSep{46}


\Chapter{V.}{Average Values in a Canonical Ensemble
of Systems.}

\First{In} the simple but important case of a system of material
points, if we use rectangular coördinates, we have for the
product of the differentials of the coördinates
\[
dx_{1}\, dy_{1}\, dz_{1} \dots dx_{\nu}\, dy_{\nu}\, dz_{\nu},
\]
and for the product of the differentials of the momenta
\[
m_{1}\, d\dot{x}_{1}\, m_{1}\, d\dot{y}_{1}\, m_{1}\, d\dot{z}_{1} \dots
m_{\nu}\, d\dot{x}_{\nu}\, m_{\nu}\, d\dot{y}_{\nu}\, m_{\nu}\, d\dot{z}_{\nu}.
\]
The product of these expressions, which represents an element
of extension-in-phase, may be briefly written
\[
m_{1}\, d\dot{x}_{1} \dots m_{\nu}\, d\dot{z}_{\nu}\, dx_{1} \dots dz_{\nu};
\]
and the integral
\[
\ints e^{\efrac{\psi - \eps}{\Theta}}\, m_{1}\, d\dot{x}_{1} \dots m_{\nu}\, d\dot{z}_{\nu}\, dx_{1} \dots dz_{\nu}
\Tag{(118)}
\]
will represent the probability that a system taken at random
from an ensemble canonically distributed will fall within any
given limits of phase.

In this case
\[
\eps = \eps_{q} + \tfrac{1}{2} m_{1} \dot{x}_{1}^{2}
 \Add{+} \cdots + \tfrac{1}{2} m_{\nu} \dot{x}_{\nu}^{2},
\Tag{(119)}
\]
and
\[
e^{\efrac{\psi - \eps}{\Theta}}
  = e^{\efrac{\psi - \eps_{q}}{\Theta}}
    e^{-\efrac{m_{1} \dot{x}_{1}^{2}}{2\Theta}} \dots e^{-\efrac{m_{\nu} \dot{x}_{\nu}^{2}}{2\Theta}}.
\Tag{(120)}
\]
The potential energy~($\eps_{q}$) is independent of the velocities,
and if the limits of integration for the coördinates are independent
of the velocities, and the limits of the several velocities
are independent of each other as well as of the coördinates,
\PageSep{47}
the multiple integral may be resolved into the product of
integrals
\[
\ints e^{\efrac{\psi - \eps_{q}}{\Theta}}\, dx_{1} \dots dz_{\nu}
  \int e^{-\efrac{m_{1} \dot{x}_{1}^{2}}{2\Theta}}\, m_{1} d\dot{x}_{1} \dots \!\!
  \int e^{-\efrac{m_{\nu} \dot{z}_{\nu}^{2}}{2\Theta}}\, m_{\nu}\, d\dot{z}_{\nu}.
\Tag{(121)}
\]
This shows that the probability that the configuration lies
within any given limits is independent of the velocities,
and that the probability that any component velocity lies
within any given limits is independent of the other component
velocities and of the configuration.

Since
\[
\int_{-\infty}^{\infty} e^{-\efrac{m_{1} \dot{x}_{1}^{2}}{2\Theta}}\, m_{1} d\dot{x}_{1}
  = \sqrt{2\pi m_{1} \Theta},
\Tag{(122)}
\]
and
\[
\int_{-\infty}^{\infty} \tfrac{1}{2} m_{1} \dot{x}_{1}^{2}\, e^{-\efrac{m_{1} \dot{x}_{1}^{2}}{2\Theta}}\, m_{1} d\dot{x}_{1}
  = \sqrt{\tfrac{1}{2}\pi m_{1} \Theta^{3}},
\Tag{(123)}
\]
the average value of the part of the kinetic energy due to the
velocity~$\dot{x}_{1}$, which is expressed by the quotient of these integrals,
is~$\frac{1}{2} \Theta$. This is true whether the average is taken for
the whole ensemble or for any particular configuration,
whether it is taken without reference to the other component
velocities, or only those systems are considered in which the
other component velocities have particular values or lie
within specified limits.

The number of coördinates is~$3\nu$ or~$n$. We have, therefore,
for the average value of the kinetic energy of a system
\[
\bar{\eps}_{p} = \tfrac{3}{2} \nu \Theta = \tfrac{1}{2} n \Theta.
\Tag{(124)}
\]
This is equally true whether we take the average for the whole
ensemble, or limit the average to a single configuration.

The distribution of the systems with respect to their component
velocities follows the `law of errors'; the probability
that the value of any component velocity lies within any given
limits being represented by the value of the corresponding
integral in~\Eq{(121)} for those limits, divided by $(2\pi m \Theta)^{\efrac{1}{2}}$,
\PageSep{48}
which is the value of the same integral for infinite limits.
Thus the probability that the value of~$\dot{x}_{1}$ lies between any
given limits is expressed by
\[
\left(\frac{m_{1}}{2\pi \Theta}\right)^{\efrac{1}{2}}
  \int e^{-\efrac{m_{1} \dot{x}_{1}^{2}}{2\Theta}}\, d\dot{x}_{1}.
\Tag{(125)}
\]
The expression becomes more simple when the velocity is
expressed with reference to the energy involved. If we set
\[
s = \left(\frac{m_{1}}{2\Theta}\right)^{\efrac{1}{2}} \dot{x}_{1},
\]
the probability that $s$~lies between any given limits is
expressed by
\[
\frac{1}{\sqrt{\pi}} \int e^{-s^{2}}\, ds.
\Tag{(126)}
\]
Here $s$~is the ratio of the component velocity to that which
would give the energy~$\Theta$; in other words, $s^{2}$~is the quotient
of the energy due to the component velocity divided by~$\Theta$.
The distribution with respect to the partial energies due to
the component velocities is therefore the same for all the component
velocities.

The probability that the configuration lies within any given
limits is expressed by the value of
\[
M^{\efrac{3}{2}} (2\pi \Theta)^{\efrac{3\nu}{2}} \ints e^{\efrac{\psi - \eps}{\Theta}} \, dx_{1} \dots dx_{\nu}
\Tag{(127)}
\]
for those limits, where $M$~denotes the product of all the
masses. This is derived from~\Eq{(121)} by substitution of the
values of the integrals relating to velocities taken for infinite
limits.

Very similar results may be obtained in the general case of
a conservative system of $n$~degrees of freedom. Since $\eps_{p}$~is a
homogeneous quadratic function of the~$p$'s, it may be divided
into parts by the formula
\[
\eps_{p} = \tfrac{1}{2} p_{1}\, \frac{d\eps_{p}}{dp_{1}} \Add{+} \cdots
        + \tfrac{1}{2} p_{n}\, \frac{d\eps_{p}}{dp_{n}}\Add{,}
\Tag{(128)}
\]
\PageSep{49}
where $\eps$~might be written for~$\eps_{p}$ in the differential coefficients
without affecting the signification. The average value of the
first of these parts, for any given configuration, is expressed
by the quotient
\[
\frac{\dsp \intsall \tfrac{1}{2} p_{1}\, \frac{d\eps}{dp_{1}} e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dp_{n}}
{\dsp \intsall e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dp_{n}}\Add{.}
\Tag{(129)}
\]
Now we have by integration by parts
\[
\int_{-\infty}^{\infty} p_{1} e^{\efrac{\psi - \eps}{\Theta}}\, \frac{d\eps}{dp_{1}}\, dp_{1}
  = \Theta \int_{-\infty}^{\infty} e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1}.
\Tag{(130)}
\]
By substitution of this value, the above quotient reduces to~$\Chg{\dfrac{\Theta}{2}}{\frac{1}{2}\Theta}$,
which is therefore the average value of $\frac{1}{2} p_{1}\, \dfrac{d\eps}{dp_{1}}$ for the
given configuration. Since this value is independent of the
configuration, it must also be the average for the whole
ensemble, as might easily be proved directly. (To make
the preceding proof apply directly to the whole ensemble, we
have only to write $dp_{1} \dots dq_{n}$ for $dp_{1} \dots dp_{n}$ in the multiple
integrals.) This gives $\frac{1}{2}n \Theta$ for the average value of the
whole kinetic energy for any given configuration, or for
the whole ensemble, as has already been proved in the case of
material points.

The mechanical significance of the several parts into which
the kinetic energy is divided in equation~\Eq{(128)} will be apparent
if we imagine that by the application of suitable forces
(different from those derived from~$\eps_{q}$ and so much greater
that the latter may be neglected in comparison) the system
was brought from rest to the state of motion considered, so
rapidly that the configuration was not sensibly altered during
the process, and in such a manner also that the ratios of the
component velocities were constant in the process. If we
write
\[
F_{1}\, dq_{1} \Add{+} \cdots + F_{n}\, dq_{n}
\]
\PageSep{50}
for the moment of these forces, we have for the period of their
action by equation~\Eq{(3)}
\[
\dot{p}_{1} = -\frac{d\eps_{p}}{dq_{1}} - \frac{d\eps_{q}}{dq_{1}} + F_{1}
  = -\frac{d\eps}{dq_{1}} + F_{1}.
\]
The work done by the force~$F_{1}$ may be evaluated as follows:
\[
\int F_{1}\, dq_{1} = \int \dot{p}_{1}\, dq_{1} + \int \frac{d\eps}{dq_{1}}\, dq_{1},
\]
where the last term may be cancelled because the configuration
does not vary sensibly during the application of the forces.
(It will be observed that the other terms contain factors which
increase as the time of the action of the forces is diminished.)
We have therefore,
\[
\int F_{1}\, dq_{1} = \int \dot{p}_{1} \dot{q}_{1}\, dt
  = \int \dot{q}_{1}\, dp_{1}
  = \frac{\dot{q}_{1}}{p_{1}} \int p_{1}\, dp_{1}.
\Tag{(131)}
\]
For since the $p$'s are linear functions of the~$\dot{q}$'s (with coefficients
involving the~$q$'s) the supposed constancy of the $q$'s and
of the ratios of the~$\dot{q}$'s will make the ratio~$\dot{q}_{1}/p_{1}$ constant.
The last integral is evidently to be taken between the limits
zero and the value of~$p_{1}$ in the phase originally considered,
and the quantities before the integral sign may be taken as
relating to that phase. We have therefore
\[
\int F_{1}\, dq_{1} = \tfrac{1}{2} p_{1} \dot{q}_{1}
  = \tfrac{1}{2} p_{1}\, \frac{d\eps}{dp_{1}}.
\Tag{(132)}
\]
That is: the several parts into which the kinetic energy is
divided in equation~\Eq{(128)} represent the amounts of energy
communicated to the system by the several forces $F_{1}$,~\dots $F_{n}$
under the conditions mentioned.

The following transformation will not only give the value
of the average kinetic energy, but will also serve to separate
the distribution of the ensemble in configuration from its distribution
in velocity.

Since $2\eps_{p}$~is a homogeneous quadratic function of the~$p$'s,
which is incapable of a negative value, it can always be expressed
(and in more than one way) as a sum of squares of
\PageSep{51}
linear functions of the~$p$'s.\footnote
  {The reduction requires only the repeated application of the process of
  `completing the square' used in the solution of quadratic equations.}
The coefficients in these linear
functions, like those in the quadratic function, must be regarded
in the general case as functions of the~$q$'s. Let
\[
2\eps_{p} = u_{1}^{2} + u_{2}^{2} \Add{+} \cdots + u_{n}^{2}
\Tag{(133)}
\]
where $u_{1}$\Add{,}~\dots $u_{n}$ are such linear functions of the~$p$'s. If we
write
\[
\frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}
\]
for the Jacobian or determinant of the differential coefficients
of the form~$dp/du$, we may substitute
\[
\frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}\, du_{1} \dots du_{n}
\]
for
\[
dp_{1} \dots dp_{n}
\]
under the multiple integral sign in any of our \Chg{formulæ}{formulae}. It
will be observed that this determinant is function of the $q$'s
alone. The sign of such a determinant depends on the relative
order of the variables in the numerator and denominator.
But since the suffixes of the~$u$'s are only used to distinguish
these functions from one another, and no especial relation is
supposed between a~$p$ and a~$u$ which have the same suffix, we
may evidently, without loss of generality, suppose the suffixes
so applied that the determinant is positive.

Since the~$u$'s are linear functions of the~$p$'s, when the integrations
are to cover all values of the~$p$'s (for constant~$q$'s)
once and only once, they must cover all values of the~$u$'s once
and only once, and the limits will be~$\pm\infty$ for all the~$u$'s.
Without the supposition of the last paragraph the upper limits
would not always be~$+\infty$, as is evident on considering the
effect of changing the sign of a~$u$. But with the supposition
which we have made (that the determinant is always positive)
we may make the upper limits~$+\infty$ and the lower~$-\infty$ for all
the~$u$'s. Analogous considerations will apply where the integrations
do not cover all values of the~$p$'s and therefore of
\PageSep{52}
the~$u$'s. The integrals may always be taken from a less to a
greater value of a~$u$.

The general integral which expresses the fractional part of
the ensemble which falls within any given limits of phase is
thus reduced to the form
\[
\ints e^{\efrac{\psi - \eps_{q}}{\Theta}}\, \frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}\, e^{-\efrac{u_{1}^{2} \Add{+} \cdots \Add{+} u_{n}^{2}}{2\Theta}}\, du_{1} \dots du_{n}\, dq_{1} \dots dq_{n}.
\Tag{(134)}
\]

For the average value of the part of the kinetic energy
which is represented by~$\frac{1}{2}u_{1}^{2}$, whether the average is taken
for the whole ensemble, or for a given configuration, we have
therefore
\[
\tfrac{1}{2} \bar{u}_{1}^{2}
  = \frac{\dsp \int_{-\infty}^{\infty} \tfrac{1}{2} u_{1}^{2} e^{-\efrac{u_{1}^{2}}{2\Theta}}\, du_{1}}{\dsp \int_{-\infty}^{\infty} e^{-\efrac{u_{1}^{2}}{2\Theta}}\, du_{1}}
  = \frac{(\frac{1}{2}\pi \Theta^{3})^{\efrac{1}{2}}}{(2\pi \Theta)^{\efrac{1}{2}}}
  = \frac{\Theta}{2},
\Tag{(135)}
\]
and for the average of the whole kinetic energy, $\frac{1}{2}n \Theta$, as
before.

The fractional part of the ensemble which lies within any
given limits of \emph{configuration}, is found by integrating~\Eq{(134)}
with respect to the~$u$'s from~$-\infty$ to~$+\infty$. This gives
\[
(2\pi \Theta)^{\efrac{n}{2}} \ints e^{\efrac{\psi - \eps_{q}}{\Theta}}\,
  \frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}\, dq_{1} \dots dq_{n}
\Tag{(136)}
\]
which shows that the value of the Jacobian is independent of
the manner in which $2\eps_{p}$~is divided into a sum of squares.
We may verify this directly, and at the same time obtain a
more convenient expression for the Jacobian, as follows.

It will be observed that since the~$u$'S are linear functions of
the~$p$'s, and the~$p$'s linear functions of the~$\dot{q}$'s, the~$u$'s will be
linear functions of the~$\dot{q}$'s, so that a differential coefficient of
the form~$du/d\dot{q}$ will be independent of the~$\dot{q}$'s, and function of
the $q$'s alone. Let us write $dp_{x}/du_{y}$ for the general element
of the Jacobian determinant. We have
\PageSep{53}
\begin{align*}
\frac{dp_{x}}{du_{y}}
  &= \frac{d}{du_{y}}\, \frac{d\eps}{d\dot{q}_{x}}
  = \frac{d}{du_{y}} \sum_{r=1}^{r=n} \frac{d\eps}{du_{r}}\, \frac{du_{r}}{d\dot{q}_{x}} \\
  &= \sum_{r=1}^{r=n} \left(\frac{d^{2}\eps}{du_{y}\, du_{r}}\, \frac{du_{r}}{d\dot{q}_{x}}\right)
  = \frac{d}{d\dot{q}_{x}}\, \frac{d\eps}{du_{y}}
  = \frac{du_{y}}{d\dot{q}_{x}}\Add{.}
\Tag{(137)}
\end{align*}
Therefore
\[
\frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}
  = \frac{d(u_{1} \dots u_{n})}{d(\dot{q}_{1} \dots \dot{q}_{n})}
\Tag{(138)}
\]
and
\[
\left(\frac{d(p_{1} \dots p_{n})}{d(u_{1} \dots u_{n})}\right)^{2}
  = \left(\frac{d(u_{1} \dots u_{n})}{d(\dot{q}_{1} \dots \dot{q}_{n})}\right)^{2}
  = \frac{d(p_{1} \dots p_{n})}{d(\dot{q}_{1} \dots \dot{q}_{n})}\Add{.}
\Tag{(139)}
\]

These determinants are all functions of the $q$'s alone.\footnote
  {It will be observed that the proof of~\Eq{(137)} depends on the linear relation
  between the~$u$'s and~$q$'s, which makes $\dfrac{du_{r}}{d\dot{q}_{x}}$ constant with respect to the differentiations
  here considered. Compare note on p.~\Pageref{12}.}
The
last is evidently the Hessian or determinant formed of the
second differential coefficients of the kinetic energy with respect
to $\dot{q}_{1}$,~\dots $\dot{q}_{n}$. We shall denote it by~$\Delta_{\dot{q}}$. The reciprocal
determinant
\[
\frac{d(\dot{q}_{1} \dots \dot{q}_{n})}{d(p_{1} \dots p_{n})},
\]
which is the Hessian of the kinetic energy regarded as function
of the~$p$'s, we shall denote by~$\Delta_{p}$.

If we set
\begin{align*}
e^{-\efrac{\psi_{p}}{\Theta}}
  &= \intsall e^{-\efrac{\eps_{p}}{\Theta}}\, \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n} \\
  &= \intsall e^{\efrac{-u_{1}^{2} \Add{-} \cdots - u_{n}^{2}}{2\Theta}}\, du_{1} \dots du_{n}
  = (2\pi \Theta)^{\efrac{n}{2}},
\Tag{(140)}
\end{align*}
and
\[
\psi_{q} = \psi - \psi_{p},
\Tag{(141)}
\]
\PageSep{54}
the fractional part of the ensemble which lies within any
given limits of configuration~\Eq{(136)} may be written
\[
\ints e^{\efrac{\psi_{q} - \eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(142)}
\]
where the constant~$\psi_{q}$ may be determined by the condition
that the integral extended over all configurations has the value
unity.\footnote
  {In the simple but important case in which $\Delta_{\dot{q}}$~is independent of the~$q$'s,
  and $\eps_{q}$~a quadratic function of the~$q$'s, if we write~$\eps_{\alpha}$ for the least value of~$\eps_{q}$
  (or of~$\eps$) consistent with the given values of the external coördinates, the
  equation determining~$\psi_{q}$ may be written
  \[
  e^{\efrac{\eps_{\alpha} - \psi_{q}}{\Theta}}
  = \Delta_{\dot{q}}^{\efrac{1}{2}}
    \intsall e^{-\efrac{(\eps_{q} - \eps_{\alpha})}{\Theta}}\, dq_{1} \dots dq_{n}.
  \]
  If we denote by $q_{1}'$,~\dots $q_{n}'$ the values of $q_{1}$,~\dots $q_{n}$ which give~$\eps_{q}$ its least value~$\eps_{\alpha}$,
  it is evident that $\eps_{q} - \eps_{\alpha}$ is a \Typo{homogenous}{homogeneous} quadratic function of the differences
  $q_{1} - q_{1}'$, etc., and that $dq_{1}$,~\dots $dq_{n}$ may be regarded as the differentials
  of these differences. The evaluation of this integral is therefore analytically
  similar to that of the integral
  \[
  \intsall e^{-\efrac{\eps_{p}}{\Theta}}\, dp_{1} \dots dp_{n},
  \]
  for which we have found the value $\Delta_{p}^{-\efrac{1}{2}} (2\pi \Theta)^{\efrac{n}{2}}$. By the same method, or
  by analogy, we get
  \[
  e^{\efrac{\eps_{\alpha} - \psi_{q}}{\Theta}} = \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}} (2\pi \Theta)^{\efrac{n}{2}},
  \]
  where $\Delta_{q}$~is the Hessian of the potential energy as function of the~$q$'s. It
  will be observed that $\Delta_{q}$~depends on the forces of the system and is independent
  of the masses, while $\Delta_{\dot{q}}$~or its reciprocal~$\Delta_{p}$ depends on the masses and
  is independent of the forces. While each Hessian depends on the system of
  coördinates employed, the ratio $\Delta_{q}/\Delta_{\dot{q}}$ is the same for all systems.

  Multiplying the last equation by~\Eq{(140)}, we have
  \[
  e^{\efrac{\eps_{\alpha} - \psi}{\Theta}} = \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}} (2\pi \Theta)^{n}.
  \]

  For the average value of the potential energy, we have
  \[
  \bar{\eps}_{q} - \eps_{\alpha}
  = \frac{\dsp \intsall (\eps_{q} - \eps_{\alpha}) e^{-\efrac{\eps_{q} - \eps_{\alpha}}{\Theta}}\, dq_{1} \dots dq_{n}}{\dsp \intsall e^{-\efrac{\eps_{q} - \eps_{\alpha}}{\Theta}}\, dq_{1} \dots dq_{n}}.
  \]
  The evaluation of this expression is similar to that of
  \[
  \frac{\dsp \intsall \eps_{p} e^{-\efrac{\eps_{p}}{\Theta}}\, dp_{1} \dots dp_{n}}{\dsp \intsall e^{-\efrac{\eps_{p}}{\Theta}}\, dp_{1} \dots dp_{n}}\Add{,}
  \]
  which expresses the average value of the kinetic energy, and for which we
  have found the value~$\frac{1}{2}n \Theta$. We have accordingly
  \[
  \bar{\eps}_{q} - \eps_{\alpha} = \tfrac{1}{2}n \Theta.
  \]

  Adding the equation
  \[
  \bar{\eps}_{p} = \tfrac{1}{2}n \Theta,
  \]
  we have
  \[
  \bar{\eps} - \eps_{\alpha} = n\Theta.
  \]}%
\PageSep{55}

When an ensemble of systems is distributed in \emph{configuration}
in the manner indicated in this formula, \ie, when its
distribution in configuration is the same as that of an ensemble
canonically distributed in phase, we shall say, without
any reference to its velocities, that it is \emph{canonically distributed
in configuration}.

For any given configuration, the fractional part of the
systems which lie within any given limits of velocity is
represented by the quotient of the multiple integral
\[
\ints e^{-\efrac{\eps_{p}}{\Theta}}\, dp_{1} \dots dp_{n},
\]
or its equivalent
\[
\ints e^{\efrac{-u_{1}^{2} \Add{-} \cdots - u_{n}^{2}}{2\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, du_{1} \dots du_{n},
\]
taken within those limits divided by the value of the same
integral for the limits~$\pm\infty$. But the value, of the second
multiple integral for the limits~$\pm\infty$ is evidently
\[
\Delta_{\dot{q}}^{\efrac{1}{2}} (2\pi \Theta)^{\efrac{n}{2}}.
\]
We may therefore write
\[
\ints e^{\efrac{\psi_{p} - \eps_{p}}{\Theta}}\, du_{1} \dots du_{n},
\Tag{(143)}
\]
\PageSep{56}
or
\[
\ints e^{\efrac{\psi_{p} - \eps_{p}}{\Theta}}\, \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\Tag{(144)}
\]
or again
\[
\ints e^{\efrac{\psi_{p} - \eps_{p}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n},
\Tag{(145)}
\]
for the fractional part of the systems of any given configuration
which lie within given limits of velocity.

When systems are distributed in velocity according to these
formulae, \ie, when the distribution in velocity is like that in
an ensemble which is canonically distributed in phase, we
shall say that they are \emph{canonically distributed in velocity}.

The fractional part of the whole ensemble which falls
within any given limits of phase, which we have before
expressed in the form
\[
\ints e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dp_{n}\, dq_{1} \dots dq_{n},
\Tag{(146)}
\]
may also be expressed in the form
\[
\ints e^{\efrac{\psi - \eps}{\Theta}}\, \Delta_{\dot{q}}\, d\dot{q}_{1} \dots d\dot{q}_{n}\, dq_{1} \dots dq_{n}.
\Tag{(147)}
\]
\PageSep{57}


\Chapter{VI.}{Extension in Configuration and Extension
in Velocity.}

\First{The} formulae relating to canonical ensembles in the closing
paragraphs of the last chapter suggest certain general notions
and principles, which we shall consider in this chapter, and
which are not at all limited in their application to the canonical
law of distribution.\footnote
  {These notions and principles are in fact such as a more logical arrangement
  of the subject would place in connection with those of Chapter~I., to
  which they are closely related. The strict requirements of logical order
  have been sacrificed to the natural development of the subject, and very
  elementary notions have been left until they have presented themselves in
  the study of the leading problems.}%

We have seen in Chapter~IV. that the nature of the distribution
which we have called canonical is independent of the
system of coördinates by which it is described, being determined
entirely by the modulus. It follows that the value
represented by the multiple integral~\Eq{(142)}, which is the fractional
part of the ensemble which lies within certain limiting
configurations, is independent of the system of coördinates,
being determined entirely by the limiting configurations with
the modulus. Now $\psi$, as we have already seen, represents
a value which is independent of the system of coördinates
by which it is defined. The same is evidently true of~$\psi_{p}$
by equation~\Eq{(140)}, and therefore, by~\Eq{(141)}, of~$\psi_{q}$.
Hence the exponential factor in the multiple integral~\Eq{(142)}
represents a value which is independent of the system of
coördinates. It follows that the value of a multiple integral
of the form
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n}
\Tag{(148)}
\]
\PageSep{58}
is independent of the system of coördinates which is employed
for its evaluation, as will appear at once, if we suppose the
multiple integral to be broken up into parts so small that
the exponential factor may be regarded as constant in each.

In the same way the formulae \Eq{(144)}~and \Eq{(145)} which express
the probability that a system (in a canonical ensemble) of given
configuration will fall within certain limits of velocity, show
that multiple integrals of the form
\[
\ints \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n}
\Tag{(149)}
\]
or
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n}
\Tag{(150)}
\]
relating to velocities possible for a given configuration, when
the limits are formed by given velocities, have values independent
of the system of coördinates employed.

These relations may easily be verified directly. It has already
been proved that
\[
\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}
  = \frac{d(\dot{q}_{1}, \dots \dot{q}_{n})}{d(Q_{1}, \dots Q_{n})}
  = \frac{d(q_{1}, \dots q_{n})}{d(Q_{1}, \dots Q_{n})}
\]
where $q_{1}$,~\dots $q_{n}$, $p_{1}$,~\dots $p_{n}$ and $Q_{1}$,~\dots $Q_{n}$, $P_{1}$,~\dots $P_{n}$ are two
systems of coördinates and momenta.\footnote
  {See equation~\Eq{(29)}.}
It follows that
{\footnotesize
\begin{multline*}
\ints \left(\frac{d(p_{1}, \dots p_{n})}{d(\dot{q}_{1}, \dots \dot{q}_{n})}\right)^{\efrac{1}{2}} dq_{1} \dots dq_{n} \\
\begin{aligned}
  &= \ints \left(\frac{d(p_{1}, \dots p_{n})}{d(\dot{q}_{1}, \dots \dot{q}_{n})}\right)^{\efrac{1}{2}} \frac{d(q_{1}, \dots q_{n})}{d(Q_{1}, \dots Q_{n})}\, dQ_{1} \dots dQ_{n} \\
  &= \ints \left(\frac{d(p_{1}, \dots p_{n})}{d(\dot{q}_{1}, \dots \dot{q}_{n})}\right)^{\efrac{1}{2}} \left(\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}\right)^{\efrac{1}{2}} \left(\frac{d(\dot{q}_{1}, \dots \dot{q}_{n})}{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}\right)^{\efrac{1}{2}} dQ_{1} \dots dQ_{n} \\
  &= \ints \left(\frac{d(P_{1}, \dots P_{n})}{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}\right)^{\efrac{1}{2}} dQ_{1} \dots dQ_{n},
\end{aligned}
\end{multline*}}
\PageSep{59}
and
{\footnotesize
\begin{multline*}
\ints \left(\frac{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}{d(P_{1}, \dots P_{n})}\right)^{\efrac{1}{2}} dP_{1} \dots \Typo{P_{n}}{dP_{n}} \\
\begin{aligned}
  &= \ints \left(\frac{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}{d(P_{1}, \dots P_{n})}\right)^{\efrac{1}{2}} \frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}\, dp_{1} \dots dp_{n} \\
  &= \ints \left(\frac{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}{d(P_{1}, \dots P_{n})}\right)^{\efrac{1}{2}} \left(\frac{d(P_{1}, \dots P_{n})}{d(p_{1}, \dots p_{n})}\right)^{\efrac{1}{2}} \left(\frac{d(\dot{q}_{1}, \dots \dot{q}_{n})}{d(\dot{Q}_{1}, \dots \dot{Q}_{n})}\right)^{\efrac{1}{2}} dp_{1} \dots dp_{n} \\
  &= \ints \left(\frac{d(\dot{q}_{1}, \dots \dot{q}_{n})}{d(p_{1}, \dots p_{n})}\right)^{\efrac{1}{2}} dp_{1} \dots dp_{n}.
\end{aligned}
\end{multline*}}

The multiple integral
\[
\ints dp_{1} \dots dp_{n}\, dq_{1} \dots dq_{n},
\Tag{(151)}
\]
which may also be written
\[
\ints \Delta_{\dot{q}}\, d\dot{q}_{1} \dots d\dot{q}_{n}\, dq_{1} \dots dq_{n},
\Tag{(152)}
\]
and which, when taken within any given limits of phase, has
been shown to have a value independent of the coördinates
employed, expresses what we have called an \emph{extension-in-phase}.\footnote
  {See Chapter~I, p.~\Pageref{10}.}
In like manner we may say that the multiple integral~\Eq{(148)}
expresses an \emph{extension-in-configuration}, and that the
multiple integrals \Eq{(149)} and \Eq{(150)} express an \emph{extension-in-velocity}.
We have called
\[
 dp_{1} \dots dp_{n}\, dq_{1} \dots dq_{n},
\Tag{(153)}
\]
which is equivalent to
\[
 \Delta_{\dot{q}}\, d\dot{q}_{1} \dots d\dot{q}_{n}\, dq_{1} \dots dq_{n},
\Tag{(154)}
\]
an element of extension-in-phase. We may call
\[
\Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n}
\Tag{(155)}
\]
an element of extension-in-configuration, and
\[
\Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\Tag{(156)}
\]
\PageSep{60}
or its equivalent
\[
\Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n},
\Tag{(157)}
\]
an element of extension-in-velocity.

An extension-in-phase may always be regarded as an integral
of elementary extensions-in-configuration multiplied each by
an extension-in-velocity. This is evident from the formulae
\Eq{(151)} and \Eq{(152)} which express an extension-in-phase, if we
imagine the integrations relative to velocity to be first carried
out.

The product of the two expressions for an element of
extension-in-velocity \Eq{(149)} and \Eq{(150)} is evidently of the same
dimensions as the product
\[
p_{1} \dots p_{n} \dot{q}_{1} \dots \dot{q}_{n}\Add{,}
\]
that is, as the $n$th~power of energy, since every product of the
form~$p_{1}\dot{q}_{1}$ has the dimensions of energy. Therefore an extension-in-velocity
has the dimensions of the square root of the
$n$th~power of energy. Again we see by \Eq{(155)} and \Eq{(156)} that
the product of an extension-in-configuration and an extension-in-velocity
have the dimensions of the $n$th~power of energy
multiplied by the $n$th~power of time. Therefore an extension-in-configuration
has the dimensions of the $n$th~power of time
multiplied by the square root of the $n$th~power of energy.

To the notion of extension-in-configuration there attach
themselves certain other notions analogous to those which have
presented themselves in connection with the notion of extension-in-phase.
The number of systems of any ensemble
(whether distributed canonically or in any other manner)
which are contained in an element of extension-in-configuration,
divided by the numerical value of that element, may be
called the \emph{density-in-configuration}. That is, if a certain configuration
is specified by the coördinates $q_{1}$\Add{,}~\dots $q_{n}$, and the
number of systems of which the coördinates fall between the
limits $q_{1}$~and $q_{1} + dq_{1}$, \dots $q_{n}$~and $q_{n} + dq_{n}$ is expressed by
\[
D_{q} \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(158)}
\]
\PageSep{61}
$D_{q}$~will be the density-in-configuration. And if we set
\[
e^{\eta_{q}} = \frac{D_{q}}{N},
\Tag{(159)}
\]
where $N$~denotes, as usual, the total number of systems in the
ensemble, the probability that an unspecified system of the
ensemble will fall within the given limits of configuration, is
expressed by
\[
e^{\eta_{q}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n}.
\Tag{(160)}
\]
We may call~$e^{\eta_{q}}$ the \emph{coefficient of probability of the configuration},
and $\eta_{q}$~the \emph{index of probability of the configuration}.

The fractional part of the whole number of systems which
are within any given limits of configuration will be expressed
by the multiple integral
\[
\ints e^{\eta_{q}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n}.
\Tag{(161)}
\]
The value of this integral (taken within any given configurations)
is therefore independent of the system of coördinates
which is used. Since the same has been proved of the same
integral without the factor~$e^{\eta_{q}}$, it follows that the values of
$\eta_{q}$ and~$D_{q}$ for a given configuration in a given ensemble are
independent of the system of coördinates which is used.

The notion of extension-in-velocity relates to systems having
the same configuration.\footnote
  {Except in some simple cases, such as a system of material points, we
  cannot compare velocities in one configuration with velocities in another, and
  speak of their identity or difference except in a sense entirely artificial. We
  may indeed say that we call the velocities in one configuration the same as
  those in another when the quantities $\dot{q}_{1}$,~\dots $\dot{q}_{n}$ have the same values in the
  two cases. But this signifies nothing until the system of coördinates has
  been defined. We might identify the velocities in the two cases which make
  the quantities $p_{1}$,~\dots $p_{n}$ the same in each. This again would signify nothing
  independently of the system of coördinates employed.}
If an ensemble is distributed
both in configuration and in velocity, we may confine our
attention to those systems which are contained within certain
infinitesimal limits of configuration, and compare the whole
number of such systems with those which are also contained
\PageSep{62}
within certain infinitesimal limits of velocity. The second
of these numbers divided by the first expresses the probability
that a system which is only specified as falling within the infinitesimal
limits of configuration shall also fall within the
infinitesimal limits of velocity. If the limits with respect to
velocity are expressed by the condition that the momenta
shall fall between the limits $p_{1}$ and $p_{1} + dp_{1}$,~\dots $p_{n}$ and
$p_{n} + dp_{n}$ the extension-in-velocity within those limits will be
\[
\Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\]
and we may express the probability in question by
\[
e^{\eta_{p}}\, \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n}.
\Tag{(162)}
\]
This may be regarded as defining~$\eta_{p}$.

The probability that a system which is only specified as
having a configuration within certain infinitesimal limits shall
also fall within any given limits of velocity will be expressed
by the multiple integral
\[
\ints e^{\eta_{p}}\, \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\Tag{(163)}
\]
or its equivalent
\[
\ints e^{\eta_{p}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n},
\Tag{(164)}
\]
taken within the given limits.

It follows that the probability that the system will fall
within the limits of velocity, $\dot{q}_{1}$~and $\dot{q}_{1} + d\dot{q}_{1}$,~\dots $\dot{q}_{n}$~and
$\dot{q}_{n} + d\dot{q}_{n}$ is expressed by
\[
e^{\eta_{p}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n}.
\Tag{(165)}
\]

The value of the integrals \Eq{(163)}, \Eq{(164)} is independent of
the system of coördinates and momenta which is used, as is
also the value of the same integrals without the factor~$e^{\eta_{p}}$;
therefore the value of~$\eta_{p}$ must be independent of the
system of coördinates and momenta. We may call~$e^{\eta_{p}}$ the
\emph{coefficient of probability of velocity}, and $\eta_{p}$~the \emph{index of probability
of velocity}.
\PageSep{63}

Comparing \Eq{(160)} and \Eq{(162)} with~\Eq{(40)}, we get
\[
e^{\eta_{q}} e^{\eta_{p}} = P = e^{\eta}
\Tag{(166)}
\]
or
\[
\eta_{q} + \eta_{p} = \eta.
\Tag{(167)}
\]
That is: the product of the coefficients of probability of configuration
and of velocity is equal to the coefficient of probability
of phase; the sum of the indices of probability of
configuration and of velocity is equal to the index of
probability of phase.

It is evident that $e^{\eta_{q}}$~and $e^{\eta_{p}}$ have the dimensions of the
reciprocals of extension-in-configuration and extension-in-velocity
respectively, \ie, the dimensions of $t^{-n} \eps^{-\efrac{n}{2}}$ and~$\eps^{-\efrac{n}{2}}$,
where $t$~\Chg{represent}{represents} any time, and $\eps$~any energy. If, therefore,
the unit of time is multiplied by~$c_{t}$, and the unit of energy by~$c_{\eps}$,
every~$\eta_{q}$ will be increased by the addition of
\[
n \log c_{t} + \tfrac{1}{2}n \log c_{\eps},
\Tag{(168)}
\]
and every~$\eta_{p}$ by the addition of
\[
\tfrac{1}{2}n \log c_{\eps}.\footnotemark
\Tag{(169)}
\]
\footnotetext{Compare \Eq{(47)} in Chapter~I\@.}%

It should be observed that the quantities which have been
called \emph{extension-in-configuration} and \emph{extension-in-velocity} are
not, as the terms might seem to imply, purely geometrical or
kinematical conceptions. To express their nature more fully,
they might appropriately have been called, respectively, the
\emph{dynamical measure of the extension in configuration}, and the
\emph{dynamical measure of the extension in velocity}. They depend
upon the masses, although not upon the forces of the
system. In the simple case of material points, where each
point is limited to a given space, the extension-in-configuration
is the product of the volumes within which the several points
are confined (these may be the same or different), multiplied
by the square root of the cube of the product of the masses of
the several points. The extension-in-velocity for such systems
is most easily defined as the extension-in-configuration of
systems which have moved from the same configuration for
the unit of time with the given velocities.
\PageSep{64}

In the general case, the notions of extension-in-configuration
and extension-in-velocity may be connected as follows.

If an ensemble of similar systems of $n$~degrees of freedom
have the same configuration at a given instant, but are distributed
throughout any finite extension-in-velocity, the same
ensemble after an infinitesimal interval of time~$\delta t$ will be
distributed throughout an extension in configuration equal to
its original extension-in-velocity multiplied by~$\delta t^{n}$.

In demonstrating this theorem, we shall write $q_{1}'$,~\dots $q_{n}'$ for
the initial values of the coördinates. The final values will
evidently be connected with the initial by the equations
\[
q_{1} - q_{1}' = \dot{q}_{1}\, \delta t,\quad \dots\Add{,} \quad
q_{n} - q_{n}' = \dot{q}_{n}\, \delta t.
\Tag{(170)}
\]
Now the original extension-in-velocity is by definition represented
by the integral
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n},
\Tag{(171)}
\]
where the limits may be expressed by an equation of the form
\[
F(\dot{q}_{1}, \dots \dot{q}_{n}) = 0.
\Tag{(172)}
\]
The same integral multiplied by the constant~$\delta t^{n}$ may be
written
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, d(\dot{q}_{1}\, \delta t)\Chg{,}{} \dots d(\dot{q}_{n}\, \delta t),
\Tag{(173)}
\]
and the limits may be written
\[
F(\dot{q}_{1}, \dots \dot{q}_{n}) = f(\dot{q}_{1}\, \delta t, \dots \dot{q}_{n}\, \delta t) = 0.
\Tag{(174)}
\]
(It will be observed that $\delta t$ as well as~$\Delta_{\dot{q}}$ is constant in the
integrations.) Now this integral is identically equal to
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, d(q_{1} - q_{1}') \dots d(q_{n} - q_{n}'),
\Tag{(175)}
\]
or its equivalent
\[
\ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(176)}
\]
with limits expressed by the equation
\[
f(q_{1} - q_{1}', \dots q_{n} - q_{n}') = 0.
\Tag{(177)}
\]
\PageSep{65}
But the systems which initially had velocities satisfying the
equation~\Eq{(172)} will after the interval~$\delta t$ have configurations
satisfying equation~\Eq{(177)}. Therefore the extension-in-configuration
represented by the last integral is that which
belongs to the systems which originally had the extension-in-velocity
represented by the integral~\Eq{(171)}.

Since the quantities which we have called extensions-in-phase,
extensions-in-configuration, and extensions-in-velocity
are independent of the nature of the system of coördinates
used in their definitions, it is natural to seek definitions which
shall be independent of the use of any coördinates. It will be
sufficient to give the following definitions without formal proof
of their equivalence with those given above, since they are
less convenient for use than those founded on systems of coördinates,
and since we shall in fact have no occasion to use
them.

We commence with the definition of extension-in-velocity.
We may imagine $n$~independent velocities, $V_{1}$,~\dots $V_{n}$ of which a
system in a given configuration is capable. We may conceive
of the system as having a certain velocity~$V_{0}$ combined with a
part of each of these velocities $V_{1}$,~\dots $V_{n}$. By a part of~$V_{1}$ is
meant a velocity of the same nature as~$V_{1}$ but in amount being
anything between zero and~$V_{1}$. Now all the velocities which
may be thus described may be regarded as forming or lying in
a certain extension of which we desire a measure. The case
is greatly simplified if we suppose that certain relations exist
between the velocities $V_{1}$,~\dots $V_{n}$ viz\Add{.}: that the kinetic energy
due to any two of these velocities combined is the sum of the
kinetic energies due to the velocities separately. In this case
the extension-in-motion is the square root of the product of
the doubled kinetic energies due to the $n$~velocities $V_{1}$,~\dots $V_{n}$
taken separately.

The more general case may be reduced to this simpler case
as follows. The velocity~$V_{2}$ may always be regarded as
composed of two velocities $V_{2}'$~and $V_{2}''$, of which $V_{2}'$~is of
the same nature as~$V_{1}$, (it may be more or less in amount, or
opposite in sign,) while $V_{2}''$~satisfies the relation that the
\PageSep{66}
kinetic energy due to $V_{1}$~and $V_{2}''$ combined is the sum of the
kinetic energies due to these velocities taken separately. And
the velocity~$V_{3}$ may be regarded as compounded of three,
$V_{3}'$, $V_{3}''$, $V_{3}'''$, of which $V_{3}'$~is of the same nature as~$V_{1}$, $V_{3}''$~of
the same nature as~$V_{2}''$, while $V_{3}'''$~satisfies the relations
that if combined either with $V_{1}$ or $V_{2}''$ the kinetic energy of
the combined velocities is the sum of the kinetic energies of
the velocities taken separately. When all the velocities
$V_{2}$,~\dots $V_{n}$ have been thus decomposed, the square root of the
product of the doubled kinetic energies of the several velocities
$V_{1}$, $V_{2}''$. $V_{3}'''$, etc., will be the value of the extension-in-velocity
which is sought.

This method of evaluation of the extension-in-velocity which
we are considering is perhaps the most simple and natural, but
the result may be expressed in a more symmetrical form. Let
us write~$\eps_{12}$ for the kinetic energy of the velocities $V_{1}$~and $V_{2}$
combined, diminished by the sum of the kinetic energies due
to the same velocities taken separately. This may be called
the mutual energy of the velocities $V_{1}$ and~$V_{2}$. Let the
mutual energy of every pair of the velocities $V_{1}$,~\dots $V_{n}$ be
expressed in the same way. Analogy would make $\eps_{11}$~represent
the energy of twice~$V_{1}$ diminished by twice the energy of~$V_{1}$,
\ie, $\eps_{11}$~would represent twice the energy of~$V_{1}$, although the
term mutual energy is hardly appropriate to this case. At all
events, let $\eps_{11}$~have this signification, and $\eps_{22}$~represent twice
the energy of~$V_{2}$, etc. The square root of the determinant
\[
\left|
\begin{array}{@{}cccc@{}}
\eps_{11} & \eps_{12} & \cdots & \eps_{1n} \\
\eps_{21} & \eps_{22} & \cdots & \eps_{2n} \\
\cdots & \cdots & \cdots & \cdots \\
\eps_{n1} & \eps_{n2} & \cdots & \eps_{nn} \\
\end{array}
\right|
\]
represents the value of the extension-in-velocity determined as
above described by the velocities $V_{1}$,~\dots $V_{n}$.

The statements of the preceding paragraph may be readily
proved from the expression~\Eq{(157)} on page~\Pageref{60}, viz.,
\[
\Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n}
\]
by which the notion of an element of extension-in-velocity was
\PageSep{67}
originally defined. Since $\Delta_{\dot{q}}$~in this expression represents
the determinant of which the general element is
\[
\frac{d^{2}\eps}{d\dot{q}_{i}\, d\dot{q}_{j}}
\]
the square of the preceding expression represents the determinant
of which the general element is
\[
\frac{d^{2}\eps}{d\dot{q}_{i}\, d\dot{q}_{j}}\, d\dot{q}_{i}\, d\dot{q}_{j}.
\]
Now we may regard the differentials of velocity $d\dot{q}_{i}$, $d\dot{q}_{j}$ as
themselves infinitesimal velocities. Then the last expression
represents the mutual energy of these velocities, and
\[
\frac{d^{2}\eps}{d\dot{q}_{i}^{2}}\, d\dot{q}_{i}^{2}
\]
represents twice the energy due to the velocity~$d\dot{q}_{i}$.

The case which we have considered is an extension-in-velocity
of the simplest form. All extensions-in-velocity do not
have this form, but all may be regarded as composed of
elementary extensions of this form, in the same manner as
all volumes may be regarded as composed of elementary
parallelepipeds.

Having thus a measure of extension-in-velocity founded, it
will be observed, on the dynamical notion of kinetic energy,
and not involving an explicit mention of coördinates, we may
derive from it a measure of extension-in-configuration by the
principle connecting these quantities which has been given in
a preceding paragraph of this chapter.

The measure of extension-in-phase may be obtained from
that of extension-in-configuration and of extension-in-velocity.
For to every configuration in an extension-in-phase there will
belong a certain extension-in-velocity, and the integral of the
elements of extension-in-configuration within any extension-in-phase
multiplied each by its extension-in-velocity is the
measure of the extension-in-phase.
\PageSep{68}


\Chapter{VII.}{Farther Discussion of Averages in a Canonical
Ensemble of Systems.}

\First{Returning} to the case of a canonical distribution, we have
for the index of probability of configuration
\[
\eta_{q} = \frac{\psi_{q} - \eps_{q}}{\Theta}
\Tag{(178)}
\]
as appears on comparison of formulae \Eq{(142)} and~\Eq{(161)}. It
follows immediately from~\Eq{(142)} that the average value in the
ensemble of any quantity~$u$ which depends on the configuration
alone is given by the formula
\[
\bar{u} = \intac u\, e^{\efrac{\psi_{q} - \eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(179)}
\]
where the integrations cover all possible configurations. The
value of~$\psi_{q}$ is evidently determined by the equation
\[
e^{-\efrac{\psi_{q}}{\Theta}} = \intac e^{-\efrac{\eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n}.
\Tag{(180)}
\]

By differentiating the last equation we may obtain results
analogous to those obtained in Chapter~IV\Add{.} from the equation
\[
e^{-\efrac{\psi}{\Theta}} = \intap e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n}.
\]
As the process is identical, it is sufficient to give the results:
\[
d\psi_{q} = \bar{\eta}_{q}\, d\Theta - \bar{A}_{1}\, da_{1} - \bar{A}_{2}\, da_{2} - \text{etc.},
\Tag{(181)}
\]
\PageSep{69}
or, since
\[
\psi_{q} = \bar{\eps}_{q} + \Theta \bar{\eta}_{q},
\Tag{(182)}
\]
and
\begin{gather*}
d\psi_{q} = d\bar{\eps}_{q} + \bar{\eta}_{q}\, d\Theta + \Theta\, d\bar{\eta}_{q},
\Tag{(183)} \\
d\bar{\eps}_{q} = -\Theta\, d\bar{\eta}_{q} - \bar{A}_{1}\, da_{1} - \bar{A}_{2}\, da_{2} - \text{etc.}
\Tag{(184)}
\end{gather*}
It appears from this equation that the differential relations
subsisting between the average potential energy in an ensemble
of systems canonically distributed, the modulus of distribution,
the average index of probability of configuration, taken
negatively, and the average forces exerted on external bodies,
are equivalent to those enunciated by Clausius for the potential
energy of a body, its temperature, a quantity which he called
the disgregation, and the forces exerted on external bodies.\footnote
  {Pogg.\ Ann.\, Bd.~CXVI, S.~73, (1862); ibid., Bd.~CXXV, S.~353, (1865),
  See also Boltzmann, Sitzb.\ der Wiener Akad., Bd.~LXIII, S.~728, (1871).}%

For the index of probability of velocity, in the case of canonical
distribution, we have by comparison of \Eq{(144)} and~\Eq{(163)},
or of \Eq{(145)} and~\Eq{(164)},
\[
\eta_{p} = \frac{\psi_{p} - \eps_{p}}{\Theta}\Add{,}
\Tag{(185)}
\]
which gives
\[
\bar{\eta}_{p} = \frac{\psi_{p} - \bar{\eps}_{p}}{\Theta};
\Tag{(186)}
\]
we have also
\[
\bar{\eps}_{p} = \tfrac{1}{2}n \Theta,
\Tag{(187)}
\]
and by~\Eq{(140)},
\[
\psi_{p} = -\tfrac{1}{2}n \Theta \log (2\pi \Theta).
\Tag{(188)}
\]
From these equations we get by differentiation
\[
d\psi_{p} = \bar{\eta}_{p}\, d\Theta,
\Tag{(189)}
\]
and
\[
d\bar{\eps}_{p} = -\Theta\, d\bar{\eta}_{p}.
\Tag{(190)}
\]
The differential relation expressed in this equation between
the average kinetic energy, the modulus, and the average index
of probability of velocity, taken negatively, is identical with
that given by Clausius \textit{locis citatis} for the kinetic energy of a
body, the temperature, and a quantity which he called the
transformation-value of the kinetic energy.\footnote
  {Verwandlungswerth des Wärmeinhaltes.}
The relations
\[
\bar{\eps} = \bar{\eps}_{q} + \bar{\eps}_{p},\quad
\bar{\eta} = \bar{\eta}_{q} + \bar{\eta}_{p}
\]
\PageSep{70}
are also identical with those given by Clausius for the corresponding
quantities.

Equations \Eq{(112)} and \Eq{(181)} show that if $\psi$~or $\psi_{q}$ is known
as function of~$\Theta$ and $a_{1}$, $a_{2}$, etc., we can obtain by differentiation
$\bar{\eps}$~or~$\bar{\eps}_{q}$, and $\bar{A}_{1}$, $\bar{A}_{2}$, etc.\ as functions of the same variables.
We have in fact
\begin{align*}
\bar{\eps} &= \psi - \Theta \bar{\eta} = \psi - \Theta\, \frac{d\psi}{d\Theta}\Chg{.}{,}
\Tag{(191)} \\
\bar{\eps}_{q} &= \psi_{q} - \Theta \bar{\eta}_{q} = \psi_{q} - \Theta\, \frac{d\psi_{q}}{d\Theta}.
\Tag{(192)} \\
\end{align*}
The corresponding equation relating to kinetic energy,
\[
\bar{\eps}_{p} = \psi_{p} - \Theta \Typo{\eta}{\bar{\eta}}_{p} = \psi_{p} - \Theta\, \frac{d\psi_{p}}{d\Theta}\Typo{.}{,}
\Tag{(193)}
\]
which may be obtained in the same way, may be verified by
the known relations \Eq{(186)}, \Eq{(187)}, and \Eq{(188)} between the
variables. We have also
\[
\bar{A}_{1} = -\frac{d\psi}{da_{1}} = -\frac{d\psi_{q}}{da_{1}},
\Tag{(194)}
\]
etc., so that the average values of the external forces may be
derived alike from~$\psi$ or from~$\psi_{q}$.

The average values of the squares or higher powers of the
energies (total, potential, or kinetic) may easily be obtained by
repeated differentiations of $\psi$, $\psi_{q}$, $\psi_{p}$, or $\bar{\eps}$, $\bar{\eps}_{q}$, $\bar{\eps}_{p}$, with
respect to~$\Theta$. By equation~\Eq{(108)} we have
\[
\bar{\eps} = \intap \eps\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n},
\Tag{(195)}
\]
and differentiating with respect to~$\Theta$,
\[
\frac{d\bar{\eps}}{d\Theta}
  = \intap \left(\frac{\eps^{2} - \psi\eps}{\Theta^{2}} + \frac{\eps}{\Theta}\, \frac{d\psi}{d\Theta}\right) e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n},
\Tag{(196)}
\]
whence, again by~\Eq{(108)},
\[
\frac{d\bar{\eps}}{d\Theta} = \frac{\bar{\eps}^{2} - \psi\bar{\eps}}{\Theta^{2}} + \frac{\bar{\eps}}{\Theta}\, \frac{d\psi}{d\Theta},
\]
\PageSep{71}
or
\[
\Bar{\eps^{2}} = \Theta^{2} \frac{d\bar{\eps}}{d\Theta} + \bar{\eps} \left(\psi - \Theta\, \frac{d\psi}{d\Theta}\right).
\Tag{(197)}
\]
Combining this with~\Eq{(191)},
\[
\Bar{\eps^{2}} = \bar{\eps}^{2} + \Theta^{2} \frac{d\bar{\eps}}{d\Theta}
  = \left(\psi - \Theta\, \frac{d\psi}{d\Theta}\right)^{2} - \Theta^{3}\, \frac{d^{2}\psi}{d\Theta^{2}}.
\Tag{(198)}
\]

In precisely the same way, from the equation
\[
\bar{\eps}_{q} = \intac \eps_{q}\, e^{\efrac{\psi_{q} - \eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(199)}
\]
we may obtain
\[
\Bar{\eps_{q}^{2}} = \bar{\eps}_{q}^{2} + \Theta^{2} \frac{d\bar{\eps}_{q}}{d\Theta}
  = \left(\psi_{q} - \Theta\, \frac{d\psi_{q}}{d\Theta}\right)^{2} - \Theta^{3}\, \frac{d^{2}\psi_{q}}{d\Theta^{2}}.
\Tag{(200)}
\]

In the same way also, if we confine ourselves to a particular
configuration, from the equation
\[
\bar{\eps}_{p} = \intav \eps_{p}\, e^{\efrac{\psi_{p} - \eps_{p}}{\Theta}}\, \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\Tag{(201)}
\]
we obtain
\[
\Bar{\eps_{p}^{2}} = \bar{\eps}_{p}^{2} + \Theta^{2} \frac{d\eps_{p}}{d\Theta}
  = \left(\psi_{p} - \Theta\, \frac{d\psi_{p}}{d\Theta}\right)^{2} - \Theta^{3}\, \frac{d^{2}\psi_{p}}{d\Theta^{2}},
\Tag{(202)}
\]
which by~\Eq{(187)} reduces to
\[
\Bar{\eps_{p}^{2}} = (\tfrac{1}{4}n^{2} + \tfrac{1}{2}n) \Theta^{2}.
\Tag{(203)}
\]
Since this value is independent of the configuration, we see
that the average square of the kinetic energy for every configuration
is the same, and therefore the same as for the whole
ensemble. Hence $\Bar{\eps_{p}^{2}}$~may be interpreted as the average either
for any particular configuration, or for the whole ensemble.
It will be observed that the value of this quantity is determined
entirely by the modulus and the number of degrees of
freedom of the system, and is in other respects independent of
the nature of the system.

Of especial importance are the anomalies of the energies, or
their deviations from their average values. The average value
\PageSep{72}
of these anomalies is of course zero. The natural measure of
such anomalies is the square root of their average square. Now
\[
\Bar{(\eps - \bar{\eps})^{2}} = \Bar{\eps^{2}} - \bar{\eps}^{2},
\Tag{(204)}
\]
identically. Accordingly
\[
\Bar{(\eps - \bar{\eps})^{2}}
  = \Theta^{2} \frac{d\bar{\eps}}{d\Theta} = -\Theta^{3}\, \frac{d^{2}\psi}{d\Theta^{2}}.
\Tag{(205)}
\]
In like manner,
\begin{gather*}
\Bar{(\eps_{q} - \bar{\eps}_{q})^{2}}
  = \Theta^{2} \frac{d\bar{\eps}_{q}}{d\Theta} = -\Theta^{3}\, \frac{d^{2}\psi_{q}}{d\Theta^{2}}\Chg{.}{,}
\Tag{(206)} \\
\Bar{(\eps_{p} - \bar{\eps}_{p})^{2}}
  = \Theta^{2} \frac{d\bar{\eps}_{p}}{d\Theta} = -\Theta^{3}\, \frac{d^{2}\psi_{p}}{d\Theta^{2}} = \tfrac{1}{2}n \Theta^{2}.
\Tag{(207)}
\end{gather*}
Hence
\[
\Bar{(\eps - \bar{\eps})^{2}}
  = \Bar{(\eps_{q} - \bar{\eps}_{q})^{2}} + \Bar{(\eps_{p} - \bar{\eps}_{p})^{2}}.
\Tag{(208)}
\]
Equation~\Eq{(206)} shows that the value of $d\bar{\eps}_{q}/d\Theta$ can never be
negative, and that the value of $d^{2}\psi_{q}/d\Theta^{2}$ or $d\bar{\eta}_{q}/d\Theta$ can never
be positive.\footnote
  {In the case discussed in the note on page~\Pageref{54}, in which the potential
  energy is a quadratic function of the~$q$'s, and~$\Delta_{\dot{q}}$ independent of the~$q$'s, we
  should get for the potential energy
  \[
  \Bar{(\eps_{q} - \bar{\eps}_{q})^{2}} = \tfrac{1}{2}n \Theta^{2},
  \]
  and for the total energy
  \[
  \Bar{(\eps - \bar{\eps})^{2}} = n\Theta^{2}.
  \]

  We may also write in this case,
  \begin{align*}% [** TN: Set on two lines in the original]
    \frac{\Fsp\Bar{(\eps_{q} - \bar{\eps}_{q})^{2}}\Fsp}{(\bar{\eps}_{q} - \eps_{\alpha})^{2}} &= \frac{2}{n}, &
    \frac{\Fsp\Bar{(\eps - \bar{\eps})^{2}}\Fsp}{(\bar{\eps} - \eps_{\alpha})^{2}} &= \frac{1}{n}. \\
  \end{align*}}%

To get an idea of the order of magnitude of these quantities,
we may use the average kinetic energy as a term of comparison,
this quantity being independent of the arbitrary constant involved
in the definition of the potential energy. Since
\PageSep{73}
\[
\bar{\eps}_{p} = \tfrac{1}{2}n \Theta,
\]
\begin{align*}
  \frac{\Fsp\Bar{(\eps_{p} - \bar{\eps}_{p})^{2}}\Fsp}{\bar{\eps}_{p}^{2}} &= \frac{2}{n},
\Tag{(209)} \\
  \frac{\Fsp\Bar{(\eps_{q} - \bar{\eps}_{q})^{2}}\Fsp}{\bar{\eps}_{p}^{2}} &= \frac{2}{n}\, \frac{d\bar{\eps}_{q}}{d\bar{\eps}_{p}},
\Tag{(210)} \\
  \frac{\Fsp\Bar{(\eps - \bar{\eps})^{2}}\Fsp}{\bar{\eps}_{p}^{2}} &= \frac{2}{n}\, \frac{d\bar{\eps}}{d\bar{\eps}_{p}} = \frac{2}{n} + \frac{2}{n}\, \frac{d\bar{\eps}_{q}}{d\bar{\eps}_{p}}.
\Tag{(211)} \\
\end{align*}

These equations show that when the number of degrees of
freedom of the systems is very great, the mean squares of the
anomalies of the energies (total, potential, and kinetic) are very
small in comparison with the mean square of the kinetic
energy, unless indeed the differential coefficient $d\bar{\eps}_{q}/d\bar{\eps}_{p}$ is
of the same order of magnitude as~$n$. Such values of $d\bar{\eps}_{q}/d\bar{\eps}_{p}$
can only occur within intervals $(\bar{\eps}_{p}'' - \bar{\eps}_{p}')$ which are of the order
of magnitude of~$n^{-1}$, unless it be in cases in which $\bar{\eps}_{q}$~is in
general of an order of magnitude higher than~$\bar{\eps}_{p}$. Postponing
for the moment the consideration of such cases, it will be interesting
to examine more closely the case of large values of
$d\bar{\eps}_{q}/d\bar{\eps}_{p}$ within narrow limits. Let us suppose that for $\bar{\eps}_{p}'$ and
$\bar{\eps}_{p}''$ the value of $d\bar{\eps}_{q}/d\bar{\eps}_{p}$ is of the order of magnitude of unity,
but between these values of~$\bar{\eps}_{p}$ very great values of the differential
coefficient occur. Then in the ensemble having modulus~$\Theta''$
and average energies $\bar{\eps}_{p}''$~and $\bar{\eps}_{q}''$, values of~$\eps_{q}$ sensibly greater
than~$\bar{\eps}_{q}''$ will be so rare that we may call them practically negligible.
They will be still more rare in an ensemble of less
modulus. For if we differentiate the equation
\[
\eta_{q} = \frac{\psi_{q} - \eps_{q}}{\Theta}
\]
regarding $\eps_{q}$~as constant, but $\Theta$~and therefore~$\psi_{q}$ as variable,
we get
\[
\left(\frac{d\eta_{q}}{d\Theta}\right)_{\eps_{q}}
  = \frac{1}{\Theta}\, \frac{d\psi_{q}}{d\Theta} - \frac{\psi_{q} - \eps_{q}}{\Theta^{2}},
\Tag{(212)}
\]
whence by~\Eq{(192)}
\[
\left(\frac{d\eta_{q}}{d\Theta}\right)_{\eps_{q}}
  = \frac{\eps_{q} - \bar{\eps}_{q}}{\Theta^{2}}.
\Tag{(213)}
\]
\PageSep{74}
That is, a diminution of the modulus will diminish the probability
of all configurations for which the potential energy exceeds
its average value in the ensemble. Again, in the ensemble
having modulus~$\Theta'$ and average energies $\bar{\eps}_{p}'$ and~$\bar{\eps}_{q}'$, values of
$\eps_{q}$ sensibly less than~$\bar{\eps}_{q}'$ will be so rare as to be practically negligible.
They will be still more rare in an ensemble of greater
modulus, since by the same equation an increase of the
modulus will diminish the probability of configurations for
which the potential energy is less than its average value in
the ensemble. Therefore, for values of~$\Theta$ between $\Theta'$ and~$\Theta''$,
and of~$\bar{\eps}_{p}$ between $\bar{\eps}_{p}'$ and~$\bar{\eps}_{p}''$, the individual values of~$\eps_{q}$ will
be practically limited to the interval between $\eps_{q}'$ and~$\eps_{q}''$.

In the cases which remain to be considered, viz., when
$d\bar{\eps}_{q}/d\bar{\eps}_{p}$ has very large values not confined to narrow limits,
and consequently the differences of the mean potential energies
in ensembles of different moduli are in general very large
compared with the differences of the mean kinetic energies, it
appears by~\Eq{(210)} that the anomalies of mean square of potential
energy, if not small in comparison with the mean kinetic
energy, will yet in general be very small in comparison with
differences of mean potential energy in ensembles having
moderate differences of mean kinetic energy,---the exceptions
being of the same character as described for the case when
$d\bar{\eps}_{q}/d\bar{\eps}_{p}$ is not in general large.

It follows that to human experience and observation with
respect to such an ensemble as we are considering, or with
respect to systems which may be regarded as taken at random
from such an ensemble, when the number of degrees of freedom
is of such order of magnitude as the number of molecules
in the bodies subject to our observation and experiment, $\eps - \bar{\eps}$,
$\eps_{p} - \bar{\eps}_{p}$, $\eps_{q} - \bar{\eps}_{q}$ would be in general vanishing quantities,
since such experience would not be wide enough to embrace
the more considerable divergencies from the mean values, and
such observation not nice enough to distinguish the ordinary
divergencies. In other words, such ensembles would appear
to human observation as ensembles of systems of uniform
energy, and in which the potential and kinetic energies (supposing
\PageSep{75}
that there were means of measuring these quantities
separately) had each separately uniform values.\footnote
  {This implies that the kinetic and potential energies of individual systems
  would each separately have values sensibly constant in time.}
Exceptions
might occur when for particular values of the modulus the
differential coefficient $d\bar{\eps}_{q}/d\bar{\eps}_{p}$ takes a very large value. To
human observation the effect would be, that in ensembles in
which $\Theta$~and $\bar{\eps}_{p}$ had certain critical values, $\bar{\eps}_{q}$~would be indeterminate
within certain limits, viz., the values which would
correspond to values of $\Theta$~and $\eps_{p}$ slightly less and slightly
greater than the critical values. Such indeterminateness corresponds
precisely to what we observe in experiments on the
bodies which nature presents to us.\footnote
  {As an example, we may take a system consisting of a fluid in a cylinder
  under a weighted piston, with a vacuum between the piston and the top of
  the cylinder, which is closed. The weighted piston is to be regarded as a
  part of the system. (This is formally necessary in order to satisfy the condition
  of the invariability of the external coördinates.) It is evident that at
  a certain temperature, viz., when the pressure of saturated vapor balances
  the weight of the piston, there is an indeterminateness in the values of the
  potential and total energies as functions of the temperature.}%

To obtain general formulae for the average values of powers
of the energies, we may proceed as follows. If $h$~is any positive
whole number, we have identically
\[
\intap \eps^{h}\, e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n}
  = \Theta^{2} \frac{d}{d\Theta} \intap \eps^{h-1}\, e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n},
\Tag{(214)}
\]
\ie, by~\Eq{(108)},
\[
\Bar{\eps^{h}}\, e^{-\efrac{\psi}{\Theta}}
  = \Theta^{2} \frac{d}{d\Theta} \left(\Bar{\eps^{h-1}}\, e^{-\efrac{\psi}{\Theta}}\right).
\Tag{(215)}
\]
Hence
\[
\Bar{\eps^{h}}\, e^{-\efrac{\psi}{\Theta}}
  = \left(\Theta^{2} \frac{d}{d\Theta}\right)^{h} e^{-\efrac{\psi}{\Theta}},
\Tag{(216)}
\]
and
\[
\Bar{\eps^{h}}
  = e^{\efrac{\psi}{\Theta}} \left(\Theta^{2} \frac{d}{d\Theta}\right)^{h} e^{-\efrac{\psi}{\Theta}}.
\Tag{(217)}
\]
\PageSep{76}
For $h = 1$, this gives
\[
\bar{\eps}
  = -\Theta^{2} \frac{d}{d\Theta} \left(\frac{\psi}{\Theta}\right)\Add{,}
\Tag{(218)}
\]
which agrees with~\Eq{(191)}.

% [** TN: Newline but no indent in the original]
From~\Eq{(215)} we have also
\begin{gather*}
\Bar{\eps^{h}} = \bar{\eps}\, \Bar{\eps^{h-1}} + \Theta^{2} \frac{d\Bar{\eps^{h-1}}}{d\Theta}
  = \left(\bar{\eps} + \Theta^{2} \frac{d}{d\Theta}\right) \eps^{h-1},
\Tag{(219)} \\
\Bar{\eps^{h}} = \left(\bar{\eps} + \Theta^{2} \frac{d}{d\Theta}\right)^{h-1} \bar{\eps}.
\Tag{(220)} \\
\end{gather*}

In like manner from the identical equation
% [** TN: Set on one line in the original]
\begin{multline*}
\intac \eps_{q}^{h}\, e^{-\efrac{\eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n} \\
  = \Theta^{2} \frac{d}{d\Theta} \intac \eps_{q}^{h-1}\, e^{-\efrac{\eps_{q}}{\Theta}}\, \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(221)}
\end{multline*}
we get
\[
\Bar{\eps_{q}^{h}} = e^{\efrac{\psi_{q}}{\Theta}} \left(\Theta^{2} \frac{d}{d\Theta}\right)^{h} e^{-\efrac{\psi_{q}}{\Theta}},
\Tag{(222)}
\]
and
\[
\Bar{\eps_{q}^{h}} = \left(\bar{\eps}_{q} + \Theta^{2} \frac{d}{d\Theta}\right)^{h-1} \bar{\eps}_{q}.
\Tag{(223)} \\
\]

With respect to the kinetic energy similar equations will
hold for averages taken for any particular configuration, or
for the whole ensemble. But since
\[
\bar{\eps}_{p} = \frac{n}{2} \Theta,
\]
the equation
\[
\Bar{\eps_{p}^{h}} = \left(\bar{\eps}_{p} + \Theta^{2} \frac{d}{d\Theta}\right) \Bar{\eps_{p}^{h-1}}
\Tag{(224)}
\]
reduces to
\[
\Bar{\eps_{p}^{h}} = \left(\frac{n}{2} \Theta + \Theta^{2} \frac{d}{d\Theta}\right) \Typo{\Bar{\eps^{h-1}}}{\Bar{\eps_{p}^{h-1}}}
  = \frac{n}{2} \left(\frac{n}{2}\Theta + \Theta^{2} \frac{d}{d\Theta}\right)^{h-1} \Theta.
\Tag{(225)}
\]
\PageSep{77}
We have therefore
\begin{gather*}
\Bar{\eps_{p}^{2}} = \left(\frac{n}{2} + 1\right) \frac{n}{2} \Theta^{2}.
\Tag{(226)}\displaybreak[0] \\
%
\Bar{\eps_{p}^{3}} = \left(\frac{n}{2} + 2\right) \left(\frac{n}{2} + 1\right) \frac{n}{2} \Theta^{3}.
\Tag{(227)}\displaybreak[0] \\
\Bar{\eps_{p}^{h}} = \frac{\Gamma(\frac{1}{2} n + h)}{\Gamma(\frac{1}{2} n)}\, \Theta^{h}.\footnotemark
\Tag{(228)}
\end{gather*}
\footnotetext{In the case discussed in the note on page~\Pageref{54} we may easily get
  \[
  \Bar{(\eps_{q} - \eps_{\alpha})^{h}}
  = \left(\bar{\eps}_{q} - \eps_{\alpha} + \Theta^{2} \frac{d}{d\Theta}\right) \Bar{(\eps_{q} - \eps_{\alpha})^{h-1}},
  \]
  which, with
  \[
  \bar{\eps}_{q} - \eps_{\alpha} = \frac{n}{2} \Theta,
  \]
  gives
  \[
  \Bar{(\eps_{q} - \eps_{\alpha})^{h}}
  = \left(\frac{n}{2} \Theta + \Theta^{2} \frac{d}{d\Theta}\right) \Bar{(\eps_{q} - \eps_{\alpha})^{h-1}}
  = \frac{n}{2} \left(\frac{n}{2} \Theta + \Theta^{2} \frac{d}{d\Theta}\right)^{h-1} \Theta.
  \]
  Hence
  \[
  \Bar{(\eps_{q} - \eps_{\alpha})^{h}} = \Bar{\eps_{p}^{h}}.
  \]
  Again
  \[
  \Bar{(\eps - \eps_{\alpha})^{h}} = \left(\bar{\eps} - \eps_{\alpha} + \Theta^{2} \frac{d}{d\Theta}\right) \Bar{(\eps - \eps_{\alpha})^{h-1}},
  \]
  which with
  \[
  \bar{\eps} - \eps_{\alpha} = n \Theta
  \]
  gives
  \[
  \Bar{(\eps - \eps_{\alpha})^{h}}
  = \left(n \Theta + \Theta^{2} \frac{d}{d\Theta}\right) \Bar{(\eps - \eps_{\alpha})^{h-1}}
  = n \left(n \Theta + \Theta^{2} \frac{d}{d\Theta}\right)^{h-1} \Theta,
  \]
  hence
  \[
  \Bar{(\eps - \eps_{\alpha})^{h}}
  = \frac{\Gamma(n + h)}{\Gamma(n)}\, \Theta^{h}.
  \]}%

The average values of the powers of the anomalies of the
energies are perhaps most easily found as follows. We have
identically, since $\bar{\eps}$~is a function of~$\Theta$, while $\eps$~is a function of
the $p$'s and~$q$'s,
\begin{multline*}
\Theta^{2} \frac{d}{d\Theta} \intap (\eps - \bar{\eps})^{h}\, e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n} \\
  = \intap \left[\eps (\eps - \bar{\eps})^{h} - h (\eps - \bar{\eps})^{h-1} \Theta^{2} \frac{d\bar{\eps}}{d\Theta}\right] e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n}\Add{,}
\Tag{(229)}
\end{multline*}
\ie, by~\Eq{(108)},
\[
\Theta^{2} \frac{d}{d\Theta} \left[\Bar{(\eps - \bar{\eps})^{h}} e^{-\efrac{\psi}{\Theta}}\right]
  = \left[\Bar{\eps (\eps - \bar{\eps})^{h}} - h \Bar{(\eps - \bar{\eps})^{h-1}} \Theta^{2} \frac{d\bar{\eps}}{d\Theta}\right] e^{-\efrac{\psi}{\Theta}},
\Tag{(230)}
\]
\PageSep{78}
or since by~\Eq{(218)}
\begin{gather*}
\Theta^{2} \frac{d}{d\Theta} e^{-\efrac{\psi}{\Theta}}
  = \bar{\eps}\, e^{-\efrac{\psi}{\Theta}}, \\
%
\Theta^{2} \frac{d}{d\Theta} \Bar{(\eps - \bar{\eps})^{h}} + \Bar{(\eps - \bar{\eps})^{h}} \bar{\eps}
  = \Bar{\eps (\eps - \bar{\eps})^{h}} - h \Bar{(\eps - \bar{\eps})^{h-1}} \Theta^{2} \frac{d\bar{\eps}}{d\Theta}, \\
%
\Bar{(\eps - \bar{\eps})^{h+1}} = \Theta^{2} \frac{d}{d\Theta} \Bar{(\eps - \bar{\eps})^{h}} + h \Bar{(\eps - \bar{\eps})^{h-1}} \Theta^{2} \frac{d\bar{\eps}}{d\Theta}.
\Tag{(231)}
\end{gather*}

In precisely the same way we may obtain for the potential
energy
\[
\Bar{(\eps_{q} - \bar{\eps}_{q})^{h+1}}
  = \Theta^{2} \frac{d}{d\Theta} \Bar{(\eps_{q} - \bar{\eps}_{q})^{h}} + h \Bar{(\eps_{q} - \bar{\eps}_{q})^{h-1}} \Theta^{2} \frac{d\bar{\eps_{q}}}{d\Theta}.
\Tag{(232)}
\]

By successive applications of~\Eq{(231)} we obtain
\begin{align*}
\Bar{(\eps - \bar{\eps})^{2}} &= D\bar{\eps}\Add{,} \\
\Bar{(\eps - \bar{\eps})^{3}} &= D^{2} \bar{\eps}\Add{,} \\
\Bar{(\eps - \bar{\eps})^{4}} &= D^{3} \bar{\eps} + 3(D\bar{\eps})^{2}\Add{,} \\
\Bar{(\eps - \bar{\eps})^{5}} &= D^{4} \bar{\eps} + 10 D\bar{\eps}\, D^{2}\bar{\eps}\Add{,} \\
\Bar{(\eps - \bar{\eps})^{6}} &= D^{5} \bar{\eps} + 15 D\bar{\eps}\, D^{3}\bar{\eps} + 10(D^{2}\bar{\eps})^{2} + 15(D\bar{\eps})^{3}\ \text{etc.}\Add{,}
\end{align*}
where $D$~represents the operator $\Theta^{2} d/d\Theta$. Similar expressions
relating to the potential energy may be derived from~\Eq{(232)}.

For the kinetic energy we may write similar equations in
which the averages may be taken either for a single configuration
or for the whole ensemble. But since
\[
\frac{d\bar{\eps}_{p}}{d\Theta} = \frac{n}{2}\Add{,}
\]
the general formula reduces to
\[
\Bar{(\eps_{p} - \bar{\eps}_{p})^{h+1}}
  = \Theta^{2} \frac{d}{d\Theta} \Bar{(\eps_{p} - \bar{\eps}_{p})^{h}}
   + \tfrac{1}{2} nh \Theta^{2} \Bar{(\eps_{p} - \bar{\eps}_{p})^{h-1}}\Add{,}
\Tag{(233)}
\]
or
\[
\frac{\Bar{(\eps_{p} - \bar{\eps}_{p})^{h+1}}}{\bar{\eps}_{p}^{h+1}}
  = \frac{2\Theta}{n}\, \frac{d}{d\Theta}\, \frac{\Bar{(\eps_{p} - \bar{\eps}_{p})^{h}}}{\bar{\eps}_{p}^{h}}
  + \frac{2h}{n}\, \frac{\Bar{(\eps_{p} - \bar{\eps}_{p})^{h}}}{\bar{\eps}_{p}^{h}}
  + \frac{2h}{n}\, \frac{\Bar{(\eps_{p} - \bar{\eps}_{p})^{h-1}}}{\bar{\eps}_{p}^{h-1}}\Add{.}
\Tag{(234)}
\]
\PageSep{79}
But since identically
\[
\frac{\Fsp\Bar{(\eps_{p} - \bar{\eps}_{p})^{0}}\Fsp}{\bar{\eps}_{p}^{0}}= 1,\qquad
\frac{\Fsp\Bar{(\Typo{\eps}{\eps_{p}} - \Typo{\bar{\eps}}{\bar{\eps}_{p}})^{1}}\Fsp}{\Typo{\bar{\eps}}{\bar{\eps}_{p}}^{1}}= 0,
\]
the value of the corresponding expression for any index will
be independent of~$\Theta$ and the formula reduces to
\[
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{h+1}}
  = \frac{2h}{n} \Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{h}}
  + \frac{2h}{n} \Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{h-1}}\Add{;}
\Tag{(235)}
\]
we have therefore
\begin{align*}
&\begin{aligned}[t]
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{0}} &= 1, \\
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{1}} &= 0, \\
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{2}} &= \frac{2}{n},
\end{aligned}
&&
\begin{aligned}[t]
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{3}} &= \frac{8}{n^{2}}, \\
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{4}} &= \frac{48}{n^{3}} + \frac{12}{n^{2}}, \\
\text{etc.}\footnotemark
\end{aligned}
\end{align*}
\footnotetext{In the case discussed in the preceding foot-notes we get easily
  \[
  \Bar{(\eps_{q} - \bar{\eps}_{q})^{h}} = \Bar{(\eps_{p} - \bar{\eps}_{p})^{h}},
  \]
  and
  \[
  \Bar{\left(\frac{\eps_{q} - \bar{\eps}_{q}}{\bar{\eps}_{q} - \eps_{\alpha}}\right)^{h}}
  = \Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{h}}.
  \]

  For the total energy we have in this case
  \begin{gather*}
  \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{h+1}}
  = \frac{h}{n} \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{h}}
  + \frac{h}{n} \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{h-1}}, \\
  \begin{aligned}
  &\begin{aligned}[t]
  \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{2}} &= \frac{1}{n}, \\
  \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{3}} &= \frac{2}{n^{2}},
  \end{aligned}
  &&\begin{aligned}
  \Bar{\left(\frac{\eps - \bar{\eps}}{\bar{\eps} - \eps_{\alpha}}\right)^{4}} &= \frac{3}{n^{2}} + \frac{6}{n^{3}}, \\
  \text{etc.} &
  \end{aligned}
  \end{aligned}
\end{gather*}}%

It will be observed that when $\psi$~or $\bar{\eps}$ is given as function of~$\Theta$,
all averages of the form~$\Bar{\eps^{h}}$ or $\Bar{(\eps - \bar{\eps})^{h}}$ are thereby determined.
\PageSep{80}
So also if $\psi_{q}$~or $\bar{\eps}_{q}$ is given as a function of~$\Theta$, all
averages of the form $\Bar{\eps_{q}^{h}}$ or $\Bar{(\eps_{q} - \bar{\eps}_{q})^{h}}$ are determined. But
\[
\bar{\eps}_{q} = \bar{\eps} - \tfrac{1}{2}n \Theta.
\]
Therefore if any one of the quantities $\psi$,~$\psi_{q}$, $\bar{\eps}$,~$\bar{\eps}_{q}$ is known
as a function of~$\Theta$, and $n$~is also known, all averages of any of
the forms mentioned are thereby determined as functions of
the same variable. In any case all averages of the form
\[
\Bar{\left(\frac{\eps_{p} - \bar{\eps}_{p}}{\bar{\eps}_{p}}\right)^{h}}
\]
are known in terms of $n$~alone, and have the same value
whether taken for the whole ensemble or limited to any
particular configuration.

If we differentiate the equation
\[
\intap e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 1
\Tag{(236)}
\]
with respect to~$a_{1}$, and multiply by~$\Theta$, we have
\[
\ints \left[\frac{d\psi}{da_{1}} - \frac{d\eps}{da_{1}}\right] e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 0.
\Tag{(237)}
\]
Differentiating again, with respect to~$a_{1}$, with respect to~$a_{2}$,
and with respect to~$\Theta$, we have
\[
\ints \left[\frac{d^{2}\psi}{da_{1}^{2}} - \frac{d^{2}\eps}{da_{1}^{2}} + \frac{1}{\Theta} \left(\frac{d\psi}{da_{1}} - \frac{d\eps}{da_{1}}\right)^{2}\right] e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 0,
\Tag{(238)}
\]
\begin{multline*}
\ints \left[\frac{d^{2}\psi}{da_{1}\, da_{2}} - \frac{d^{2}\eps}{da_{1}\, da_{2}} + \frac{1}{\Theta} \left(\frac{d\psi}{da_{1}} - \frac{d\eps}{da_{1}}\right) \left(\frac{d\psi}{da_{2}} - \frac{d\eps}{da_{2}}\right)\right] \\
  e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 0,
\Tag{(239)}
\end{multline*}
\begin{multline*}
\ints \left[\frac{d^{2}\psi}{da_{1}\, d\Theta} + \left(\frac{d\psi}{da_{1}} - \frac{d\eps}{da_{1}}\right)\left(\frac{1}{\Theta}\, \frac{d\psi}{d\Theta} - \frac{\psi - \eps}{\Theta^{2}}\right)\right] \\
  e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 0.
\Tag{(240)}
\end{multline*}
\PageSep{81}
The multiple integrals in the last four equations represent the
average values of the expressions in the brackets, which we
may therefore set equal to zero. The first gives
\[
\frac{d\psi}{da_{1}} = \frac{\Bar{d\eps}}{da_{1}} = -\bar{A}_{1},
\Tag{(241)}
\]
as already obtained. With this relation and~\Eq{(191)} we get
from the other equations
\[
\Bar{(A_{1} - \bar{A}_{1})^{2}}
  = \Theta \left(\frac{\Bar{d^{2}\eps}}{da_{1}^{2}} - \frac{d^{2}\psi}{da_{1}^{2}}\right)
  = \Theta \left(\frac{d\bar{A}_{1}}{da_{1}} - \frac{\Bar{dA}_{1}}{da_{1}}\right)\Add{,}
\Tag{(242)}
\]
\begin{multline*}
\Bar{(A_{1} - \bar{A}_{1})(A_{2} - \bar{A}_{2})}
  = \Theta \biggl(\frac{\Bar{d^{2} \eps}}{da_{1}\, da_{2}} - \frac{\Bar{d^{2}\psi}}{da_{1}\, da_{2}}\biggr) \\
  = \Theta \left(\frac{d\bar{A}_{1}}{da_{2}} - \frac{\Bar{dA}_{1}}{da_{2}}\right)
  = \Theta \left(\frac{d\bar{A}_{2}}{da_{1}} - \frac{\Bar{dA}_{2}}{da_{1}}\right)\Add{,}
\Tag{(243)}
\end{multline*}
\[
\Bar{(A_{1} - \bar{A}_{1})(\eps - \bar{\eps})}
  = -\Theta^{2} \frac{d^{2}\psi}{da_{1}\, d\Theta}
  = \Theta^{2} \frac{d\bar{A}_{1}}{d\Theta}
  = -\Theta^{2} \frac{d\bar{\eta}}{da_{1}}.
\]
We may add for comparison equation~\Eq{(205)}, which might be
derived from~\Eq{(236)} by differentiating twice with respect to~$\Theta$:
\[
\Bar{(\eps - \bar{\eps})^{2}}
  = -\Theta^{2} \frac{d^{2}\psi}{d\Theta^{2}}
  = \Theta^{2} \frac{d\bar{\eps}}{d\Theta}.
\Tag{(244)}
\]
The two last equations give
\[
\Bar{(A_{1} - \bar{A}_{1})(\eps - \bar{\eps})}
  = \frac{d\bar{A}}{d\bar{\eps}}\, \Bar{(\eps - \bar{\eps})^{2}}.
\Tag{(245)}
\]

If $\psi$~or $\bar{\eps}$ is known as function of $\Theta$, $a_{1}$, $a_{2}$, etc., $\Bar{(\eps - \bar{\eps})^{2}}$ may
be obtained by differentiation as function of the same variables.
And if $\psi$, or~$\bar{A}_{1}$, or $\bar{\eta}$ is known as function of $\Theta$, $a_{1}$, etc.,
$\Bar{(A_{1} - \bar{A}_{1})(\eps - \bar{\eps})}$ may be obtained by differentiation. But
$\Bar{(A_{1} - \bar{A}_{1})^{2}}$ and $\Bar{(A_{1} - \bar{A}_{1})(A_{2} - \bar{A}_{2})}$ cannot be obtained in any
similar manner. We have seen that $\Bar{(\eps - \bar{\eps})^{2}}$ is in general a
vanishing quantity for very great values of~$n$, which we may
regard as contained implicitly in~$\Theta$ as a divisor. The same is
true of $\Bar{(A_{1} - \bar{A}_{1})(\eps - \bar{\eps})}$. It does not appear that we can
assert the same of $\Bar{(A_{1} - \bar{A}_{1})^{2}}$ or $\Bar{(A_{1} - \bar{A}_{1})(A_{2} - \bar{A}_{2})}$, since
\PageSep{82}
$d^{2}\eps/da_{1}^{2}$ may be very great. The quantities $d^{2}\eps/da_{1}^{2}$ and $d^{2}\psi/da_{1}^{2}$
belong to the class called elasticities. The former expression
represents an elasticity measured under the condition that
while $a_{1}$~is varied the internal coördinates $q_{1}$,~\dots $q_{n}$ all remain
fixed. The latter is an elasticity measured under the condition
that when $a_{1}$~is varied the ensemble remains canonically
distributed within the same modulus. This corresponds to
an elasticity in physics measured under the condition of constant
temperature. It is evident that the former is greater
than the latter, and it may be enormously greater.

The divergences of the force~$A_{1}$ from its average value are
due in part to the differences of energy in the systems of the
ensemble, and in part to the differences in the value of
the forces which exist in systems of the same energy. If we
write~$\Avg{A_{1}}$ for the average value of~$A_{1}$ in systems of the
ensemble which have any same energy, it will be determined
by the equation
\[
\Avg{A_{1}} = \frac{\dsp\ints -\frac{d\eps}{da_{1}}\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}}{\dsp\ints e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}}\Add{,}
\Tag{(246)}
\]
where the limits of integration in both multiple integrals are
two values of the energy which differ infinitely little, say $\eps$~and
$\eps + d\eps$. This will make the factor $e^{\efrac{\psi - \eps}{\Theta}}$ constant within the
limits of integration, and it may be cancelled in the numerator
and denominator, leaving
\[
\Avg{A_{1}} = \frac{\dsp\ints -\frac{d\eps}{da_{1}}\, dp_{1} \dots dq_{n}}{\dsp\ints dp_{1} \dots dq_{n}}\Add{,}
\Tag{(247)}
\]
where the integrals as before are to be taken between $\eps$~and
$\eps + d\eps$. $\Avg{A_{1}}$~is therefore independent of~$\Theta$, being a function
of the energy and the external coördinates.
\PageSep{83}

Now we have identically
\[
A_{1} - \bar{A}_{1} = (A_{1} - \Avg{A_{1}}) + (\Avg{A_{1}} - \bar{A}_{1}),
\]
where $A_{1} - \Avg{A_{1}}$ denotes the excess of the force (tending to
increase~$a_{1}$) exerted by any system above the average of such
forces for systems of the same energy. Accordingly,
\[
\Bar{(A_{1} - \bar{A}_{1})^{2}}
  = \Bar{(A_{1} - \Avg{A_{1}})^{2}}
  + 2\Bar{(A_{1} - \Avg{A_{1}})(\Avg{A_{1}} - \bar{A}_{1})}
  + \Bar{(\Avg{A_{1}} - \bar{A}_{1})^{2}}.
\]
But the average value of $(A_{1} - \Avg{A_{1}})(\Avg{A_{1}} - \bar{A}_{1})$ for systems
of the ensemble which have the same energy is zero, since for
such systems the second factor is constant. Therefore the
average for the whole ensemble is zero, and
\[
\Bar{(A_{1} - \bar{A}_{1})^{2}}
  = \Bar{(A_{1} - \Avg{A_{1}})^{2}} + \Bar{(\Avg{A_{1}} - \bar{A}_{1})^{2}}.
\Tag{(248)}
\]
In the same way it may be shown that
\[
\Bar{(A_{1} - \bar{A}_{1})(\eps - \bar{\eps})}
  = \Bar{(\Avg{A_{1}} - \bar{A}_{1})(\eps - \bar{\eps})}.
\Tag{(249)}
\]
It is evident that in ensembles in which the anomalies of
energy $\eps - \bar{\eps}$ may be regarded as insensible the same will be
true of the quantities represented by $\Avg{A_{1}} - \bar{A}_{1}$.

The properties of quantities of the form~$\Avg{A_{1}}$ will be
farther considered in Chapter~X, which will be devoted to
ensembles of constant energy.

It may not be without interest to consider some general
formulae relating to averages in a canonical ensemble, which
embrace many of the results which have been given in this
chapter.

Let $u$~be any function of the internal and external coördinates
with the momenta and modulus. We have by definition
\[
\bar{u} = \intap u\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}\Add{.}
\Tag{(250)}
\]
If we differentiate with respect to~$\Theta$, we have
\[
\frac{d\bar{u}}{d\Theta}
  = \intap \left(\frac{du}{d\Theta} - \frac{u}{\Theta^{2}}(\psi - \eps) + \frac{u}{\Theta}\, \frac{d\psi}{d\Theta}\right) e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n},
\]
\PageSep{84}
or
\[
\frac{d\bar{u}}{d\Theta}
  = \frac{\Bar{du}}{d\Theta} - \frac{\Bar{u(\psi - \eps)}}{\Theta^{2}} + \frac{\bar{u}}{\Theta}\, \frac{d\psi}{d\Theta}.
\Tag{(251)}
\]
Setting $u = 1$ in this equation, we get
\[
\frac{d\psi}{d\Theta} = \frac{\psi - \bar{\eps}}{\Theta},
\]
and substituting this value, we have
\[
\frac{d\bar{u}}{d\Theta}
  = \frac{\Bar{du}}{d\Theta} + \frac{\Bar{u\eps}}{\Theta^{2}} - \frac{\bar{u} \bar{\eps}}{\Theta^{2}}\Add{,}
\]
or
\[
\Theta^{2} \frac{d\bar{u}}{d\Theta} - \Theta^{2} \frac{\Bar{du}}{d\Theta}
  = \Bar{u\eps} - \bar{u} \bar{\eps}
  = \Bar{(u - \bar{u})(\eps - \bar{\eps})}.
\Tag{(252)}
\]

If we differentiate equation~\Eq{(250)} with respect to~$a$ (which
may represent any of the external coördinates), and write $A$
for the force~$-\dfrac{d\eps}{\Typo{dA}{da}}$, we get
\[
\frac{d\bar{u}}{da}
  = \intap \left(\frac{du}{da} + \frac{u}{\Theta}\, \frac{d\psi}{da} + \frac{u}{\Theta} A\right) e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}\Add{,}
\]
or
\[
\frac{d\bar{u}}{da}
  = \frac{\Bar{du}}{da} + \frac{\bar{u}}{\Theta}\, \frac{d\psi}{da} + \frac{\Bar{uA}}{\Theta}\Add{.}
\Tag{(253)}
\]
Setting $u = 1$ in this equation, we get
\[
\frac{d\psi}{da} = -\bar{A}.
\]
Substituting this value, we have
\[
\frac{d\bar{u}}{da}
  = \frac{\Bar{du}}{da} + \frac{\Bar{uA}}{\Theta} - \frac{\bar{u} \bar{A}}{\Theta},
\Tag{(254)}
\]
or
\[
\Theta\, \frac{d\bar{u}}{da} - \Theta\, \frac{\Bar{du}}{da}
  = \Bar{uA} - \bar{u} \bar{A}
  = \Bar{(u - \bar{u})(A - \bar{A})}.
\Tag{(255)}
\]

Repeated applications of the principles expressed by equations
\Eq{(252)} and~\Eq{(255)} are perhaps best made in the particular
cases. Yet we may write~\Eq{(252)} in this form
\PageSep{85}
\[
\Bar{(\eps + D)(u - \bar{u})},
\Tag{(256)}
\]
where $D$~represents the operator $\Theta^{2} d/d\Theta$.

% [** TN: Newline but no indent in the original]
Hence
\[
\Bar{(\eps + D)^{h}(u - \bar{u})} = 0,
\Tag{(257)}
\]
where $h$~is any positive whole number. It will be observed,
that since $\eps$~is not function of~$\Theta$, $(\eps + D)^{h}$~may be expanded by
the binomial theorem. Or, we may write
\[
\Bar{(\eps + D) u} = (\bar{\eps} + D) \bar{u},
\Tag{(258)}
\]
whence
\[
\Bar{(\eps + D)^{h} u} = (\bar{\eps} + D)^{h} \bar{u}.
\Tag{(259)}
\]
But the operator $(\bar{\eps} + D)^{h}$, although in some respects more
simple than the operator without the average sign on the~$\eps$,
cannot be expanded by the binomial theorem, since $\bar{\eps}$~is a
function of~$\Theta$ with the external coördinates.

So from equation~\Eq{(254)} we have
\[
\Bar{\left(\frac{A}{\Theta} + \frac{d}{da}\right)(u - \bar{u})} = 0,
\Tag{(260)}
\]
whence
\[
\Bar{\left(\frac{A}{\Theta} + \frac{d}{da}\right)^{h} (u - \bar{u})} = 0;
\Tag{(261)}
\]
and
\[
\Bar{\left(\frac{A}{\Theta} + \frac{d}{da}\right) u}
  = \left(\frac{\bar{A}}{\Theta} + \frac{d}{da}\right) \bar{u},
\Tag{(262)}
\]
whence
\[
\Bar{\left(\frac{A}{\Theta} + \frac{d}{da}\right)^{h} u}
  = \left(\frac{\bar{A}}{\Theta} + \frac{d}{da}\right)^{h} \bar{u}.
\Tag{(263)}
\]
The binomial theorem cannot be applied to these operators.

Again, if we now distinguish, as usual, the several external
coördinates by suffixes, we may apply successively to the
expression $u - \bar{u}$ any or all of the operators
\[
\eps + \Theta^{2} \frac{d}{d\Theta},\quad
A_{1} + \Theta\, \frac{d}{da_{1}},\quad
A_{2} + \Theta\, \frac{d}{da_{2}},\quad \text{etc.}
\Tag{(264)}
\]
\PageSep{86}
as many times as we choose, and in any order, the average
value of the result will be zero. Or, if we apply the same
operators to~$u$, and finally take the average value, it will be the
same as the value obtained by writing the sign of average
separately as~$u$, and on $\eps$, $A_{1}$, $A_{2}$, etc., in all the operators.

If $u$~is independent of the momenta, formulae similar to
the preceding, but having~$\eps_{q}$ in place of~$\eps$, may be derived
from equation~\Eq{(179)}.
\PageSep{87}


\Chapter{VIII.}{On Certain Important Functions of the
Energies of a System.}

\First{In} order to consider more particularly the distribution of a
canonical ensemble in energy, and for other purposes, it will
be convenient to use the following definitions and notations.

Let us denote by~$V$ the extension-in-phase below a certain
limit of energy which we shall call~$\eps$. That is, let
\[
V = \ints dp_{1} \dots dq_{n},
\Tag{(265)}
\]
the integration being extended (with constant values of the
external coördinates) over all phases for which the energy is
less than the limit~$\eps$. We shall suppose that the value of this
integral is not infinite, except for an infinite value of the limiting
energy. This will not exclude any kind of system to
which the canonical distribution is applicable. For if
\[
\ints e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n}
\]
taken without limits has a finite value,\footnote
  {This is a necessary condition of the canonical distribution. See
  Chapter~IV, p.~\Pageref{35}.}
the less value represented
by
\[
e^{-\efrac{\eps}{\Theta}} \ints dp_{1} \dots dq_{n}
\]
taken below a limiting value of~$\eps$, and with the~$\eps$ before the
integral sign representing that limiting value, will also be
finite. Therefore the value of~$V$, which differs only by a
constant factor, will also be finite, for finite~$\eps$. It is a function
of~$\eps$ and the external coördinates, a continuous increasing
\PageSep{88}
function of~$\eps$, which becomes infinite with~$\eps$, and vanishes
for the smallest possible value of~$\eps$, or for $\eps = -\infty$, if the
energy may be diminished without limit.

Let us also set
\[
\phi = \log \frac{dV}{d\eps}.
\Tag{(266)}
\]
The extension in phase between any two limits of energy, $\eps'$
and~$\eps''$, will be represented by the integral
\[
\int_{\eps'}^{\eps''} e^{\phi}\, d\eps.
\Tag{(267)}
\]
And in general, we may substitute $e^{\phi}\, d\eps$ for $dp_{1} \dots dq_{n}$ in a
$2n$-fold integral, reducing it to a simple integral, whenever
the limits can be expressed by the energy alone, and the other
factor under the integral sign is a function of the energy alone,
or with quantities which are constant in the integration.

In particular we observe that the probability that the energy
of an unspecified system of a canonical ensemble lies between
the limits $\eps'$~and $\eps''$ will be represented by the integral\footnote
  {Compare equation~\Eq{(93)}.}%
\[
\int_{\eps'}^{\eps''} e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps,
\Tag{(268)}
\]
and that the average value in the ensemble of any quantity
which only varies with the energy is given by the equation\footnote
  {Compare equation~\Eq{(108)}.}%
\[
\bar{u} = \int_{V=0}^{\eps=\infty} u\, e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps,
\Tag{(269)}
\]
where we may regard the constant~$\psi$ as determined by the
equation\footnote
  {Compare equation~\Eq{(92)}.}
\[
e^{-\efrac{\psi}{\Theta}} = \int_{V=0}^{\eps=\infty} e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps\Typo{,}{.}
\Tag{(270)}
\]
In regard to the lower limit in these integrals, it will be observed
that $V = 0$ is equivalent to the condition that the
value of~$\eps$ is the least possible.
\PageSep{89}

In like manner, let us denote by~$V_{q}$ the extension-in-configuration
below a certain limit of potential energy which we may
call~$\eps_{q}$. That is, let
\[
V_{q} = \ints \Delta_{\dot{q}}^{\efrac{1}{2}}\, dq_{1} \dots dq_{n},
\Tag{(271)}
\]
the integration being extended (with constant values of the
external coördinates) over all configurations for which the
potential energy is less than~$\eps_{q}$. $V_{q}$~will be a function of~$\eps_{q}$
with the external coördinates, an increasing function of~$\eps_{q}$,
which does not become infinite (in such cases as we shall consider\footnote
  {If $V_{q}$~were infinite for finite values of~$\eps_{q}$, $V$~would evidently be infinite
  for finite values of~$\eps$.})
for any finite value of~$\eps_{q}$. It vanishes for the least
possible value of~$\eps_{q}$, or for $\eps_{q} = -\infty$, if $\eps_{q}$~can be diminished
without limit. It is not always a continuous function of~$\eps_{q}$.
In fact, if there is a finite extension-in-configuration of constant
potential energy, the corresponding value of~$V_{q}$ will
not include that extension-in-configuration, but if $\eps_{q}$~be increased
infinitesimally, the corresponding value of~$V_{q}$ will be
increased by that finite extension-in-configuration.

Let us also set
\[
\phi_{q} = \log \frac{dV_{q}}{d\eps_{q}}.
\Tag{(272)}
\]
The extension-in-configuration between any two limits of
potential energy $\eps_{q}'$~and $\eps_{q}''$ may be represented by the integral
\[
\int_{\eps_{q}'}^{\eps_{q}''} e^{\phi_{q}}\, d\eps_{q}
\Tag{(273)}
\]
whenever there is no discontinuity in the value of~$V_{q}$ as
function of~$\eps_{q}$ between or at those limits, that is, whenever
there is no finite extension-in-configuration of constant
potential energy between or at the limits. And in general,
with the restriction mentioned, we may substitute $e^{\phi_{q}}\, d\eps_{q}$ for
$\Typo{\Delta_{\dot{q}}}{\Delta_{\dot{q}}^{\efrac{1}{2}}}\, dq_{1} \dots dq_{n}$ in an $n$-fold integral, reducing it to a simple
integral, when the limits are expressed by the potential energy,
and the other factor under the integral sign is a function of
\PageSep{90}
the potential energy, either alone or with quantities which are
constant in the integration.

We may often avoid the inconvenience occasioned by formulae
becoming illusory on account of discontinuities in the
values of~$V_{q}$ as function of~$\eps_{q}$ by substituting for the given
discontinuous function a continuous function which is practically
equivalent to the given function for the purposes of the
evaluations desired. It only requires infinitesimal changes of
potential energy to destroy the finite extensions-in-configuration
of constant potential energy which are the cause of the
difficulty.

In the case of an ensemble of systems canonically distributed
in configuration, when $V_{q}$~is, or may be regarded as, a continuous
function of~$\eps_{q}$ (within the limits considered), the probability
that the potential energy of an unspecified system lies
between the limits $\eps_{q}'$~and $\eps_{q}''$ is given by the integral
\[
\int_{\eps_{q}'}^{\eps_{q}''} e^{\efrac{\psi_{q} - \eps_{q}}{\Theta} + \phi_{q}}\, d\eps_{q},
\Tag{(274)}
\]
where $\psi$~may be determined by the condition that the value of
the integral is unity, when the limits include all possible
values of~$\eps_{q}$. In the same case, the average value in the ensemble
of any function of the potential energy is given by the
equation
\[
\bar{u} = \int_{V_{q}=0}^{\eps_{q}=\infty} u\, e^{\efrac{\psi - \eps_{q}}{\Theta} + \phi_{q}}\, d\eps_{q}.
\Tag{(275)}
\]
When $V_{q}$~is not a continuous function of~$\eps_{q}$, we may write~$dV_{q}$
for $e^{\phi_{q}}\, d\eps_{q}$ in these formulae.

In like manner also, for any given configuration, let us
denote by~$V_{p}$ the extension-in-velocity below a certain limit of
kinetic energy specified by~$\eps_{p}$. That is, let
\[
V_{p} = \ints \Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n},
\Tag{(276)}
\]
\PageSep{91}
the integration being extended, with constant values of the
coörd\-inates, both internal and external, over all values of the
momenta for which the kinetic energy is less than the limit~$\eps_{p}$.
$V_{p}$~will evidently be a continuous increasing function of~$\eps_{p}$
which vanishes and becomes infinite with~$\eps_{p}$. Let us set
\[
\phi_{p} = \log \frac{dV_{p}}{d\eps_{p}}.
\Tag{(277)}
\]
The extension-in-velocity between any two limits of kinetic
energy $\eps_{p}'$~and $\eps_{p}''$ may be represented by the integral
\[
\int_{\eps_{p}'}^{\eps_{p}''} e^{\phi_{p}}\, d\eps_{p}.
\Tag{(278)}
\]
And in general, we may substitute $e^{\phi_{p}}\, d\eps_{p}$ for $\Delta_{p}^{\efrac{1}{2}}\, dp_{1} \dots dp_{n}$
or $\Delta_{\dot{q}}^{\efrac{1}{2}}\, d\dot{q}_{1} \dots d\dot{q}_{n}$ in an $n$-fold integral in which the coördinates
are constant, reducing it to a simple integral, when the
limits are expressed by the kinetic energy, and the other factor
under the integral sign is a function of the kinetic energy,
either alone or with quantities which are constant in the
integration.

It is easy to express $V_{p}$~and $\phi_{p}$ in terms of~$\eps_{p}$. Since $\Delta_{p}$~is
function of the coördinates alone, we have by definition
\[
V_{p} = \Delta_{p}^{\efrac{1}{2}} \ints dp_{1} \dots dp_{n}
\Tag{(279)}
\]
the limits of the integral being given by~$\eps_{p}$. That is, if
\[
\eps_{p} = F(p_{1}, \dots p_{n}),
\Tag{(280)}
\]
the limits of the integral for $\eps_{p} = 1$, are given by the equation
\[
F(p_{1}, \dots p_{n}) = 1,
\Tag{(281)}
\]
and the limits of the integral for $\eps_{p} = a^{2}$, are given by the
equation
\[
F(p_{1}, \dots p_{n}) = a^{2}.
\Tag{(282)}
\]
But since $F$~represents a quadratic function, this equation
may be written
\[
F\left(\frac{p_{1}}{a}, \dots \frac{p_{n}}{a}\right) = 1\Add{.}
\Tag{(283)}
\]
\PageSep{92}
The value of~$V_{p}$ may also be put in the form
\[
V_{p} = a^{n}\, \Delta_{p}^{\efrac{1}{2}} \ints d\frac{p_{1}}{a} \dots d\frac{p_{n}}{a}.
\Tag{(284)}
\]
Now we may determine~$V_{p}$ for $\eps_{p} = 1$ from\Eq{(279)} where the
limits are expressed by~\Eq{(281)}, and $V_{p}$~for $\eps_{p} = a^{2}$ from~\Eq{(284)}
taking the limits from~\Eq{(283)}. The two integrals thus determined
are evidently identical, and we have
\[
(V_{p})_{\eps_{p}=a^{2}} = a^{n} (V_{p})_{\eps_{p}=1}\Add{,}
\Tag{(285)}
\]
\ie, $V_{p}$~varies as~$\eps_{p}^{\efrac{n}{2}}$. We may therefore set
\[
V_{p} = C\eps_{p}^{\efrac{n}{2}},\qquad
e^{\phi_{p}} = \frac{n}{2}\, C\eps_{p}^{\efrac{n}{2} - 1}
\Tag{(286)}
\]
where $C$~is a constant, at least for fixed values of the internal
coördinates.

To determine this constant, let us consider the case of a
canonical distribution, for which we have
\[
\int_{0}^{\infty} e^{\efrac{\psi_{p} - \eps_{p}}{\Theta} + \phi_{p}}\, d\eps_{p} = 1,
\]
where
\[
e^{\efrac{\psi_{p}}{\Theta}} = (2\pi \Theta)^{-\efrac{n}{2}}.
\]

Substituting this value, and that of~$e^{\phi_{p}}$ from~\Eq{(286)}, we get
\begin{gather*}
\frac{n}{2} C \int_{0}^{\infty} e^{-\efrac{\eps_{p}}{\Theta}}\, \eps_{p}^{\efrac{n}{2} - 1}\, d\eps_{p} = (2\pi \Theta)^{\efrac{n}{2}},\displaybreak[0] \\
%
\frac{n}{2} C \int_{0}^{\infty} e^{-\efrac{\eps_{p}}{\Theta}} \left(\frac{\eps_{p}}{\Theta}\right)^{\efrac{n}{2} - 1}\, d\left(\frac{\eps_{p}}{\Theta}\right) = (2\pi)^{\efrac{n}{2}},\displaybreak[0] \\
%
\frac{n}{2} C \Gamma\left(\frac{n}{2}\right) = (2\pi)^{\efrac{n}{2}},
\Tag{(287)}
\displaybreak[0] \\
%
C = \frac{(2\pi)^{\efrac{n}{2}}}{\Gamma(\frac{1}{2}n + 1)}.
\end{gather*}
Having thus determined the value of the constant~$C$, we may
\PageSep{93}
substitute it in the general expressions~\Eq{(286)}, and obtain the
following values, which are perfectly general:
\begin{align*}
V_{p} &= \frac{(2\pi \eps_{p})^{\efrac{n}{2}}}{\Gamma(\frac{1}{2}n + 1)}\Add{,}
\Tag{(288)}
\displaybreak[0] \\
%
e^{\phi_{p}} &= \frac{(2\pi)^{\efrac{n}{2}} \eps_{p}^{\efrac{n}{2}- 1}}{\Gamma(\frac{1}{2}n)}\Add{.}\footnotemark
\Tag{(289)}
\end{align*}
\footnotetext{Very similar values for $V_{q}$, $e^{\phi_{q}}$, $V$, and~$e^{\phi}$ may be found in the same
  way in the case discussed in the preceding foot-notes (see pages \Pageref{54}, \Pageref{72}, \Pageref{77}, and~\Pageref{79}),
  in which $\eps_{q}$~is a quadratic function of the~$q$'s, and $\Delta_{\dot{q}}$~independent of the~$q$'s.
  In this case we have
  \begin{align*}
  V_{q} &= \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}}\, \frac{(2\pi)^{\efrac{n}{2}} (\eps_{q} - \eps_{\alpha})^{\efrac{n}{2}}}{\Gamma(\frac{1}{2}n+1)}, \\
%
  e^{\phi_{q}} &= \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}}\, \frac{(2\pi)^{\efrac{n}{2}} (\eps_{q} - \eps_{\alpha})^{\efrac{n}{2} - 1}}{\Gamma(\frac{1}{2}n)}, \\
%
  V &= \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}}\, \frac{(2\pi)^{n} (\eps - \eps_{\alpha})^{n}}{\Gamma(n + 1)}, \\
%
  e^{\phi} &= \left(\frac{\Delta_{\dot{q}}}{\Delta_{q}}\right)^{\efrac{1}{2}}\, \frac{(2\pi)^{n} (\eps - \eps_{\alpha})^{n-1}}{\Gamma(n)}.
\end{align*}}%

It will be observed that the values of $V_{p}$~and $\phi_{p}$ for any
given~$\eps_{p}$ are independent of the configuration, and even of the
nature of the system considered, except with respect to its
number of degrees of freedom.

Returning to the canonical ensemble, we may express the
probability that the kinetic energy of a system of a given
configuration, but otherwise unspecified, falls within given
limits, by either member of the following equation
\[
\int e^{\efrac{\psi_{p} - \eps_{p}}{\Theta} + \phi_{p}}\, d\eps_{p}
  = \frac{1}{\Gamma(\frac{1}{2}n)} \int e^{-\efrac{\eps_{p}}{\Theta}} \left(\frac{\eps_{p}}{\Theta}\right)^{\efrac{n}{2} - 1}\, d\left(\frac{\eps_{p}}{\Theta}\right).
\Tag{(290)}
\]
Since this value is independent of the coördinates it also
represents the probability that the kinetic energy of an
unspecified system of a canonical ensemble falls within the
limits. The form of the last integral also shows that the probability
that the ratio of the kinetic energy to the modulus
\PageSep{94}
falls within given limits is independent also of the value of
the modulus, being determined entirely by the number of
degrees of freedom of the system and the limiting values
of the ratio.

The average value of any function of the kinetic energy,
either for the whole ensemble, or for any particular configuration,
is given by
\[
\bar{u} = \frac{1}{\Theta^{\efrac{n}{2}} \Gamma(\frac{1}{2}n)} \int_{0}^{\infty} u\, e^{-\efrac{\eps_{p}}{\Theta}}\, \eps_{p}^{\efrac{n}{2}-1}\, d\eps_{p}\Add{.}\footnotemark
\Tag{(291)}
\]
\footnotetext{The corresponding equation for the average value of any function of
  the potential energy, when this is a quadratic function of the~$q$'s, and $\Delta_{\dot{q}}$~is
  independent of the~$q$'s, is
  \[
  \bar{u} = \frac{1}{\Theta^{\efrac{n}{2}} \Gamma(\frac{1}{2}n)} \int_{\eps_{\alpha}}^{\infty} u\, e^{-\efrac{\eps_{q} - \eps_{\alpha}}{\Theta}}\, (\eps_{q} - \eps_{\alpha})^{\efrac{n}{2}-1}\, d\eps_{q}.
  \]
  In the same case, the average value of any function of the (total) energy is
  given by the equation
  \[
  \bar{u} = \frac{1}{\Theta^{n} \Gamma(n)} \int_{0}^{\infty} u\, e^{-\efrac{\eps - \eps_{\alpha}}{\Theta}}\, (\eps - \eps_{\alpha})^{n-1}\, d\eps.
  \]

  Hence in this case
  \begin{gather*}
    \begin{aligned}
    \Bar{(\eps_{q} - \eps_{\alpha})^{m}} &= \frac{\Gamma(m + \frac{1}{2}n)}{\Gamma(\frac{1}{2}n)}\, \Theta^{m},\quad\text{if $m + \tfrac{1}{2}n > 0$.} \\
%
    \Bar{(\eps - \eps_{\alpha})^{m}} &= \frac{\Gamma(m + n)}{\Gamma(n)}\, \Theta^{m},\quad\text{if $m + n > 0$.}
    \end{aligned} \\
%
    \Bar{e^{-\phi_{q}}\, V_{q}} = \Bar{e^{-\phi}\, V} = \Theta, \\
    \frac{\Fsp\Bar{d\phi_{q}}\Fsp}{d\eps_{q}} = \frac{1}{\Theta},\quad\text{if $n > 2$,} \\
\intertext{and}
    \frac{\Fsp\Bar{d\phi}\Fsp}{d\eps} = \frac{1}{\Theta},\quad\text{if $n > 1$.}
  \end{gather*}
  If $n = 1$, $e^{\phi} = 2\pi$ and $d\phi/d\eps = 0$ for any value of~$\eps$. If $n = 2$, the case is
  the same with respect to~$\phi_{q}$.}%

Thus:
\begin{gather*}
\Bar{\eps_{p}^{m}} = \frac{\Gamma(m + \frac{1}{2}n)}{\Gamma(\frac{1}{2}n)}\, \Theta^{m},\quad\text{if $m + \tfrac{1}{2}n > 0$;}\footnotemark
\Tag{(292)} \\
%
\bar{V}_{p} = \frac{\Gamma(n)}{\Gamma(\frac{1}{2}n + 1) \Gamma(\frac{1}{2}n)}\, (2\pi \Theta)^{\efrac{n}{2}};
\Tag{(293)}
\end{gather*}
\footnotetext{This equation has already been proved for positive integral powers of
  the kinetic energy. See page~\Pageref{77}.}
\PageSep{95}
\begin{gather*}
\Bar{e^{\phi_{p}}} = \frac{\Gamma(n - 1)}{[\Gamma(\frac{1}{2}n)]^{2}}\, (2\pi)^{\efrac{n}{2}}\, \Theta^{\efrac{n}{2}-1},\quad\text{if $n > 1$;}
\Tag{(294)} \\
%
\frac{\Fsp\Bar{d\phi_{p}}\Fsp}{d\eps_{p}} = \frac{1}{\Theta},\quad\text{if $n > 2$;}
\Tag{(295)} \\
\Bar{e^{-\phi_{p}}\, V_{p}} = \Theta.
\Tag{(296)}
\end{gather*}
If $n = 2$, $e^{\phi_{p}} = 2\pi$, and $d\phi_{p}/d\eps_{p} = 0$, for any value of~$\eps_{p}$.

The definitions of $V$, $V_{q}$, and $V_{p}$ give
\[
V = \iint dV_{p}\, dV_{q}\Add{,}
\Tag{(297)}
\]
where the integrations cover all phases for which the energy
is less than the limit~$\eps$, for which the value of~$V$ is sought.
This gives
\[
V = \int_{V_{q}=0}^{\eps_{q}=\eps} V_{p}\, dV_{q},
\Tag{(298)}
\]
and
\[
e^{\phi} = \frac{dV}{d\eps} = \int_{V_{q}=0}^{\eps_{q}=\eps} e^{\phi_{p}}\, dV_{q},
\Tag{(299)}
\]
where $V_{p}$~and $e^{\phi_{p}}$ are connected with~$V_{q}$ by the equation
\[
\eps_{p} + \eps_{q} = \constant = \eps.
\Tag{(300)}
\]

If $n > 2$, $e^{\phi_{p}}$~vanishes at the upper limit, \ie, for $\eps_{p} = 0$, and
we get by another differentiation
\[
e^{\phi}\, \frac{d\phi}{d\eps}
  = \int_{V_{q}=0}^{\eps_{q}=\eps} e^{\phi_{p}}\, \frac{d\phi_{p}}{d\eps_{p}}\, dV_{q}.
\Tag{(301)}
\]
We may also write
\begin{gather*}
V = \int_{V_{q}=0}^{\eps_{q}=\eps} V_{p}\, e^{\phi_{q}}\, d\eps_{q},
\Tag{(302)} \\
%
e^{\phi} = \int_{V_{q}=0}^{\eps_{q}=\eps} e^{\phi_{p} + \phi_{q}}\, d\eps_{q},
\Tag{(303)}
\end{gather*}
\PageSep{96}
etc., when $V_{q}$~is a continuous function of~$\eps_{q}$ commencing with
the value $V_{q} = 0$, or when we choose to attribute to~$V_{q}$ a
fictitious continuity commencing with the value zero, as described
on page~\Pageref{90}.

If we substitute in these equations the values of $V_{p}$~and $e^{\phi_{p}}$
which we have found, we get
\begin{gather*}
V = \frac{(2\pi)^{\efrac{n}{2}}}{\Gamma(\frac{1}{2}n + 1)} \int_{V_{q} = 0}^{\eps_{q} = \eps} (\eps - \eps_{q})^{\efrac{n}{2}}\, dV_{q},
\Tag{(304)} \\
e^{\phi} = \frac{(2\pi)^{\efrac{n}{2}}}{\Gamma(\Chg{\frac{n}{2}}{\frac{1}{2}n})} \int_{V_{q} = 0}^{\eps_{q} = \eps} (\eps - \eps_{q})^{\efrac{n}{2}-1}\, dV_{q},
\Tag{(305)}
\end{gather*}
where $e^{\phi_{q}}\, d\eps_{q}$ may be substituted for~$dV_{q}$ in the cases above
described. If, therefore, $n$~is known, and $V_{q}$~as function of
$\eps_{q}$, $V$ and~$\Typo{\eps}{e}^{\phi}$ may be found by quadratures.

It appears from these equations that $V$~is always a continuous
increasing function of~$\eps$, commencing with the value $V = 0$,
even when this is not the case with respect to $V_{q}$ and~$\eps_{q}$.
The same is true of~$e^{\phi}$, when $n > 2$, or when $n = 2$ if $V_{q}$~increases
continuously with~$\eps_{q}$ from the value $V_{q} = 0$.

The last equation may be derived from the preceding by
differentiation with respect to~$\eps$. Successive differentiations
give, if $h < \frac{1}{2}n + 1$,
\[
\frac{d^{h}V}{d\eps^{h}}
  = \int_{V_{q}=0}^{\eps_{q}=\eps} \frac{d^{h}V_{p}}{d\eps_{p}^{h}}\, dV_{q}
  = \frac{(2\pi)^{\efrac{n}{2}}}{\Gamma(\frac{1}{2}n + 1 - h)} \int_{V_{q}=0}^{\eps_{q}=\eps} (\eps - \eps_{q})^{\efrac{n}{2}-h}\, dV_{q}.
\Tag{(306)}
\]
$d^{h}V/d\eps^{h}$~is therefore positive if $h < \frac{1}{2}n + 1$. It is an increasing
function of~$\eps$, if $h < \frac{1}{2}n$. If $\eps$~is not capable of
being diminished without limit, $d^{h}V/d\eps^{h}$~vanishes for the
least possible value of~$\eps$, if $h < \frac{1}{2}n$.

% [** TN: Newline but no indent in the original]
If $n$~is even,
\[
\frac{d^{\efrac{n}{2}}V}{d\eps^{\efrac{n}{2}}} = (2\pi)^{\efrac{n}{2}} (V_{q})_{\eps_{q}=\eps}\Add{.}
\Tag{(307)}
\]
\PageSep{97}
That is, $V_{q}$~is the same function of~$\eps_{q}$ as $\dfrac{1}{(2\pi)^{\efrac{n}{2}}}\, \dfrac{d^{\efrac{n}{2}}V}{d\eps^{\efrac{n}{2}}}$ of~$\eps$.

When $n$~is large, approximate formulae will be more available.
It will be sufficient to indicate the method proposed,
without precise discussion of the limits of its applicability or
of the degree of its approximation. For the value of~$e^{\phi}$ corresponding
to any given~$\eps$, we have
\[
e^{\phi} = \int_{V_{q}=0}^{\eps_{q}=\eps} e^{\phi_{p} + \phi_{q}}\, d\eps_{q}
  = \int_{0}^{\eps} e^{\phi_{p} + \phi_{q}}\, d\eps_{p},
\Tag{(308)}
\]
where the variables are connected by the equation~\Eq{(300)}.
The maximum value of $\phi_{p} + \phi_{q}$ is therefore characterized by
the equation
\[
\frac{d\phi_{p}}{d\eps_{p}} = \frac{d\phi_{q}}{d\eps_{q}}.
\Tag{(309)}
\]
The values of $\eps_{p}$~and $\eps_{q}$ determined by this maximum we shall
distinguish by accents, and mark the corresponding values of
functions of $\eps_{p}$~and $\eps_{q}$ in the same way. Now we have by
Taylor's theorem
\begin{align*}
\phi_{p} &= \phi_{p}'
    + \left(\frac{d\phi_{p}}{d\eps_{p}}\right)' (\eps_{p} - \eps_{p}')
    + \left(\frac{d^{2}\phi_{p}}{d\eps_{p}^{2}}\right)' \frac{(\eps_{p} - \eps_{p}')^{2}}{2} + \text{etc.}
\Tag{(310)} \\
\phi_{q} &= \phi_{q}'
    + \left(\frac{d\phi_{q}}{d\eps_{q}}\right)' (\eps_{q} - \eps_{q}')
    + \left(\frac{d^{2}\phi_{q}}{d\eps_{q}^{2}}\right)' \frac{(\eps_{q} - \eps_{q}')^{2}}{2} + \text{etc.}
\Tag{(311)} \\
\end{align*}
If the approximation is sufficient without going beyond the
quadratic terms, since by~\Eq{(300)}
\[
\eps_{p} - \eps_{p'} = -(\eps_{q} - \eps_{q}'),
\]
we may write
\[
e^{\phi} = e^{\phi_{p}' + \phi_{q}'}
  \int_{-\infty}^{\infty} e^{\left[\left(\efrac{d^{2}\phi_{p}'}{d\eps_{p}^{2}}\right)' + \left(\efrac{d^{2}\phi_{q}'}{d\eps_{q}^{2}}\right)'\right] \efrac{(\eps_{q} - \eps_{q}')^{2}}{2}}\, d\eps_{q},
\Tag{(312)}
\]
where the limits have been made~$\pm\infty$ for analytical simplicity.
This is allowable when the quantity in the square brackets
has a very large negative value, since the part of the integral
\PageSep{98}
corresponding to other than very small values of $\eps_{q} - \eps_{q}'$ may
be regarded as a vanishing quantity.

This gives
\[
e^{\phi} = e^{\phi_{p}' + \phi_{q}'} \Biggl[\frac{-2\pi}{\left(\dfrac{d^{2}\phi_{p}'}{d\eps_{p}^{2}}\right)' + \left(\dfrac{d^{2}\phi_{q}'}{d\eps_{q}^{2}}\right)'}\Biggr]^{\efrac{1}{2}}\Add{,}
\Tag{(313)}
\]
or
\[
\phi = \phi_{p}' + \phi_{q}' + \tfrac{1}{2} \log(2\pi) - \tfrac{1}{2} \log\left[-\left(\frac{d^{2}\phi_{p}'}{d\eps_{p}^{2}}\right)' - \left(\frac{d^{2}\phi_{q}'}{d\eps_{q}^{2}}\right)'\right].
\Tag{(314)}
\]
From this equation, with \Eq{(289)}, \Eq{(300)} and~\Eq{(309)}, we may
determine the value of~$\phi$ corresponding to any given value of~$\eps$,
when $\phi_{q}$~is known as function of~$\eps_{q}$.

Any two systems may be regarded as together forming a
third system. If we have $V$~or $\phi$ given as function of~$\eps$ for
any two systems, we may express by quadratures $V$~and $\phi$ for
the system formed by combining the two. If we distinguish
by the suffixes $(\ )_{1}$, $(\ )_{2}$, $(\ )_{12}$ the quantities relating to
the three systems, we have easily from the definitions of these
quantities
\begin{gather*}
V_{12} = \iint dV_{1}\, dV_{2}
  = \int V_{2}\, dV_{1}
  = \int V_{1}\, dV_{2}
  = \int V_{1} e^{\phi_{2}}\, d\eps_{2},
\Tag{(315)} \\
e^{\phi_{12}} = \int e^{\phi_{2}}\, dV_{1}
  = \int e^{\phi_{1}}\, dV_{2}
  = \int e^{\phi_{1} + \phi_{2}}\, d\eps_{2},
\Tag{(316)}
\end{gather*}
where the double integral is to be taken within the limits
\[
V_{1} = 0,\quad
V_{2} = 0,\quad\text{and}\quad
\eps_{1} + \eps_{2} = \eps_{12},
\]
and the variables in the single integrals are connected by the
last of these equations, while the limits are given by the first
two, which characterize the least possible values of $\eps_{1}$~and $\eps_{2}$
respectively.

It will be observed that these equations are identical in
form with those by which $V$~and $\phi$ are derived from $V_{p}$~or $\phi_{p}$
and $V_{q}$~or $\phi_{q}$, except that they do not admit in the general
case those transformations which result from substituting for
$V_{p}$~or $\phi_{p}$ the particular functions which these symbols always
represent.
\PageSep{99}

Similar formulae may be used to derive $V_{q}$~or $\phi_{q}$ for the
compound system, when one of these quantities is known
as function of the potential energy in each of the systems
combined.

The operation represented by such an equation as
\[
e^{\phi_{12}} = \int e^{\phi_{1}} e^{\phi_{2}}\, d\eps_{1}
\]
is identical with one of the fundamental operations of the
theory of errors, viz., that of finding the probability of an error
from the probabilities of partial errors of which it is made up.
It admits a simple geometrical illustration.

We may take a horizontal line as an axis of abscissas, and lay
off~$\eps_{1}$ as an abscissa measured to the right of any origin, and
erect~$e^{\phi_{1}}$ as a corresponding ordinate, thus determining a certain
curve. Again, taking a different origin, we may lay off~$\eps_{2}$ as
abscissas measured to the left, and determine a second curve by
erecting the ordinates~$e^{\phi_{2}}$. We may suppose the distance between
the origins to be~$\eps_{12}$, the second origin being to the right
if $\eps_{12}$~is positive. We may determine a third curve by erecting
at every point in the line (between the least values of $\eps_{1}$ and~$\eps_{2}$)
an ordinate which represents the product of the two ordinates
belonging to the curves already described. The area between
this third curve and the axis of abscissas will represent the value
of~$e^{\phi_{12}}$. To get the value of this quantity for varying values
of~$\eps_{12}$, we may suppose the first two curves to be rigidly constructed,
and to be capable of being moved independently. We
may increase or diminish~$\eps_{12}$ by moving one of these curves to
the right or left. The third curve must be constructed anew
for each different value of~$\eps_{12}$.
\PageSep{100}


\Chapter[The Function phi and the Canonical Distribution.]{IX.}{The Function $\phi$ and the Canonical Distribution.}

\First{In} this chapter we shall return to the consideration of the
canonical distribution, in order to investigate those properties
which are especially related to the function of the energy
which we have denoted by~$\phi$.

If we denote by~$N$, as usual, the total number of systems
in the ensemble,
\[
N\, e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps
\]
will represent the number having energies between the limits
$\eps$~and $\eps + d\eps$. The expression
\[
N\, e^{\efrac{\psi - \eps}{\Theta} + \phi}
\Tag{(317)}
\]
represents what may be called the \emph{density-in-energy}. This
vanishes for $\eps = \infty$, for otherwise the necessary equation
\[
\int_{V=0}^{\eps=\infty} e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps = 1
\Tag{(318)}
\]
could not be fulfilled. For the same reason the density-in-energy
will vanish for $\eps = -\infty$, if that is a possible value of
the energy. Generally, however, the least possible value of
the energy will be a finite value, for which, if $n > 2$, $e^{\phi}$~will
vanish,\footnote
  {See page~\Pageref{96}.}
and therefore the density-in-energy. Now the density-in-energy
is necessarily positive, and since it vanishes for
extreme values of the energy if $n > 2$, it must have a maximum
in such cases, in which the energy may be said to have
\PageSep{101}
its most common or most probable value, and which is
determined by the equation
\[
\frac{d\phi}{d\eps} = \frac{1}{\Theta}.
\Tag{(319)}
\]

This value of $d\phi/d\eps$ is also, when $n > 2$, its average value
in the ensemble. For we have identically, by integration by
parts,
\[
\int_{V=0}^{\eps=\infty} \frac{d\phi}{d\eps}\, e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps
  = \left[e^{\efrac{\psi - \eps}{\Theta} + \phi}\right]_{V=0}^{\eps=\infty}
  + \frac{1}{\Theta} \int_{V=0}^{\eps=\infty} e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps.
\Tag{(320)}
\]
If $n > 2$, the expression in the brackets, which multiplied by~$N$
would represent the density-in-energy, vanishes at the limits,
and we have by \Eq{(269)} and~\Eq{(318)}
\[
\frac{\Fsp\Bar{d\phi}\Fsp}{d\eps} = \frac{1}{\Theta}.
\Tag{(321)}
\]
It appears, therefore, that for systems of more than two degrees
of freedom, the average value of~$d\phi/d\eps$ in an ensemble canonically
distributed is identical with the value of the same differential
coefficient as calculated for the most common energy
in the ensemble, both values being reciprocals of the modulus.

Hitherto, in our consideration of the quantities $V$,~$V_{q}$, $V_{p}$, $\phi$,
$\phi_{q}$,~$\phi_{p}$, we have regarded the external coördinates as constant.
It is evident, however, from their definitions that $V$~and $\phi$ are
in general functions of the external coördinates and the energy~($\eps$),
that $V_{q}$~and $\phi_{q}$ are in general functions of the external
coördinates and the potential energy~($\eps_{q}$). $V_{p}$~and $\phi_{p}$ we have
found to be functions of the kinetic energy~($\eps_{p}$) alone. In the
equation
\[
e^{-\efrac{\psi}{\Theta}} = \int_{V=0}^{\eps=\infty} e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps,
\Tag{(322)}
\]
by which $\psi$~may be determined, $\Theta$~and the external coördinates
(contained implicitly in~$\phi$) are constant in the integration.
The equation shows that $\psi$~is a function of these constants.
\PageSep{102}
If their values are varied, we shall have by differentiation, if
$n > 2$,
\begin{multline*}
e^{-\efrac{\psi}{\Theta}} \left(-\frac{1}{\Theta}\, d\psi + \frac{\psi}{\Theta^{2}}\, d\Theta\right)
  = \frac{1}{\Theta^{2}}\, d\Theta \int_{V=0}^{\eps=\infty} \eps\, e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps \\
  + da_{1} \int_{V=0}^{\eps=\infty} \frac{d\phi}{da_{1}}\, e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps
  + da_{2} \int_{V=0}^{\eps=\infty} \frac{d\phi}{da_{2}}\, e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps + \text{etc.},
\Tag{(323)}
\end{multline*}
(Since $e^{\phi}$~vanishes with~$V$, when $n > 2$, there are no terms due
to the variations of the limits.) Hence by~\Eq{(269)}
\[
-\frac{1}{\Theta}\, d\psi + \frac{\psi}{\Theta^{2}}\, d\Theta
  = \frac{\bar{\eps}}{\Theta^{2}}\, d\Theta
  + \frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}}\, da_{1}
  + \frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}}\, da_{2} + \text{etc.},
\Tag{(324)}
\]
or, since
\begin{gather*}
\frac{\psi \Typo{+}{-} \bar{\eps}}{\Theta} = \bar{\eta},
\Tag{(325)} \\
%
d\psi = \bar{\eta}\, d\Theta
  - \Theta\, \frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}}\, da_{1}
  - \Theta\, \frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}}\, da_{2} - \text{etc.}
\Tag{(326)}
\end{gather*}
Comparing this with~\Eq{(112)}, we get
\[
\frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}} = \frac{\bar{A}_{1}}{\Theta},\quad
\frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}} = \frac{\bar{A}_{2}}{\Theta},\quad \text{etc.}
\Tag{(327)}
\]
The first of these equations might be written\footnote
  {See equations \Eq{(321)} and~\Eq{(104)}. Suffixes are here added to the differential
  coefficients, to make the meaning perfectly distinct, although the same quantities
  may be written elsewhere without the suffixes, when it is believed that
  there is no danger of misapprehension. The suffixes indicate the quantities
  which are constant in the differentiation, the single letter~$a$ standing for all
  the letters $a_{1}$, $a_{2}$, etc., or all except the one which is explicitly varied.}%
\[
\Bar{\left(\frac{d\phi}{da_{1}}\right)}_{\eps,a}
  = -\Bar{\left(\frac{d\phi}{d\eps}\right)}_{a} \Bar{\left(\frac{d\eps}{da_{1}}\right)}_{a,q}
\Tag{(328)}
\]
but must not be confounded with the equation
\[
\Bar{\left(\frac{d\phi}{da_{1}}\right)}_{\eps,a}
  = -\Bar{\left(\frac{d\phi}{d\eps}\right)_{a} \left(\frac{d\eps}{da_{1}}\right)}_{\phi,a}
\Tag{(329)}
\]
which is derived immediately from the identity
\[
\left(\frac{d\phi}{da_{1}}\right)_{\eps,a}
  = -\left(\frac{d\phi}{d\eps}\right)_{a} \left(\frac{d\eps}{da_{1}}\right)_{\phi,a}\Add{.}
\Tag{(330)}
\]
\PageSep{103}

Moreover, if we eliminate~$d\psi$ from~\Eq{(326)} by the equation
\[
d\psi = \Theta\, d\bar{\eta} + \bar{\eta}\, d\Theta + d\bar{\eps},
\Tag{(331)}
\]
obtained by differentiating~\Eq{(325)}, we get
\[
d\bar{\eps} = -\Theta\, d\bar{\eta}
  - \Theta\, \frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}}\, da_{1}
  - \Theta\, \frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}}\, da_{2} - \text{etc.},
\Tag{(332)}
\]
or by~\Eq{(321)},
\[
-d\bar{\eta}
  = \frac{\Fsp\Bar{d\phi}\Fsp}{d\eps}\, d\bar{\eps}
  + \frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}}\, da_{1}
  + \frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}}\, da_{2} + \text{etc.}
\Tag{(333)}
\]
Except for the signs of average, the second member of this
equation is the same as that of the identity
\[
d\phi = \frac{d\phi}{d\eps}\, d\eps
  + \frac{d\phi}{da_{1}}\, da_{1}
  + \frac{d\phi}{da_{2}}\, da_{2} + \text{etc.}
\Tag{(334)}
\]
For the more precise comparison of these equations, we may
suppose that the energy in the last equation is some definite
and fairly representative energy in the ensemble. For this
purpose we might choose the average energy. It will perhaps
be more convenient to choose the most common energy,
which we shall denote by~$\eps_{0}$. The same suffix will be applied
to functions of the energy determined for this value. Our
identity then becomes
\[
d\phi_{0} = \left(\frac{d\phi}{d\eps}\right)_{0} d\eps_{0}
  + \left(\frac{d\phi}{da_{1}}\right)_{0} da_{1}
  + \left(\frac{d\phi}{da_{2}}\right)_{0} da_{2} + \text{etc.}
\Tag{(335)}
\]
It has been shown that
\[
\frac{\Fsp\Bar{d\phi}\Fsp}{d\eps}
  = \left(\frac{d\phi}{d\eps}\right)_{0} = \frac{1}{\Theta},
\Tag{(336)}
\]
when $n > 2$. Moreover, since the external coördinates have
constant values throughout the ensemble, the values of
$d\phi/da_{1}$, $d\phi/da_{2}$, etc.\ vary in the ensemble only on account
of the variations of the energy~($\eps$), which, as we have seen,
may be regarded as sensibly constant throughout the ensemble,
when $n$~is very great. In this case, therefore, we may
regard the average values
\[
\frac{\Fsp\Bar{d\phi}\Fsp}{da_{1}},\quad
\frac{\Fsp\Bar{d\phi}\Fsp}{da_{2}},\quad \text{etc.},
\]
\PageSep{104}
as practically equivalent to the values relating to the most
common energy
\[
\left(\frac{d\phi}{da_{1}}\right)_{0},\quad
\left(\frac{d\phi}{da_{2}}\right)_{0},\quad \text{etc.}
\]
In this case also $d\bar{\eps}$~is practically equivalent to~$d\eps_{0}$. We have
therefore, for very large values of~$n$,
\[
-d\bar{\eta} = d\phi_{0}
\Tag{(337)}
\]
approximately. That is, except for an additive constant, $-\bar{\eta}$~may
be regarded as practically equivalent to~$\phi_{0}$, when the
number of degrees of freedom of the system is very great.
It is not meant by this that the variable part of $\bar{\eta} + \phi_{0}$ is
numerically of a lower order of magnitude than unity. For
when $n$~is very great, $-\bar{\eta}$~and $\phi_{0}$ are very great, and we can
only conclude that the variable part of $\bar{\eta} + \phi_{0}$ is insignificant
compared with the variable part of~$\bar{\eta}$ or of~$\phi_{0}$, taken
separately.

Now we have already noticed a certain correspondence
between the quantities $\Theta$~and $\bar{\eta}$ and those which in thermodynamics
are called temperature and entropy. The property
just demonstrated, with those expressed by equation~\Eq{(336)},
therefore suggests that the quantities $\phi$~and $d\eps/d\phi$ may also
correspond to the thermodynamic notions of entropy and temperature.
We leave the discussion of this point to a subsequent
chapter, and only mention it here to justify the
somewhat detailed investigation of the relations of these
quantities.

We may get a clearer view of the limiting form of the
relations when the number of degrees of freedom is indefinitely
increased, if we expand the function~$\phi$ in a series
arranged according to ascending powers of $\eps - \eps_{0}$. This expansion
may be written
\[
\phi = \phi_{0} + \left(\frac{d\phi}{d\eps}\right)_{0} (\eps - \eps_{0})
  + \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0} \frac{(\eps - \eps_{0})^{2}}{2}
  + \left(\frac{d^{3}\phi}{d\eps^{3}}\right)_{0} \frac{(\eps - \eps_{0})^{3}}{\Fact[]{3}} + \text{etc.}
\Tag{(338)}
\]
Adding the identical equation
\PageSep{105}
\[
\frac{\psi - \eps}{\Theta} = \frac{\psi - \eps_{0}}{\Theta} - \frac{\eps - \eps_{0}}{\Theta},
\]
we get by~\Eq{(336)}
\[
\frac{\psi - \eps}{\Theta} + \phi
  = \frac{\psi - \eps_{0}}{\Theta} + \phi_{0}
  + \Typo{\left(\frac{d^{2}\phi}{d\eps^{2}}\right)}{\left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}} \frac{(\eps - \eps_{0})^{2}}{2}
  + \Typo{\left(\frac{d^{3}\phi}{d\eps^{3}}\right)}{\left(\frac{d^{3}\phi}{d\eps^{3}}\right)_{0}} \frac{(\eps - \eps_{0})^{3}}{\Fact[]{3}} + \text{etc.}
\Tag{(339)}
\]
Substituting this value in
\[
\int_{\eps'}^{\eps''} e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps,
\]
which expresses the probability that the energy of an unspecified
system of the ensemble lies between the limits $\eps'$~and $\eps''$,
we get
\[
e^{\efrac{\psi - \eps_{0}}{\Theta} + \phi_{0}}
  \int_{\eps'}^{\eps''} e^{\left(\efrac{d^{2}\phi}{d\eps^{2}}\right)_{0} \efrac{(\eps - \eps_{0})^{2}}{2} + \left(\efrac{d^{3}\phi}{d\eps^{3}}\right)_{0} \efrac{(\eps - \eps_{0})^{3}}{\Fact[]{3}} + \text{etc.}}\, d\eps.
\Tag{(340)}
\]
When the number of degrees of freedom is very great, and
$\eps - \eps_{0}$ in consequence very small, we may neglect the higher
powers and write\footnote
  {If a higher degree of accuracy is desired than is afforded by this formula,
  it may be multiplied by the series obtained from
  \[
  e^{\left(\efrac{d^{3}\phi}{d\eps^{3}}\right)_{0} \efrac{(\eps - \eps_{0})^{3}}{\Fact[]{3}} + \text{etc.}}
  \]
  by the ordinary formula for the expansion in series of an exponential function.
  There would be no especial analytical difficulty in taking account of
  a moderate number of terms of such a series, which would commence
  \[
  1 + \left(\frac{d^{3}\phi}{d\eps^{3}}\right)_{0} \frac{(\eps - \eps_{0})^{3}}{\Fact[]{3}}
    + \left(\frac{d^{4}\phi}{d\eps^{4}}\right)_{0} \frac{(\eps - \eps_{0})^{4}}{\Fact[]{4}} + \text{etc.}
  \]
}%
\[
e^{\efrac{\psi - \eps_{0}}{\Theta} + \phi_{0}}
  \int_{\eps'}^{\eps''} e^{\left(\efrac{d^{2}\phi}{d\eps^{2}}\right)_{0} \efrac{(\eps - \eps_{0})^{2}}{2}}\, d\eps.
\Tag{(341)}
\]

This shows that for a very great number of degrees of
freedom the probability of deviations of energy from the most
probable value~($\eps_{0}$) approaches the form expressed by the
`law of errors.' With this approximate law, we get
\PageSep{106}
\begin{gather*}
e^{\efrac{\psi - \eps_{0}}{\Theta} + \phi_{0}} \Biggl(\frac{-2\pi}{\left(\dfrac{d^{2}\phi}{d\eps^{2}}\right)_{0}}\Biggr)^{\efrac{1}{2}} = 1,
\Tag{(342)} \\
%
\bar{\eps} = \eps_{0},\qquad
\Bar{(\eps - \eps_{0})^{2}} = -\Typo{\left(\dfrac{d^{2}\phi}{d\eps^{2}}\right)^{-\efrac{1}{2}}}{\left(\dfrac{d^{2}\phi}{d\eps^{2}}\right)_{0}},
\Tag{(343)}
\end{gather*}
whence
\[
\frac{\psi - \eps_{0}}{\Theta} + \phi_{0}
  = \tfrac{1}{2} \log \frac{\left(\dfrac{d^{2}\phi}{d\eps^{2}}\right)_{0}}{-2\pi}
  = -\tfrac{1}{2} \log \bigl(2\pi \Bar{(\eps - \bar{\eps})^{2}}\bigr).
\Tag{(344)}
\]
Now it has been proved in Chapter~VII that
\[
\Bar{(\eps - \bar{\eps})^{2}} = \frac{2}{n}\, \frac{d\bar{\eps}}{d\bar{\eps}_{p}}\, \bar{\eps}_{p}^{2}.
\]
We have therefore
\[
\bar{\eta} + \phi_{0}
  = \frac{\psi - \bar{\eps}}{\Theta} + \phi_{0}
  = -\tfrac{1}{2} \log \bigl(2\pi \Bar{(\eps - \bar{\eps})^{2}}\bigr)
  = -\tfrac{1}{2} \log \left(\frac{4\pi}{n}\, \frac{d\bar{\eps}}{d\bar{\eps}_{p}}\, \bar{\eps}_{p}^{2}\right)
\Tag{(345)}
\]
approximately. The order of magnitude of $\bar{\eta} - \phi_{0}$ is therefore
that of~$\log n$. This magnitude is mainly constant.
The order of magnitude of $\bar{\eta} + \phi_{0} - \frac{1}{2} \log n$ is that of unity.
The order of magnitude of~$\phi_{0}$, and therefore of~$-\bar{\eta}$, is that
of~$n$.\footnote
  {Compare~\Eq{(289)}, \Eq{(314)}.}

Equation~\Eq{(338)} gives for the first approximation
\begin{gather*}
\bar{\phi} - \phi_{0}
  = \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0} \frac{\Bar{(\eps - \eps_{0})^{2}}}{2}
  = -\tfrac{1}{2},
\Tag{(346)}\displaybreak[0] \\
%
\Bar{(\phi - \phi_{0})(\eps - \eps_{0})}
  = \frac{\Bar{(\eps - \eps_{0})^{2}}}{\Theta}
  = \frac{d\bar{\eps}}{d\bar{\eps}_{p}}\, \bar{\eps}_{p},
\Tag{(347)}\displaybreak[0] \\
%
\Bar{(\phi - \phi_{0})^{2}}
  = \frac{\Bar{(\eps - \eps_{0})^{2}}}{\Theta^{2}}
  = \frac{n}{2}\, \frac{d\bar{\eps}}{d\bar{\eps}_{p}}\Add{.}
\Tag{(348)}
\end{gather*}
The members of the last equation have the order of magnitude
of~$n$. Equation~\Eq{(338)} gives also for the first approximation
\[
\frac{d\phi}{d\eps} - \frac{1}{\Theta}
  = \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0} (\eps - \eps_{0}),
\]
\PageSep{107}
whence
\begin{gather*}
\Bar{\left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)(\eps - \eps_{0})}
  = \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0} \Bar{(\eps - \eps_{0})^{2}}
  = -1,
\Tag{(349)}\displaybreak[0] \\
%
\Bar{\left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)^{2}}
  = \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}^{2} \Bar{(\eps - \eps_{0})^{2}}
  = \frac{1}{\Bar{(\eps - \eps_{0})^{2}}}
  = -\left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}^{2}.
\Tag{(350)}
\end{gather*}
This is of the order of magnitude of~$n$.\footnote
  {We shall find hereafter that the equation
  \[
  \Bar{\left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)(\eps - \bar{\eps})} = -1
  \]
  is exact for any value of~$n$ greater than~$2$, and that the equation
  \[
  \Bar{\left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)^{2}}
  = -\frac{\Bar{d^{2}\phi}}{d\eps^{2}}
  \]
  is exact for any value of~$n$ greater than~$4$.}%

It should be observed that the approximate distribution of
the ensemble in energy according to the `law of errors' is
not dependent on the particular form of the function of the
energy which we have assumed for the index of probability~($\eta$).
In any case, we must have
\[
\int_{V=0}^{\eps=\infty} e^{\eta + \phi}\, d\eps = 1,
\Tag{(351)}
\]
where $e^{\eta + \phi}$~is necessarily positive. This requires that it
shall vanish for $\eps = \infty$, and also for $\eps = -\infty$, if this is a possible
value. It has been shown in the last chapter that if $\eps$~has
a (finite) least possible value (which is the usual case) and
$n > 2$, $e^{\phi}$~will vanish for that least value of~$\eps$. In general
therefore $\eta + \phi$ will have a maximum, which determines the
most probable value of the energy. If we denote this value
by~$\eps_{0}$, and distinguish the corresponding values of the functions
of the energy by the same suffix, we shall have
\[
\left(\frac{d\eta}{d\eps}\right)_{0} + \left(\frac{d\phi}{d\eps}\right)_{0} = 0.
\Tag{(352)}
\]
The probability that an unspecified system of the ensemble
\PageSep{108}
falls within any given limits of energy ($\eps'$~and $\eps''$) is represented
by
\[
\int_{\eps'}^{\eps''} e^{\eta + \phi}\, d\eps.
\]
If we expand $\eta$~and $\phi$ in ascending powers of $\eps - \eps_{0}$, without
going beyond the squares, the probability that the energy falls
within the given limits takes the form of the `law of errors'---
\[
e^{\phi_{0} + \eta_{0}}
  \int_{\eps'}^{\eps''} e^{\left[\left(\efrac{d^{2}\eta}{d\eps^{2}}\right)_{0} + \left(\efrac{d^{2}\phi}{d\eps^{2}}\right)_{0}\right] \efrac{(\eps - \eps_{0})^{2}}{2}}\, d\eps.
\Tag{(353)}
\]
This gives
\[
\eta_{0} + \phi_{0}
  = \tfrac{1}{2} \log \left[-\frac{1}{2\pi} \left(\frac{d^{2}\eta}{d\eps^{2}}\right)_{0} - \frac{1}{2\pi} \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}\right],
\Tag{(354)}
\]
and
\[
\Bar{(\eps - \eps_{0})^{2}}
  = \left[-\left(\frac{d^{2}\eta}{d\eps^{2}}\right)_{0} - \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}\right]^{-1}.
\Tag{(355)}
\]
We shall have a close approximation in general when the
quantities equated in~\Eq{(355)} are very small, \ie, when
\[
-\left(\frac{d^{2}\eta}{d\eps^{2}}\right)_{0} - \left(\frac{d^{2}\phi}{d\eps^{2}}\right)_{0}
\Tag{(356)}
\]
is very great. Now when $n$~is very great, $-d^{2}\phi/d\eps^{2}$~is of the
same order of magnitude, and the condition that \Eq{(356)}~shall
be very great does not restrict very much the nature of the
function~$\eta$.

We may obtain other properties pertaining to average values
in a canonical ensemble by the method used for the average of
$d\phi/d\eps$. Let $u$~be any function of the energy, either alone or
with~$\Theta$ and the external coördinates. The average value of~$u$
in the ensemble is determined by the equation
\[
\bar{u} = \int_{V=0}^{\eps=\infty} u\, e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps.
\Tag{(357)}
\]
\PageSep{109}
Now we have identically
\[
\int_{V=0}^{\eps=\infty} u\, \left(\frac{du}{d\eps} - \frac{u}{\Theta} + u\, \frac{d\phi}{d\eps}\right) e^{\efrac{\psi - \eps}{\Theta} + \phi}\, d\eps
  = \left[u\,  e^{\efrac{\psi - \eps}{\Theta} + \phi}\right]_{V=0}^{\eps=\infty}\Add{.}
\Tag{(358)}
\]
Therefore, by the preceding equation
\[
\frac{\Bar{du}}{d\eps} - \frac{\bar{u}}{\Theta} + \Bar{u\, \frac{d\phi}{d\eps}}
  = \left[u\,  e^{\efrac{\psi - \eps}{\Theta} + \phi}\right]_{V=0}^{\eps=\infty}\Add{.}\footnotemark
\Tag{(359)}
\]
\footnotetext{A more general equation, which is not limited to ensembles canonically
  distributed, is
  \[
  \frac{\Bar{du}}{d\eps} + \Bar{u\, \frac{d\eta}{d\eps}} + \Bar{u\, \frac{d\phi}{d\eps}}
  = \left[u\,  e^{\eta + \phi}\right]_{V=0}^{\eps=\infty}\Add{,}
  \]
  where $\eta$~denotes, as usual, the index of probability of phase.}%

If we set $u = 1$, (a value which need not be excluded,) the
second member of this equation vanishes, as shown on page~\Pageref{101},
if $n > 2$, and we get
\[
\frac{\Bar{d\phi}}{d\eps} = \frac{1}{\Theta},
\Tag{(360)}
\]
as before. It is evident from the same considerations that the
second member of~\Eq{(359)} will always vanish if $n > 2$, unless $u$~becomes
infinite at one of the limits, in which case a more careful
examination of the value of the expression will be necessary.
To facilitate the discussion of such cases, it will be convenient
to introduce a certain limitation in regard to the nature of the
system considered. We have necessarily supposed, in all our
treatment of systems canonically distributed, that the system
considered was such as to be capable of the canonical distribution
with the given value of the modulus. We shall now
suppose that the system is such as to be capable of a canonical
distribution with any (finite)\footnote
  {The term \emph{finite} applied to the modulus is intended to exclude the value
  zero as well as infinity.}
modulus. Let us see what
cases we exclude by this last limitation.
\PageSep{110}

The impossibility of a canonical distribution occurs when
the equation
\[
e^{-\efrac{\psi}{\Theta}} = \int_{V=0}^{\eps=\infty} e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps
\Tag{(361)}
\]
fails to determine a finite value for~$\psi$. Evidently the equation
cannot make~$\psi$ an infinite positive quantity, the impossibility
therefore occurs when the equation makes $\psi = -\infty$. Now
we get easily from~\Eq{(191)}
\[
d\frac{\psi}{\Theta} = -\frac{\bar{\eps}}{\Theta^{2}}\, d\Theta.
\]
If the canonical distribution is possible for any values of~$\Theta$,
we can apply this equation so long as the canonical distribution
is possible. The equation shows that as $\Theta$~is increased
(without becoming infinite) $-\psi$~cannot become infinite unless
$\bar{\eps}$~simultaneously becomes infinite, and that as $\Theta$~is decreased
(without becoming zero) $-\psi$~cannot become infinite unless
simultaneously $\bar{\eps}$~becomes an infinite negative quantity. The
corresponding cases in thermodynamics would be bodies which
could absorb or give out an infinite amount of heat without
passing certain limits of temperature, when no external work
is done in the positive or negative sense. Such infinite values
present no analytical difficulties, and do not contradict the
general laws of mechanics or of thermodynamics, but they
are quite foreign to our ordinary experience of nature. In
excluding such cases (which are certainly not entirely devoid
of interest) we do not exclude any which are analogous to
any actual cases in thermodynamics.

We assume then that for any finite value of~$\Theta$ the second
member of~\Eq{(361)} has a finite value.

When this condition is fulfilled, the second member of~\Eq{(359)}
will vanish for $u = e^{-\phi} V$. For, if we set $\Theta' = 2\Theta$,
\[
e^{-\efrac{\eps}{\Theta}} V
  = e^{-\efrac{\eps}{\Theta}} \int_{V=0}^{\eps} e^{\phi}\, d\eps
  \leq e^{-\efrac{\eps}{\Theta'}} \int_{V=0}^{\eps} e^{-\efrac{\eps}{\Theta'} + \phi}\, d\eps
  \leq e^{-\efrac{\eps'}{\Theta'}}e^{-\efrac{\psi'}{\Theta'}},
\]
\PageSep{111}
where $\psi'$~denotes the value of~$\psi$ for the modulus~$\Theta'$. Since
the last member of this formula vanishes for $\eps = \infty$, the
less value represented by the first member must also vanish
for the same value of~$\eps$. Therefore the second member of~\Eq{(359)},
which differs only by a constant factor, vanishes at
the upper limit. The case of the lower limit remains to be
considered. Now
\[
e^{-\efrac{\eps}{\Theta}} V
  \leq \int_{V=0}^{\eps} e^{-\efrac{\eps}{\Theta} + \phi}\, d\eps.
\]
The second member of this formula evidently vanishes for
the value of~$\eps$, which gives $V = 0$, whether this be finite or
negative infinity. Therefore, the second member of~\Eq{(359)}
vanishes at the lower limit also, and we have
\[
1 - \Bar{e^{-\phi} V\, \frac{d\phi}{d\eps}} - \Bar{e^{-\phi} \frac{V}{\Theta}} + \Bar{e^{-\phi} V\, \frac{d\phi}{d\eps}} = 0,
\]
or
\[
\Bar{e^{-\phi} V} = \Theta.
\Tag{(362)}
\]
This equation, which is subject to no restriction in regard to
the value of~$n$, suggests a connection or analogy between the
function of the energy of a system which is represented by
$e^{-\phi} V$ and the notion of temperature in thermodynamics. We
shall return to this subject in Chapter~XIV\@.

If $n > 2$, the second member of~\Eq{(359)} may easily be shown
to vanish for any of the following values of~$u$ viz.: $\phi$,~$e^{\phi}$, $\eps$,~$\eps^{m}$,
where $m$~denotes any positive number. It will also
vanish, when $n > 4$, for $u = d\phi/d\eps$, and when $n > 2h$ for
$u = e^{-\phi}\, d^{h}V/d\eps^{h}$. When the second member of~\Eq{(359)} vanishes,
and $n > 2$, we may write
\[
\Bar{(u - \bar{u}) \left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)}
  = \Bar{u\, \frac{d\phi}{d\eps}} - \frac{\bar{u}}{\Theta}
  = -\frac{\Bar{du}}{d\eps}.
\Tag{(363)}
\]
We thus obtain the following equations:

If $n > 2$,
\[
\Bar{(\phi - \bar{\phi}) \left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)}
  = \Bar{\phi\, \frac{d\phi}{d\eps}} - \frac{\bar{\phi}}{\Theta}
  = -\frac{1}{\Theta},
\Tag{(364)}
\]
\PageSep{112}
\[
\Bar{(e^{\phi} - \Bar{e^{\phi}}) \left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)}
  = \Bar{e^{\phi}\, \frac{d\phi}{d\eps}} - \frac{\Bar{e^{\phi}}}{\Theta}
  = -\Bar{e^{\phi}\, \frac{d\phi}{d\eps}},
\Tag{(365)}
\]
or
\begin{gather*}
2\Bar{e^{\phi}\, \frac{d\phi}{d\eps}}
  = \Bar{e^{\phi}}\, \frac{\Bar{d\phi}}{d\eps} = \frac{\Bar{e^{\phi}}}{\Theta},
\Tag{(366)}\displaybreak[0] \\
%
\Bar{(\eps - \bar{\eps}) \left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)}
  = \Bar{\eps\, \frac{d\phi}{d\eps}} - \frac{\Bar{\eps}}{\Theta}
  = -1,\footnotemark
\Tag{(367)}\displaybreak[0] \\
%
\Bar{(\eps^{m} - \bar{\eps}^{m}) \left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)}
  = \Bar{\eps^{m}\, \frac{d\phi}{d\eps}} - \frac{\Bar{\eps^{m}}}{\Theta}
  = -m \Bar{\eps^{m-1}}.
\Tag{(368)}
\end{gather*}
\footnotetext{This equation may also be obtained from equations \Eq{(252)} and~\Eq{(321)}.
  Compare also equation~\Eq{(349)} which was derived by an approximative method.}%

If $n > 4$,
\[
\Bar{\left(\frac{d\phi}{d\eps} - \frac{1}{\Theta}\right)^{2}}
  = \Bar{\left(\frac{d\phi}{d\eps}\right)^{2}} - \frac{1}{\Theta^{2}}
  = -\Bar{\Typo{\left(\frac{d^{2}\phi}{d\eps^{2}}\right)^{2}}{\frac{d^{2}\phi}{d\eps^{2}}}}.\footnotemark
\Tag{(369)}
\]
\footnotetext{Compare equation~\Eq{(360)}, obtained by an approximative method.}%

If $n > 2h$
\[
\Bar{e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}\, \frac{d\phi}{d\eps}} - \frac{1}{\Theta}\, \Bar{e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}}
  = \Bar{e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}\, \frac{d\phi}{d\eps}} - \Bar{e^{-\phi}\, \frac{d^{h+1}V}{d\eps^{h+1}}}
\]
or
\[
\Bar{e^{-\phi}\, \frac{d^{h+1}V}{d\eps^{h+1}}}
  = \frac{1}{\Theta}\, \Bar{e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}}\Add{,}
\Tag{(370)}
\]
whence
\[
\Bar{e^{-\phi}\, \frac{d^{h+1}V}{d\eps^{h+1}}}
  = \frac{1}{\Theta^{h}}.
\Tag{(371)}
\]
Giving $h$~the values $1$, $2$, $3$, etc., we have
\begin{gather*}
\frac{\Bar{d\phi}}{d\eps} = \frac{1}{\Theta},\quad\text{if $n > 2$,} \\
\frac{\Bar{d^{2}\phi}}{d\eps^{2}} + \Bar{\left(\frac{d\phi}{d\eps}\right)^{2}}
  = \frac{1}{\Theta^{2}},\quad\text{if $n > 4$,} \\
\end{gather*}
as already obtained. Also
\[
\frac{\Bar{d^{3}\phi}}{d\eps^{3}} + 3 \Bar{\frac{d^{2}\phi}{d\eps^{2}}\, \frac{d\phi}{d\eps}} + \Bar{\left(\frac{d\phi}{d\eps}\right)^{3}}
  = \frac{1}{\Theta^{3}},\quad\text{if $n > 6$.}
\Tag{(372)}
\]
\PageSep{113}

If $V_{q}$~is a continuous increasing function of~$\eps_{q}$, commencing
with $V_{q} = 0$, the average value in a canonical ensemble of any
function of~$\eps_{q}$, either alone or with the modulus and the external
coördinates, is given by equation~\Eq{(275)}, which is identical
with~\Eq{(357)} except that $\eps$,~$\phi$, and~$\psi$ have the suffix $(\ )_{q}$. The
equation may be transformed so as to give an equation identical
with~\Eq{(359)} except for the suffixes. If we add the same
suffixes to equation~\Eq{(361)}, the finite value of its members will
determine the possibility of the canonical distribution.

From these data, it is easy to derive equations similar to
\Eq{(360)}, \Eq{(362)}--\Eq{(372)}, except that the conditions of their validity
must be differently stated. The equation
\[
\Bar{e^{-\phi_{q}} V_{q}} = \Theta
\]
requires only the condition already mentioned with respect to~$V_{q}$.
This equation corresponds to~\Eq{(362)}, which is subject to
no restriction with respect to the value of~$n$. We may observe,
however, that $V$~will always satisfy a condition similar
to that mentioned with respect to~$V_{q}$.

If $V_{q}$~satisfies the condition mentioned, and $e^{\phi_{q}}$~a similar
condition, \ie, if $e^{\phi_{q}}$~is a continuous increasing function of~$\eps_{q}$,
commencing with the value $\eps_{\phi_{q}} = 0$, equations will hold similar
to those given for the case when $n > 2$, viz., similar to
\Eq{(360)}, \Eq{(364)}--\Eq{(368)}. Especially important is
\[
\frac{\Fsp\Bar{d\phi_{q}}\Fsp}{d\eps_{q}} = \frac{1}{\Theta}.
\]
If $V_{q}$, $e^{\phi_{q}}$, (or $dV_{q}/d\eps_{q}$), $d^{2}V_{q}/d\eps_{q}^{2}$ all satisfy similar conditions,
we shall have an equation similar to~\Eq{(369)}, which was subject
to the condition $n > 4$. And if $d^{3}V_{q}/d\eps_{q}^{3}$ also satisfies a
similar condition, we shall have an equation similar to~\Eq{(372)},
for which the condition was $n > 6$. Finally, if $V_{q}$~and $h$ successive
differential coefficients satisfy conditions of the kind
mentioned, we shall have equations like \Eq{(370)}~and \Eq{(371)} for
which the condition was $n > 2h$.
\PageSep{114}

These conditions take the place of those given above relating
to~$n$. In fact, we might give conditions relating to the
differential coefficients of~$V$, similar to those given relating to
the differential coefficients of~$V_{q}$, instead of the conditions
relating to~$n$, for the validity of equations \Eq{(360)}, \Eq{(363)}--\Eq{(372)}.
This would somewhat extend the application of the equations.
\PageSep{115}


\Chapter{X.}{On a Distribution in Phase called Microcanonical
in which all the Systems have the same energy.}

\First{An} important case of statistical equilibrium is that in which
all systems of the ensemble have the same energy. We may
arrive at the notion of a distribution which will satisfy the
necessary conditions by the following process. We may
suppose that an ensemble is distributed with a uniform density-in-phase
between two limiting values of the energy, $\eps'$ and~$\eps''$,
and with density zero outside of those limits. Such an
ensemble is evidently in statistical equilibrium according to
the criterion in Chapter~IV, since the density-in-phase may be
regarded as a function of the energy. By diminishing the
difference of $\eps'$ and~$\eps''$, we may diminish the differences of
energy in the ensemble. The limit of this process gives us
a permanent distribution in which the energy is constant.

We should arrive at the same result, if we should make the
density any function of the energy between the limits $\eps'$ and~$\eps''$,
and zero outside of those limits. Thus, the limiting distribution
obtained from the part of a canonical ensemble
between two limits of energy, when the difference of the
limiting energies is indefinitely diminished, is independent of
the modulus, being determined entirely by the energy, and
is identical with the limiting distribution obtained from a
uniform density between limits of energy approaching the
same value.

We shall call the limiting distribution at which we arrive
by this process \emph{microcanonical}.

We shall find however, in certain cases, that for certain
values of the energy, viz., for those for which $e^{\phi}$~is infinite,
\PageSep{116}
this process fails to define a limiting distribution in any such
distinct sense as for other values of the energy. The difficulty
is not in the process, but in the nature of the case, being
entirely analogous to that which we meet when we try to find
a canonical distribution in cases when $\psi$~becomes infinite.
We have not regarded such cases as affording true examples
of the canonical distribution, and we shall not regard the cases
in which $e^{\phi}$~is infinite as affording true examples of the microcanonical
distribution. We shall in fact find as we go on that
in such cases our most important formulae become illusory.

The use of formulae relating to a canonical ensemble which
contain $e^{\phi}\, d\eps$ instead of $dp_{1} \dots dq_{n}$, as in the preceding chapters,
amounts to the consideration of the ensemble as divided into
an infinity of microcanonical elements.

From a certain point of view, the microcanonical distribution
may seem more simple than the canonical, and it has perhaps
been more studied, and been regarded as more closely related
to the fundamental notions of thermodynamics. To this last
point we shall return in a subsequent chapter. It is sufficient
here to remark that analytically the canonical distribution is
much more manageable than the microcanonical.

We may sometimes avoid difficulties which the microcanonical
distribution presents by regarding it as the result of the
following process, which involves conceptions less simple but
more amenable to analytical treatment. We may suppose an
ensemble distributed with a density proportional to
\[
e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}}},
\]
where $\omega$~and $\eps'$ are constants, and then diminish indefinitely
the value of the constant~$\omega$. Here the density is nowhere
zero until we come to the limit, but at the limit it is zero for
all energies except~$\eps'$. We thus avoid the analytical complication
of discontinuities in the value of the density, which
require the use of integrals with inconvenient limits.

In a microcanonical ensemble of systems the energy~($\eps$) is
constant, but the kinetic energy~($\eps_{p}$) and the potential energy~($\eps_{q}$)
\PageSep{117}
vary in the different systems, subject of course to the condition
\[
\eps_{p} + \eps_{q} = \eps = \constant.
\Tag{(373)}
\]
Our first inquiries will relate to the division of energy into
these two parts, and to the average values of functions of~$\eps_{p}$
and~$\eps_{q}$.

We shall use the notation~$\Avg{u}$ to denote an average value in
a microcanonical ensemble of energy~$\eps$. An average value
in a canonical ensemble of modulus~$\Theta$, which has hitherto
been denoted by~$\bar{u}$, we shall in this chapter denote by~$\Avg[\Theta]{u}$, to
distinguish more clearly the two kinds of averages.

The extension-in-phase within any limits which can be given
in terms of $\eps_{p}$~and $\eps_{q}$ may be expressed in the notations of the
preceding chapter by the double integral
\[
\iint dV_{p}\, dV_{q}
\]
taken within those limits. If an ensemble of systems is distributed
within those limits with a uniform density-in-phase,
the average value in the ensemble of any function~($u$) of the
kinetic and potential energies will be expressed by the quotient
of integrals
\[
\frac{\dsp \iint u\, dV_{p}\, dV_{q}}{\dsp \iint dV_{p}\, dV_{q}}\Add{.}
\]

Since $dV_{p} = e^{\phi_{p}}\, d\eps_{p}$, and $d\eps_{p} = d\eps$ when $\eps_{q}$~is constant, the
expression may be written
\[
\frac{\dsp \iint u\, e^{\phi_{p}}\, d\eps\, dV_{q}}{\dsp \iint e^{\phi_{p}}\, d\eps\, dV_{q}}\Add{.}
\]
To get the average value of~$u$ in an ensemble distributed
microcanonically with the energy~$\eps$, we must make the integrations
cover the extension-in-phase between the energies
$\eps$~and $\eps + d\eps$. This gives
\PageSep{118}
\[
\Avg{u} = \frac{\dsp d\eps \int_{V_{q}=0}^{\eps_{q}=\eps} u\, e^{\phi_{p}}\, dV_{q}}
               {\dsp d\eps \int_{V_{q}=0}^{\eps_{q}=\eps} e^{\phi_{p}}\, dV_{q}}\Add{.}
\]
But by~\Eq{(299)} the value of the integral in the denominator
is~$e^{\phi}$. We have therefore
\[
\Avg{u} = e^{-\phi} \int_{V_{q}=0}^{\eps_{q}=\eps} u\, e^{\phi_{p}}\, dV_{q},
\Tag{(374)}
\]
where $e^{\phi_{p}}$~and $V_{q}$ are connected by equation~\Eq{(373)}, and $u$, if
given as function of~$\eps_{p}$, or of $\eps_{p}$~and $\eps_{q}$, becomes in virtue of
the same equation a function of $\eps_{q}$~alone.

We shall assume that $e^{\phi}$~has a finite value. If $n > 1$, it is
evident from equation~\Eq{(305)} that $e^{\phi}$~is an increasing function
of~$\eps$, and therefore cannot be infinite for one value of~$\eps$ without
being infinite for all greater values of~$\eps$, which would make
$-\psi$ infinite.\footnote
  {See equation~\Eq{(322)}.}
When $n > 1$, therefore, if we assume that $e^{\phi}$~is
finite, we only exclude such cases as we found necessary
to exclude in the study of the canonical distribution. But
when $n = 1$, cases may occur in which the canonical distribution
is perfectly applicable, but in which the formulae for the
microcanonical distribution become illusory, for particular values
of~$\eps$, on account of the infinite value of~$e^{\phi}$. Such failing
cases of the microcanonical distribution \emph{for particular values
of the energy} will not prevent us from regarding the canonical
ensemble as consisting of an infinity of microcanonical
ensembles.\footnote
  {An example of the failing case of the microcanonical distribution is
  afforded by a material point, under the influence of gravity, and constrained
  to remain in a vertical circle. The failing case occurs when the energy is
  just sufficient to carry the material point to the highest point of the circle.

  It will be observed that the difficulty is inherent in the nature of the case,
  and is quite independent of the mathematical formulae. The nature of the
  difficulty is at once apparent if we try to distribute a finite number of
  material points with this particular value of the energy as nearly as possible
  in statistical equilibrium, or if we ask: What is the probability that a point
  taken at random from an ensemble in statistical equilibrium with this value
  of the energy will be found in any specified part of the circle?}
\PageSep{119}

From the last equation, with~\Eq{(298)}, we get
\[
\Avg{e^{-\phi_{p}} V_{p}} = e^{-\phi} \int_{V=0}^{\eps_{p}=\eps} V_{p}\, dV_{q}
  = e^{-\phi} V.
\Tag{(375)}
\]
But by equations \Eq{(288)} and~\Eq{(289)}
\[
e^{-\phi_{p}} V_{p} = \frac{2}{n} \eps_{p}.
\Tag{(376)}
\]
Therefore
\[
e^{-\phi} V = \Avg{e^{-\phi_{p}} V_{p}} = \frac{2}{n}\, \Avg{\eps_{p}}.
\Tag{(377)}
\]

Again, with the aid of equation~\Eq{(301)}, we get
\[
\Avg{\frac{d\phi_{p}}{d\eps_{p}}}
  = e^{-\phi} \int_{V=0}^{\eps_{p}=\eps} \frac{d\phi_{p}}{d\eps_{p}} e^{\phi_{p}}\, dV_{q}
  = \frac{d\phi}{d\eps},
\Tag{(378)}
\]
if $n > 2$. Therefore, by~\Eq{(289)}
\[
\frac{d\phi}{d\eps} = \Avg{\frac{d\phi_{p}}{d\eps_{p}}}
  = \left(\frac{n}{2} - 1\right) \Avg{\eps_{p}^{-1}},\quad\text{if $n > 2$.}
\Tag{(379)}
\]

These results are interesting on account of the relations of
the functions $e^{-\phi} V$ and $\dfrac{d\phi}{d\eps}$ to the notion of temperature in
thermodynamics,---a subject to which we shall return hereafter.
They are particular cases of a general relation easily
deduced from equations \Eq{(306)}, \Eq{(374)}, \Eq{(288)} and~\Eq{(289)}. We
have
\[
\frac{d^{h}V}{d\eps^{h}}
  = \int_{V=0}^{\eps_{p}=\eps} \frac{d^{h}V_{p}}{d\eps_{p}^{h}}\, dV_{q},\quad\text{if $h < \tfrac{1}{2}n + 1$.}
\]
The equation may be written
\[
e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}
  = e^{-\phi} \int_{V=0}^{\eps_{p}=\eps} e^{-\phi_{p}} \frac{d^{h}V_{p}}{d\eps_{p}^{h}} e^{-\phi_{p}}\, dV_{q}.
\]
\PageSep{120}
We have therefore
\[
e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}
  = \Avg{e^{-\phi_{p}}\, \frac{d^{h}V_{p}}{d\eps_{p}^{h}}}
  = \frac{\Gamma(\frac{1}{2}n)}{\Gamma(\frac{1}{2}n - h + 1)}\, \Avg{\eps_{p}^{h-1}},
\Tag{(380)}
\]
if $h < \frac{1}{2}n + 1$. For example, when $n$~is even, we may make
$h = \frac{1}{2}n$, which gives, with~\Eq{(307)},
\[
(2\pi)^{\efrac{n}{2}} e^{-\phi} (V_{q})_{\eps_{q}=\eps}
  = \Gamma(\tfrac{1}{2}n)\, \Avg{\eps_{p}^{1 - \efrac{n}{2}}}.
\Tag{(381)}
\]

Since any canonical ensemble of systems may be regarded
as composed of microcanonical ensembles, if any quantities
$u$~and $v$ have the same average values in every microcanonical
ensemble, they will have the same values in every canonical
ensemble. To bring equation~\Eq{(380)} formally under this rule,
we may observe that the first member being a function of~$\eps$ is
a constant value in a microcanonical ensemble, and therefore
identical with its average value. We get thus the general
equation
\[
\Avg[\Theta]{e^{-\phi}\, \frac{d^{h}V}{d\eps^{h}}}
  = \Avg[\Theta]{e^{-\phi_{p}}\, \frac{d^{h}V_{p}}{d\eps_{p}^{h}}}
  = \frac{\Gamma(\frac{1}{2}n)}{\Gamma(\frac{1}{2}n - h + 1)}\, \Avg[\Theta]{\eps_{p}^{h-1}}
  = \Theta^{1-h},
\Tag{(382)}
\]
if $h < \frac{1}{2}n + 1$.\footnote
  {See equation~\Eq{(292)}.}
The equations
\begin{gather*}
\Theta = \Avg[\Theta]{e^{-\phi} V}
  = \Avg[\Theta]{e^{-\phi_{p}} V_{p}}
  = \frac{2}{n}\, \Avg[\Theta]{\eps_{p}},
\Tag{(383)}\displaybreak[0] \\
%
\frac{1}{\Theta} = \Avg[\Theta]{\frac{d\phi}{d\eps}}
  = \Avg[\Theta]{\frac{d\phi_{p}}{d\eps_{p}}}
  = \left(\frac{n}{2} - 1\right) \Avg[\Theta]{\eps_{p}^{-1}},
\Tag{(384)}
\end{gather*}
may be regarded as particular cases of the general equation.
The last equation is subject to the condition that $n > 2$.

The last two equations give for a canonical ensemble,
if $n > 2$,
\[
\left(1 - \frac{2}{n}\right) \Avg[\Theta]{\eps_{p}}\, \Avg[\Theta]{\eps_{p}^{-1}} = 1.
\Tag{(385)}
\]
The corresponding equations for a microcanonical ensemble
give, if $n > 2$,
\[
\left(1 - \frac{2}{n}\right) \Avg{\eps_{p}}\, \Avg{\eps_{p}^{-1}} = \frac{d\phi}{d\log V},
\Tag{(386)}
\]
\PageSep{121}
which shows that $d\phi\, d\log V$ approaches the value unity
when $n$~is very great.

If a system consists of two parts, having separate energies,
we may obtain equations similar in form to the preceding,
which relate to the system as thus divided.\footnote
  {If this condition is rigorously fulfilled, the parts will have no influence
  on each other, and the ensemble formed by distributing the whole microcanonically
  is too arbitrary a conception to have a real interest. The principal
  interest of the equations which we shall obtain will be in cases in
  which the condition is approximately fulfilled. But for the purposes of a
  theoretical discussion, it is of course convenient to make such a condition
  absolute. Compare Chapter~IV, pp.~\Pageref{35}~ff., where a similar condition is considered
  in connection with canonical ensembles.}
We shall
distinguish quantities relating to the parts by letters with
suffixes, the same letters without suffixes relating to the
whole system. The extension-in-phase of the whole system
within any given limits of the energies may be represented by
the double integral
\[
\iint dV_{1}\, dV_{2}
\]
taken within those limits, as appears at once from the definitions
of Chapter~VIII\@. In an ensemble distributed with
uniform density within those limits, and zero density outside,
the average value of any function of $\eps_{1}$~and $\eps_{2}$ is given by the
quotient
\[
\frac{\dsp \iint u\, dV_{1}\, dV_{2}}{\dsp \iint dV_{1}\, dV_{2}}
\]
which may also be written\footnote
  {Where the analytical transformations are identical in form with those
  on the preceding pages, it does not appear necessary to give all the steps
  with the same detail.}%
\[
\frac{\dsp \iint u\, e^{\phi_{1}}\, d\eps\, dV_{2}}{\dsp \iint e^{\phi_{1}}\, d\eps\, dV_{2}}\Add{.}
\]
If we make the limits of integration $\eps$~and $\eps + d\eps$, we get the
\PageSep{122}
average value of~$u$ in an ensemble in which the whole system
is microcanonically distributed in phase, viz.,
\[
\Avg{u} = e^{-\phi} \int_{V_{2}=0}^{\eps_{2}=\eps} u\, e^{\phi_{1}}\, dV_{2},
\Tag{(387)}
\]
where $\phi_{1}$~and $V_{2}$ are connected by the equation
\[
\eps_{1} + \eps_{2} = \constant = \eps,
\Tag{(388)}
\]
and $u$, if given as function of~$\eps_{1}$, or of $\eps_{1}$ and~$\eps_{2}$, becomes in
virtue of the same equation a function of $\eps_{2}$~alone.\footnote
  {In the applications of the equation~\Eq{(387)}, we cannot obtain all the results
  corresponding to those which we have obtained from equation~\Eq{(374)}, because
  $\phi_{p}$~is a known function of~$\eps_{p}$, while $\phi_{1}$~must be treated as an arbitrary function
  of~$\eps_{1}$, or nearly so.}

%[** TN: Newline but no indent in the original]
Thus
\begin{gather*}
\Avg{e^{-\phi_{1}} V_{1}} = e^{-\phi} \int_{V_{2}=0}^{\eps_{2}=\eps} V_{1}\, dV_{2},
\Tag{(389)}\displaybreak[0] \\
%
e^{-\phi} V = \Avg{e^{-\phi_{1}} V_{1}} = \Avg{e^{-\phi_{2}} V_{2}}.
\Tag{(390)}
\end{gather*}
This requires a similar relation for canonical averages
\[
\Theta = \Avg[\Theta]{e^{-\phi} V}
  = \Avg[\Theta]{e^{-\phi_{1}} V_{1}}
  = \Avg[\Theta]{e^{-\phi_{2}} V_{2}}.
\Tag{(391)}
\]
Again
\[
\Avg{\frac{d\phi_{1}}{d\eps_{1}}}
  = e^{-\phi} \int_{V_{2}=0}^{\eps_{2}=\eps} \frac{d\phi_{1}}{d\eps_{1}}\, e^{\phi_{1}}\, dV_{2}.
\Tag{(392)}
\]
But if $n_{1} > 2$, $e^{\phi_{1}}$~vanishes for $V_{1} = 0$,\footnote
  {See Chapter~VIII, equations \Eq{(306)} and~\Eq{(316)}.}
and
\[
\frac{d}{d\eps} e^{\phi}
  = \frac{d}{d\eps} \int_{V_{2}=0}^{\eps_{2}=\eps} e^{\phi_{1}}\, dV_{2}
  = \int_{V_{2}=0}^{\eps_{2}=\eps} \frac{d\phi_{1}}{d\eps_{1}}\, e^{\phi_{1}}\, dV_{2}.
\Tag{(393)}
\]
Hence, if $n_{1} > 2$, and $n_{2} > 2$,
\[
\frac{d\phi}{d\eps}
  = \Avg{\frac{d\phi_{1}}{d\eps_{1}}}
  = \Avg{\frac{d\phi_{2}}{d\eps_{2}}},
\Tag{(394)}
\]
\PageSep{123}
and
\[
\frac{1}{\Theta}
  = \Avg[\Theta]{\frac{d\phi}{d\eps}}
  = \Avg[\Theta]{\frac{d\phi_{1}}{d\eps_{1}}}
  = \Avg[\Theta]{\frac{d\phi_{2}}{d\eps_{2}}}.
\Tag{(395)}
\]

We have compared certain functions of the energy of the
whole system with average values of similar functions of
the kinetic energy of the whole system, and with average
values of similar functions of the whole energy of a part of
the system. We may also compare the same functions with
average values of the kinetic energy of a part of the system.

We shall express the total, kinetic, and potential energies of
the whole system by $\eps$,~$\eps_{p}$, and $\eps_{q}$, and the kinetic energies of the
parts by $\eps_{1p}$ and~$\eps_{2p}$. These kinetic energies are necessarily separate:
we need not make any supposition concerning potential
energies. The extension-in-phase within any limits which can
be expressed in terms of $\eps_{q}$, $\eps_{1p}$, $\eps_{2p}$ may be represented in the
notations of Chapter~VIII by the triple integral
\[
\iiint dV_{1p}\, dV_{2p}\, dV_{q}
\]
taken within those limits. And if an ensemble of systems is
distributed with a uniform density within those limits, the
average value of any function of $\eps_{q}$, $\eps_{1p}$, $\eps_{2p}$ will be expressed
by the quotient
\[
\frac{\dsp \iiint u\, dV_{1p}\, dV_{2p}\, dV_{q}}{\dsp \iiint dV_{1p}\, dV_{2p}\, dV_{q}}
\]
or
\[
\frac{\dsp \iiint u\, e^{\phi_{1p}}\, d\eps\, dV_{2p}\, dV_{q}}{\dsp \iiint e^{\phi_{1p}}\, d\eps\, dV_{2p}\, dV_{q}}\Add{.}
\]
To get the average value of~$u$ for a microcanonical distribution,
we must make the limits $\eps$~and $\eps + d\eps$. The denominator
in this case becomes $e^{\phi}\, d\eps$, and we have
\[
\Avg{u} = e^{-\phi} \int_{V_{q}=0}^{\eps_{q}=\eps} \int_{\eps_{2p}=0}^{\eps_{2p}=\eps-\eps_{q}} u\, e^{\phi_{1p}}\, dV_{2p}\, dV_{q},
\Tag{(396)}
\]
\PageSep{124}
where $\phi_{1p}$, $V_{2p}$, and $V_{q}$ are connected by the equation
\[
\eps_{1p} + \eps_{2p} + \eps_{q} = \constant = \eps.
\]

Accordingly
\[
\Avg{e^{-\phi_{1p}} V_{1p}}
  = e^{-\phi} \int_{V_{q}=0}^{\eps_{q}=\eps} \int_{\eps_{2p}=0}^{\eps_{2p}=\eps-\eps_{q}} V_{1p}\, dV_{2p}\, dV_{q}
  = e^{-\phi} V,
\Tag{(397)}
\]
and we may write
\[
e^{-\phi} V
  = \Avg{e^{-\phi_{1p}} V_{1p}}
  = \Avg{e^{-\phi_{2p}} V_{2p}}
  = \frac{2}{n_{1}}\, \Avg{\eps_{1p}}
  = \frac{2}{n_{2}}\, \Avg{\eps_{2p}},
\Tag{(398)}
\]
and
\[
\Theta
  = \Avg[\Theta]{e^{-\phi} V}
  = \Avg[\Theta]{e^{-\phi_{1p}} V_{1p}}
  = \Avg[\Theta]{e^{-\phi_{2p}} V_{2p}}
  = \frac{2}{n_{1}}\, \Avg[\Theta]{\eps_{1p}}
  = \frac{2}{n_{2}}\, \Avg[\Theta]{\eps_{2p}}.
\Tag{(399)}
\]

Again, if $n_{1} > 2$,
\begin{multline*}
\Avg{\frac{d\phi_{1p}}{d\eps_{1p}}}
  = e^{-\phi} \int_{V_{q}=0}^{\eps_{q}=\eps} \int_{\eps_{2p}=0}^{\eps_{2p}=\eps-\eps_{q}} \frac{d\phi_{1p}}{d\eps_{1p}}\, e^{\phi_{1p}}\, dV_{2p}\, dV_{q} \\
  = e^{-\phi} \int_{V_{q}=0}^{\eps_{q}=\eps} \frac{de^{\phi_{p}}}{d\eps_{p}}\, dV_{q}
  = e^{-\phi} \frac{de^{\phi}}{d\eps}
  = \frac{d\phi}{d\eps}.
\Tag{(400)}
\end{multline*}
Hence, if $n_{1} > 2$, and $n_{2} > 2$,
\begin{gather*}
\frac{d\phi}{d\eps}
  = \Avg{\frac{d\phi_{1p}}{d\eps_{1p}}}
  = \Avg{\frac{d\phi_{2p}}{d\eps_{2p}}}
  = (\tfrac{1}{2}n_{1} - 1) \Avg{\eps_{1p}^{-1}}
  = (\tfrac{1}{2}n_{2} - 1) \Avg{\eps_{2p}^{-1}},
\Tag{(401)}\displaybreak[0] \\
%
\frac{1}{\Theta}
  = \Avg[\Theta]{\frac{d\phi}{d\eps}}\!\!
  = \Avg[\Theta]{\frac{d\phi_{1p}}{d\eps_{1p}}}\!\!
  = \Avg[\Theta]{\frac{d\phi_{2p}}{d\eps_{2p}}}\!\!
  = (\tfrac{1}{2}n_{1} - 1) \Avg[\Theta]{\eps_{1p}^{-1}}
  = (\tfrac{1}{2}n_{2} - 1) \Avg[\Theta]{\eps_{2p}^{-1}}.
\Tag{(402)}
\end{gather*}

We cannot apply the methods employed in the preceding
pages to the microcanonical averages of the (generalized)
forces $A_{1}$, $A_{2}$, etc., exerted by a system on external bodies,
since these quantities are not functions of the energies, either
kinetic or potential, of the whole or any part of the system.
We may however use the method described on page~\Pageref{116}.%
\PageSep{125}

Let us imagine an ensemble of systems distributed in phase
according to the index of probability
\[
c - \frac{(\eps - \eps')^{2}}{\omega^{2}},
\]
where $\eps'$~is any constant which is a possible value of the
energy, except only the least value which is consistent with
the values of the external coördinates, and $c$~and $\omega$ are other
constants. We have therefore
\[
\intap e^{c - \efrac{(\eps - \eps')^{2}}{\omega^{2}}}\, dp_{1} \dots dq_{n} = 1,
\Tag{(403)}
\]
or
\[
e^{-c} = \intap e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}}}\, dp_{1} \dots dq_{n},
\Tag{(404)}
\]
or again
\[
e^{-c} = \int_{V=0}^{\eps=\infty} e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps.
\Tag{(405)}
\]
From \Eq{(404)} we have
\begin{align*}
\frac{de^{-c}}{da_{1}}
  &= \intap 2 \frac{\eps - \eps'}{\omega^{2}} A_{1}\, e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}}}\, dp_{1} \dots dq_{n} \\
  &= \int_{V=0}^{\eps=\infty} 2 \frac{\eps - \eps'}{\omega^{2}} \Avg{A_{1}}\, e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps,
\Tag{(406)}
\end{align*}
where $\Avg{A_{1}}$ denotes the average value of~$A_{1}$ in those systems
of the ensemble which have any same energy~$\eps$. (This
is the same thing as the average value of~$A_{1}$ in a microcanonical
ensemble of energy~$\eps$.) The validity of the transformation
is evident, if we consider separately the part of each integral
which lies between two infinitesimally differing limits of
energy. Integrating by parts, we get
\PageSep{126}
\begin{multline*}
\frac{de^{-c}}{da_{1}}
  = -\left[\Avg{A_{1}}\, e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\right]_{V=0}^{\eps=\infty} \\
  + \int_{V=0}^{\eps=\infty} \left(\frac{d\Avg{A_{1}}}{d\eps} + \Avg{A_{1}}\, \frac{d\phi}{d\eps}\right) e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps.
\Tag{(407)}
\end{multline*}
Differentiating~\Eq{(405)}, we get
\[
\frac{de^{-c}}{da_{1}}
  = \int_{V=0}^{\eps=\infty} \frac{d\phi}{da_{1}}\, e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps
  - \left(e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, \frac{d\eps_{\alpha}}{da_{1}}\right)_{V=0}\Add{,}
\Tag{(408)}
\]
where $\eps_{\alpha}$~denotes the least value of~$\eps$ consistent with the external
coördinates. The last term in this equation represents the
part of $de^{-c}/da_{1}$ which is due to the variation of the lower
limit of the integral. It is evident that the expression in the
brackets will vanish at the upper limit. At the lower limit,
at which $\eps_{p} = 0$, and $\eps_{q}$~has the least value consistent with the
external coördinates, the average sign on~$\Avg{A_{1}}$ is superfluous,
as there is but one value of~$A_{1}$ which is represented by
$-d\eps_{\alpha}/da_{1}$. Exceptions may indeed occur for particular values
of the external coördinates, at which $d\eps_{\alpha}/da_{1}$ receive a finite
increment, and the formula becomes illusory. Such particular
values we may for the moment leave out of account. The
last term of~\Eq{(408)} is therefore equal to the first term of the
second member of~\Eq{(407)}. (We may observe that both vanish
when $n > 2$ on account of the factor~$e^{\phi}$.)

We have therefore from these equations
\[
\int_{V=0}^{\eps=\infty}\! \left(\frac{d\Avg{A_{1}}}{d\eps} + \Avg{A_{1}}\, \frac{d\phi}{d\eps}\right) e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps\!
  = \int_{V=0}^{\eps=\infty} \frac{d\phi}{da_{1}}\, e^{-\efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps,
\]
or
\[
\int_{V=0}^{\eps=\infty} \left(\frac{d\Avg{A_{1}}}{d\eps} + \Avg{A_{1}}\, \frac{d\phi}{d\eps} - \frac{d\phi}{da_{1}}\right) e^{c - \efrac{(\eps - \eps')^{2}}{\omega^{2}} + \phi}\, d\eps = 0.
\Tag{(409)}
\]
That is: the average value in the ensemble of the quantity
represented by the principal parenthesis is zero. This must
\PageSep{127}
be true for any value of~$\omega$. If we diminish~$\omega$, the average
value of the parenthesis at the limit when $\omega$~vanishes becomes
identical with the value for $\eps = \eps'$. But this may be any value
of the energy, except the least possible. We have therefore
\[
\frac{d\Avg{A_{1}}}{d\eps} + \Avg{A_{1}}\, \frac{d\phi}{d\eps} - \frac{d\phi}{da_{1}} = 0,
\Tag{(410)}
\]
unless it be for the least value of the energy consistent with
the external coördinates, or for particular values of the external
coördinates. But the value of any term of this equation
as determined for particular values of the energy and
of the external coördinates is not distinguishable from its
value as determined for values of the energy and external
coördinates indefinitely near those particular values. The
equation therefore holds without limitation. Multiplying
by~$e^{\phi}$, we get
\[
e^{\phi}\, \frac{d\Avg{A_{1}}}{d\eps} + \Avg{A_{1}}\, e^{\phi}\, \frac{d\phi}{d\eps}
  = e^{\phi}\, \frac{d\phi}{da_{1}}
  = \frac{de^{\phi}}{da_{1}}
  = \frac{d^{2}V}{da_{1}\, d\eps}.
\Tag{(411)}
\]
The integral of this equation is
\[
\Avg{A_{1}}\, e^{\phi} = \frac{dV}{da_{1}} + F_{1},
\Tag{(412)}
\]
where $F_{1}$~is a function of the external coördinates. We have
an equation of this form for each of the external coördinates.
This gives, with~\Eq{(266)}, for the complete value of the differential
of~$V$
\[
dV = e^{\phi}\, d\eps
  + (e^{\phi} \Avg{A_{1}} - F_{1})\, da_{1}
  + (e^{\phi} \Avg{A_{2}} - F_{2})\, da_{2} + \text{etc.},
\Tag{(413)}
\]
or
\[
dV = e^{\phi}(d\eps + \Avg{A_{1}}\, da_{1} + \Avg{A_{2}}\, da_{2} + \text{etc.})
  - F_{1}\, da_{1} - F_{2}\, da_{2} - \text{etc.}
\Tag{(414)}
\]
To determine the values of the functions $F_{1}$, $F_{2}$, etc., let
us suppose $a_{1}$, $a_{2}$, etc.\ to vary arbitrarily, while $\eps$~varies so
as always to have the least value consistent with the values
of the external coördinates. This will make $V = 0$, and
$dV = 0$. If $n < 2$, we shall have also $e^{\phi} = 0$, which will
give
\[
F_{1} = 0,\quad F_{2} = 0,\quad\text{etc.}
\Tag{(415)}
\]
\PageSep{128}
The result is the same for any value of~$n$. For in the variations
considered the kinetic energy will be constantly zero,
and the potential energy will have the least value consistent
with the external coördinates. The condition of the least
possible potential energy may limit the ensemble at each instant
to a single configuration, or it may not do so; but in any
case the values of $A_{1}$, $A_{2}$, etc.\ will be the same at each instant
for all the systems of the ensemble,\footnote
  {This statement, as mentioned before, may have exceptions for particular
  values of the external coördinates. This will not invalidate the reasoning,
  which has to do with varying values of the external coördinates.}
and the equation
\[
d\eps + A_{1}\, da_{1} + A_{2}\, da_{2} + \text{etc.} = 0
\]
will hold for the variations considered. Hence the functions
$F_{1}$, $F_{2}$, etc.\ vanish in any case, and we have the equation
\[
dV = e^{\phi}\, d\eps
  + e^{\phi}\, \Avg{A_{1}}\, da_{1}
  + e^{\phi}\, \Avg{A_{2}}\, da_{2} + \text{etc.},
\Tag{(416)}
\]
or
\[
d\log V = \frac{d\eps + \Avg{A_{1}}\, da_{1} + \Avg{A_{2}}\, da_{2} + \text{etc.}}{e^{-\phi} V},
\Tag{(417)}
\]
or again
\[
d\eps  = e^{-\phi} V d\log V
  - \Avg{A_{1}}\, da_{1}
  - \Avg{A_{2}}\, da_{2} - \text{etc.}
\Tag{(418)}
\]
It will be observed that the two last equations have the form
of the fundamental differential equations of thermodynamics,
$e^{-\phi} V$~corresponding to temperature and $\log V$ to entropy.
We have already observed properties of~$e^{-\phi} V$ suggestive of an
analogy with temperature.\footnote
  {See Chapter~IX, page~\Pageref{111}; also this chapter, page~\Pageref{119}.}
The significance of these facts
will be discussed in another chapter.

The two last equations might be written more simply
\begin{gather*}
dV = \frac{d\eps + \Avg{A_{1}}\, da_{1} + \Avg{A_{2}}\, da_{2} + \text{etc.}}{e^{-\phi}}, \\
d\eps  = e^{-\phi}\, dV - \Avg{A_{1}}\, da_{1} - \Avg{A_{2}}\, da_{2} - \text{etc.},
\end{gather*}
and still have the form analogous to the thermodynamic
equations, but $e^{-\phi}$~has nothing like the analogies with temperature
which we have observed in~$e^{-\phi} V$.
\PageSep{129}


\Chapter{XI.}{Maximum and Minimum Properties of Various Distributions
in Phase.}

\First{In} the following theorems we suppose, as always, that the
systems forming an ensemble are identical in nature and in
the values of the external coördinates, which are here regarded
as constants.

\Theorem{I\@.} If an ensemble of systems is so distributed in
phase that the index of probability is a function of the energy,
the average value of the index is less than for any other distribution
in which the distribution in energy is unaltered.

Let us write~$\eta$ for the index which is a function of the
energy, and $\eta + \Delta\eta$ for any other which gives the same distribution
in energy. It is to be proved that
\[
\intap (\eta + \Delta\eta) e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  > \intap \eta e^{\eta}\, dp_{1} \dots dq_{n},
\Tag{(419)}
\]
where $\eta$~is a function of the energy, and $\Delta\eta$~a function of the
phase, which are subject to the conditions that
\[
\intap e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intap e^{\eta}\, dp_{1} \dots dq_{n} = 1,
\Tag{(420)}
\]
and that for any value of the energy~($\eps'$)
\[
\intslim[\eps=\eps']{\eps=\eps' + d\eps'} e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intslim[\eps=\eps']{\eps=\eps' + d\eps'} e^{\eta}\, dp_{1} \dots dq_{n}.
\Tag{(421)}
\]
{\Loosen Equation~\Eq{(420)} expresses the general relations which $\eta$ and
$\eta + \Delta\eta$ must satisfy in order to be indices of any distributions,
and \Eq{(421)}~expresses the condition that they give the same
distribution in energy.}
\PageSep{130}

Since $\eta$~is a function of the energy, and may therefore be regarded
as a constant within the limits of integration of~\Eq{(421)},
we may multiply by~$\eta$ under the integral sign in both members,
which gives
\[
\intslim[\eps=\eps']{\eps=\eps' + d\eps'} \eta e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intslim[\eps=\eps']{\eps=\eps' + d\eps'} \eta e^{\eta}\, dp_{1} \dots dq_{n}.
\]
Since this is true within the limits indicated, and for every
value of~$\eps'$, it will be true if the integrals are taken for all
phases. We may therefore cancel the corresponding parts of~\Eq{(419)},
which gives
\[
\intap \Delta\eta\, e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n} > 0.
\Tag{(422)}
\]
But by~\Eq{(420)} this is equivalent to
\[
\intap (\Delta\eta\, e^{\Delta\eta} + 1 - e^{\Delta\eta}) e^{\eta}\, dp_{1} \dots dq_{n} > 0.
\Tag{(423)}
\]
Now $\Delta\eta\, e^{\Delta\eta} + 1 - e^{\Delta\eta}$ is a decreasing function of~$\Delta\eta$ for negative
values of~$\Delta\eta$, and an increasing function of~$\Delta\eta$ for positive
values of~$\Delta\eta$. It vanishes for $\Delta\eta = 0$. The expression is
therefore incapable of a negative value, and can have the value~$0$
only for $\Delta\eta = 0$. The inequality~\Eq{(423)} will hold therefore
unless $\Delta\eta = 0$ for all phases. The theorem is therefore
proved.

\Theorem{II\@.} If an ensemble of systems is canonically distributed
in phase, the average index of probability is less than
in any other distribution of the ensemble having the same
average energy.

{\Loosen For the canonical distribution let the index be $(\psi - \eps)/\Theta$,
and for another having the same average energy let the index
be $(\psi - \eps)/\Theta + \Delta\eta$, where $\Delta\eta$~is an arbitrary function of the
phase subject only to the limitation involved in the notion of
the index, that}
\PageSep{131}
\[
\intap e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intap e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 1,
\Tag{(424)}
\]
and to that relating to the constant average energy, that
\[
\intap \eps\, e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intap \eps\, e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}.
\Tag{(425)}
\]

It is to be proved that
\begin{multline*}
\intap \left(\frac{\psi}{\Theta} - \frac{\eps}{\Theta} + \Delta\eta\right) e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n} \\
  > \intap \left(\frac{\psi}{\Theta} - \frac{\eps}{\Theta}\right)e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n}.
\Tag{(426)}
\end{multline*}
Now in virtue of the first condition~\Eq{(424)} we may cancel the
constant term~$\psi/\Theta$ in the parentheses in~\Eq{(426)}, and in virtue
of the second condition~\Eq{(425)} we may cancel the term~$\eps/\Theta$.
The proposition to be proved is thus reduced to
\[
\intap \Delta\eta\, e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n} > 0,
\]
which may be written, in virtue of the condition~\Eq{(424)},
\[
\intap (\Delta\eta\, e^{\Delta\eta} + 1 - e^{\Delta\eta}) e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} > 0.
\Tag{(427)}
\]
In this form its truth is evident for the same reasons which
applied to~\Eq{(423)}.

\Theorem{III\@.} If $\Theta$~is any positive constant, the average
value in an ensemble of the expression $\eta + \eps/\Theta$ ($\eta$~denoting
as usual the index of probability and $\eps$~the energy) is less when
the ensemble is distributed canonically with modulus~$\Theta$, than
for any other distribution whatever.

In accordance with our usual notation let us write
$(\psi - \eps)/\Theta$ for the index of the canonical distribution. In any
other distribution let the index be $(\psi - \eps)/\Theta + \Delta\eta$.
\PageSep{132}

In the canonical ensemble $\eta + \eps/\Theta$ has the constant value
$\psi/\Theta$; in the other ensemble it has the value $\psi/\Theta + \Delta\eta$.
The proposition to be proved may therefore be written
\[
\frac{\psi}{\Theta} < \intap \left(\frac{\psi}{\Theta} + \Delta\eta\right) e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n},
\Tag{(428)}
\]
where
\[
\intap e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \intap e^{\efrac{\psi - \eps}{\Theta}}\, dp_{1} \dots dq_{n} = 1.
\Tag{(429)}
\]
In virtue of this condition, since $\psi/\Theta$~is constant, the proposition
to be proved reduces to
\[
0 < \intap \Delta\eta\, e^{\efrac{\psi - \eps}{\Theta} + \Delta\eta}\, dp_{1} \dots dq_{n},
\Tag{(430)}
\]
where the demonstration may be concluded as in the last
theorem.

If we should substitute for the energy in the preceding
theorems any other function of the phase, the theorems, \textit{mutatis
mutandis}, would still hold. On account of the unique
importance of the energy as a function of the phase, the theorems
as given are especially worthy of notice. When the case
is such that other functions of the phase have important
properties relating to statistical equilibrium, as described
in Chapter~IV,\footnote
  {See pages \Pageref{37}--\Pageref{41}.}
the three following theorems, which are
generalizations of the preceding, may be useful. It will be
sufficient to give them without demonstration, as the principles
involved are in no respect different.

\Theorem{IV\@.} If an ensemble of systems is so distributed in
phase that the index of probability is any function of $F_{1}$, $F_{2}$,
etc., (these letters denoting functions of the phase,) the average
value of the index is less than for any other distribution in
phase in which the distribution with respect to the functions
$F_{1}$, $F_{2}$, etc.\ is unchanged.%
\PageSep{133}

\Theorem{V\@.} If an ensemble of systems is so distributed
in phase that the index of probability is a linear function of
$F_{1}$, $F_{2}$, etc., (these letters denoting functions of the phase,) the
average value of the index is less than for any other distribution
in which the functions $F_{1}$, $F_{2}$, etc.\ have the same average
values.

\Theorem{VI\@.} The average value in an ensemble of systems
of $\eta + F$ (where $\eta$~denotes as usual the index of probability and
$F$~any function of the phase) is less when the ensemble is so
distributed that $\eta + F$ is constant than for any other distribution
whatever.

\Theorem{VII\@.} If a system which in its different phases
constitutes an ensemble consists of two parts, and we consider
the average index of probability for the whole system, and
also the average indices for each of the parts taken separately,
the sum of the average indices for the parts will be either less
than the average index for the whole system, or equal to it,
but cannot be greater. The limiting case of equality occurs
when the distribution in phase of each part is independent of
that of the other, and only in this case.

Let the coördinates and momenta of the whole system be
$q_{1}$\Add{,}~\dots $q_{n}$, $p_{1}$,~\dots $p_{n}$, of which $q_{1}$\Add{,}~\dots $q_{m}$, $p_{1}$,~\dots $p_{m}$ relate to one
part of the system, and $q_{m+1}$,~\dots $q_{n}$, $p_{m+1}$,~\dots $p_{n}$ to the other.
If the index of probability for the whole system is denoted by~$\eta$,
the probability that the phase of an unspecified system lies
within any given limits is expressed by the integral
\[
\ints e^{\eta}\, dp_{1} \dots dq_{n}
\Tag{(431)}
\]
taken for those limits. If we set
\[
\ints e^{\eta}\, dp_{m+1} \dots dp_{n}\, dq_{m+1} \dots dq_{n} = e^{\eta_{1}},
\Tag{(432)}
\]
where the integrations cover all phases of the second system,
and
\[
\ints e^{\eta}\, dp_{1} \dots dp_{m}\, dq_{1} \dots dq_{m} = e^{\eta_{2}},
\Tag{(433)}
\]
\PageSep{134}
where the integrations cover all phases of the first system,
the integral~\Eq{(431)} will reduce to the form
\[
\ints e^{\eta_{1}}\, dp_{1} \dots dp_{m}\, dq_{1} \dots dq_{m},
\Tag{(434)}
\]
when the limits can be expressed in terms of the coördinates
and momenta of the first part of the system. The same integral
will reduce to
\[
\ints e^{\eta_{2}}\, dp_{m+1} \dots dp_{n}\, dq_{m+1} \dots dq_{n},
\Tag{(435)}
\]
when the limits can be expressed in terms of the coördinates
and momenta of the second part of the system. It is evident
that $\eta_{1}$~and $\eta_{2}$ are the indices of probability for the two parts
of the system taken separately.

The main proposition to be proved may be written
\begin{multline*}
\ints \eta_{1} e^{\eta_{1}}\, dp_{1} \dots dq_{m}
  + \ints \eta_{2} e^{\eta_{2}}\, dp_{m+1} \dots dq_{n} \\
  \leq \ints \eta e^{\eta}\, dp_{1} \dots dq_{n},
\Tag{(436)}
\end{multline*}
where the first integral is to be taken over all phases of the first
part of the system, the second integral over all phases of the
second part of the system, and the last integral over all phases
of the whole system. Now we have
\begin{gather*}
\ints e^{\eta}\, dp_{1} \dots dq_{n} = 1,
\Tag{(437)}\displaybreak[0] \\
%
\ints e^{\eta_{1}}\, dp_{1} \dots dq_{m} = 1,
\Tag{(438)}\displaybreak[0] \\
%
\intertext{and}
\ints e^{\eta_{2}}\, dp_{m+1} \dots dq_{n} = 1,
\Tag{(439)}
\end{gather*}
where the limits cover in each case all the phases to which the
variables relate. The two last equations, which are in themselves
evident, may be derived by partial integration from the
first.
\PageSep{135}

It appears from the definitions of $\eta_{1}$~and $\eta_{2}$ that \Eq{(436)}~may
also be written
\begin{multline*}
\ints \eta_{1} e^{\eta}\, dp_{1} \dots dq_{n}
  + \ints \eta_{2} e^{\eta}\, dp_{1} \dots dq_{n} \\
  \leq \ints \eta e^{\eta}\, dp_{1} \dots dq_{n},
\Tag{(440)}
\end{multline*}
or
\[
\ints (\eta - \eta_{1} -\eta_{2}) e^{\eta}\, dp_{1} \dots dq_{n} \geq 0,
\]
where the integrations cover all phases. Adding the equation
\[
\ints e^{\eta_{1} + \eta_{2}}\, dp_{1} \dots dq_{n} = 1,
\Tag{(441)}
\]
which we get by multiplying \Eq{(438)} and~\Eq{(439)}, and subtracting~\Eq{(437)},
we have for the proposition to be proved
\[
\intap \bigl[(\eta - \eta_{1} -\eta_{2}) e^{\eta} + e^{\eta_{1} + \eta_{2}} - e^{\eta}\bigr]\, dp_{1} \dots dq_{n} \geq 0.
\Tag{(442)}
\]
Let
\[
u = \eta - \eta_{1} -\eta_{2}.
\Tag{(443)}
\]
The main proposition to be proved may be written
\[
\intap (u\, e^{u} + 1 - e^{u}) e^{\eta_{1} + \eta_{2}}\, dp_{1} \dots dq_{n} \geq 0.
\Tag{(444)}
\]
This is evidently true since the quantity in the parenthesis is
incapable of a negative value.\footnote
  {See Theorem~I, where this is proved of a similar expression.}
Moreover the sign~$=$ can
hold only when the quantity in the parenthesis vanishes for
all phases, \ie, when $u = 0$ for all phases. This makes
$\eta = \eta_{1} + \eta_{2}$ for all phases, which is the analytical condition
which expresses that the distributions in phase of the two
parts of the system are independent.

\Theorem{VIII\@.} If two or more ensembles of systems which
are identical in nature, but may be distributed differently in
phase, are united to form a single ensemble, so that the probability-coefficient
of the resulting ensemble is a linear function
\PageSep{136}
of the probability-coefficients of the original ensembles, the
average index of probability of the resulting ensemble cannot
be greater than the same linear function of the average indices
of the original ensembles. It can be equal to it only when
the original ensembles are similarly distributed in phase.

Let $P_{1}$, $P_{2}$, etc.\ be the probability-coefficients of the original
ensembles, and $P$~that of the ensemble formed by combining
them; and let $N_{1}$, $N_{2}$, etc.\ be the numbers of systems in the
original ensembles. It is evident that we shall have
\[
P = c_{1} P_{1} + c_{2} P_{2} + \text{etc.} = \sum (c_{1} P_{1}),
\Tag{(445)}
\]
where
\[
c_{1} = \frac{N_{1}}{\sum N_{1}},\quad
c_{2} = \frac{N_{2}}{\sum N_{1}},\quad \text{etc.}
\Tag{(446)}
\]
The main proposition to be proved is that
\[
\intap\! P \log P\, dp_{1} \dots dq_{n}
  \leq \sum \biggl[c_{1}\! \intap\! P_{1} \log P_{1}\, dp_{1} \dots dq_{n}\biggr]
\Tag{(447)}
\]
or
\[
\intap \left[\sum (c_{1} P_{1} \log P_{1}) - P \log P\right]\, dp_{1} \dots dq_{n} \geq 0.
\Tag{(448)}
\]

If we set
\[
Q_{1} = P_{1} \log P_{1} - P_{1} \log P - P_{1} + P
\]
$Q_{1}$~will be positive, except when it vanishes for $P_{1} = P$. To
prove this, we may regard $P_{1}$~and $P$ as any positive quantities.
Then
\begin{gather*}
\left(\frac{dQ_{1}}{dP_{1}}\right)_{P} = \log P_{1} - \log P,\displaybreak[0] \\
\left(\frac{d^{2}Q_{1}}{dP_{1}^{2}}\right)_{P} = \frac{1}{P_{1}}.
\end{gather*}
{\Loosen Since $Q_{1}$~and $dQ_{1}/dP_{1}$ vanish for $P_{1} = P$, and the second
differential coefficient is always positive, $Q_{1}$~must be positive
except when $P_{1} = P$. Therefore, if $Q_{2}$,~etc.\ have similar
definitions,}
\[
\sum (c_{1}Q_{1}) \geq 0.
\Tag{(449)}
\]
\PageSep{137}
But since
\[
\sum (c_{1}P_{1}) = P
\]
and
\begin{gather*}
\sum c_{1} = 1, \\
\sum (c_{1}Q_{1}) = \sum (c_{1}P_{1} \log P_{1}) - P \log P.
\Tag{(450)}
\end{gather*}
This proves~\Eq{(448)}, and shows that the sign~$=$ will hold only
when
\[
P_{1} = P,\quad P_{2} = P,\quad\text{etc.}
\]
for all phases, \ie, only when the distribution in phase of the
original ensembles are all identical.

\Theorem{IX\@.} A uniform distribution of a given number of
systems within given limits of phase gives a less average index
of probability of phase than any other distribution.

Let $\eta$~be the constant index of the uniform distribution, and
$\eta + \Delta\eta$ the index of some other distribution. Since the number
of systems within the given limits is the same in the two
distributions we have
\[
\ints e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  = \ints e^{\eta}\, dp_{1} \dots dq_{n},
\Tag{(451)}
\]
where the integrations, like those which follow, are to be
taken within the given limits. The proposition to be proved
may be written
\[
\ints (\eta + \Delta\eta) e^{\eta + \Delta\eta}\, dp_{1} \dots dq_{n}
  > \ints \eta e^{\eta}\, dp_{1} \dots dq_{n},
\Tag{(452)}
\]
or, since $\eta$~is constant,
\[
\ints (\eta + \Delta\eta) e^{\Delta\eta}\, dp_{1} \dots dq_{n}
  > \ints \eta\, dp_{1} \dots dq_{n}.
\Tag{(453)}
\]
In \Eq{(451)} also we may cancel the constant factor~$e^{\eta}$, and multiply
by the constant factor $(\eta + 1)$. This gives
\[
\ints (\eta + 1) e^{\Delta\eta}\, dp_{1} \dots dq_{n}
  = \ints (\eta + 1)\, dp_{1} \dots dq_{n}.
\]
The subtraction of this equation will not alter the inequality
to be proved, which may therefore be written
\[
\ints (\Delta\eta - 1) e^{\Delta\eta}\, dp_{1} \dots dq_{n}
  > \ints -dp_{1} \dots dq_{n}
\]
\PageSep{138}
or
\[
\ints (\Delta\eta e^{\Delta\eta} - e^{\Delta\eta} + 1)\, dp_{1} \dots dq_{n} > 0.
\Tag{(454)}
\]
Since the parenthesis in this expression represents a positive
value, except when it vanishes for $\Delta\eta = 0$, the integral will
be positive unless $\Delta\eta$~vanishes everywhere within the limits,
which would make the difference of the two distributions
vanish. The theorem is therefore proved.
\PageSep{139}


\Chapter{XII.}{On the Motion of Systems and Ensembles of Systems
through Long Periods of Time.}

\First{An} important question which suggests itself in regard to any
case of dynamical motion is whether the system considered
will return in the course of time to its initial phase, or, if it
will not return exactly to that phase, whether it will do so to
any required degree of approximation in the course of a sufficiently
long time. To be able to give even a partial answer
to such questions, we must know something in regard to the
dynamical nature of the system. In the following theorem,
the only assumption in this respect is such as we have found
necessary for the existence of the canonical distribution.

If we imagine an ensemble of identical systems to be
distributed with a uniform density throughout any finite
extension-in-phase, the number of the systems which leave
the extension-in-phase and will not return to it in the course
of time is less than any assignable fraction of the whole
number; \emph{provided}, that the total extension-in-phase for the
systems considered between two limiting values of the energy
is finite, these limiting values being less and greater respectively
than any of the energies of the first-mentioned extension-in-phase.

To prove this, we observe that at the moment which we
call initial the systems occupy the given extension-in-phase.
It is evident that some systems must leave the extension
immediately, unless all remain in it forever. Those systems
which leave the extension at the first instant, we shall call
the \emph{front} of the ensemble. It will be convenient to speak of
this front as \emph{generating} the extension-in-phase through which it
passes in the course of time, as in geometry a surface is said to
\PageSep{140}
generate the volume through which it passes. In equal times
the front generates equal extensions in phase. This is an
immediate consequence of the principle of \emph{conservation of
extension-in-phase}, unless indeed we prefer to consider it as
a slight variation in the expression of that principle. For in
two equal short intervals of time let the extensions generated
be $A$ and~$B$. (We make the intervals short simply to avoid
the complications in the enunciation or interpretation of the
principle which would arise when the same extension-in-phase
is generated more than once in the interval considered.) Now
if we imagine that at a given instant systems are distributed
throughout the extension~$A$, it is evident that the same
systems will after a certain time occupy the extension~$B$,
which is therefore equal to~$A$ in virtue of the principle cited.
The front of the ensemble, therefore, goes on generating
equal extensions in equal times. But these extensions are
included in a finite extension, viz., that bounded by certain
limiting values of the energy. Sooner or later, therefore,
the front must generate phases which it has before generated.
Such second generation of the same phases must commence
with the initial phases. Therefore a portion at least of the
front must return to the original extension-in-phase. The
same is of course true of the portion of the ensemble which
follows that portion of the front through the same phases at
a later time.

It remains to consider how large the portion of the ensemble
is, which will return to the original extension-in-phase. There
can be no portion of the given extension-in-phase, the systems
of which leave the extension and do not return. For we can
prove for any portion of the extension as for the whole, that
at least a portion of the systems leaving it will return.

We may divide the given extension-in-phase into parts as
follows. There may be parts such that the systems within
them will never pass out of them. These parts may indeed
constitute the whole of the given extension. But if the given
extension is very small, these parts will in general be non-*existent.
There may be parts such that systems within them
\PageSep{141}
will all pass out of the given extension and all return within
it. The whole of the given extension-in-phase is made up of
parts of these two kinds. This does not exclude the possibility
of phases on the boundaries of such parts, such that
systems starting with those phases would leave the extension
and never return. But in the supposed distribution of an
ensemble of systems with a uniform density-in-phase, such
systems would not constitute any assignable fraction of the
whole number.

These distinctions may be illustrated by a very simple
example. If we consider the motion of a rigid body of
which one point is fixed, and which is subject to no forces,
we find three cases. (1)~The motion is periodic. (2)~The
system will never return to its original phase, but will return
infinitely near to it. (3)~The system will never return either
exactly or approximately to its original phase. But if we
consider any extension-in-phase, however small, a system
leaving that extension will return to it except in the case
called by Poinsot `singular,' viz., when the motion is a
rotation about an axis lying in one of two planes having
a fixed position relative to the rigid body. But all such
phases do not constitute any true \emph{extension-in-phase} in the
sense in which we have defined and used the term.\footnote
  {An ensemble of systems distributed in phase is a less simple and elementary
  conception than a single system. But by the consideration of
  suitable ensembles instead of single systems, we may get rid of the inconvenience
  of having to consider exceptions formed by particular cases of the
  integral equations of motion, these cases simply disappearing when the
  ensemble is substituted for the single system as a subject of study. This
  is especially true when the ensemble is distributed, as in the case called
  canonical, throughout an extension-in-phase. In a less degree it is true of
  the microcanonical ensemble, which does not occupy any extension-in-phase,
  (in the sense in which we have used the term,) although it is convenient to
  regard it as a limiting case with respect to ensembles which do, as we thus
  gain for the subject some part of the analytical simplicity which belongs to
  the theory of ensembles which occupy true extensions-in-phase.}%

In the same way it may be proved that the systems in a
canonical ensemble which at a given instant are contained
within any finite extension-in-phase will in general return to
\PageSep{142}
that extension-in-phase, if they leave it, the exceptions, \ie,
the number which pass out of the extension-in-phase and do
not return to it, being less than any assignable fraction of the
whole number. In other words, the probability that a system
taken at random from the part of a canonical ensemble which
is contained within any given extension-in-phase, will pass out
of that extension and not return to it, is zero.

A similar theorem may be enunciated with respect to a
microcanonical ensemble. Let us consider the fractional part
of such an ensemble which lies within any given limits of
phase. This fraction we shall denote by~$F$. It is evidently
constant in time since the ensemble is in statistical equilibrium.
The systems within the limits will not in general
remain the same, but some will pass out in each unit of time
while an equal number come in. Some may pass out never
to return within the limits. But the number which in any
time however long pass out of the limits never to return will
not bear any finite ratio to the number within the limits at
a given instant. For, if it were otherwise, let $f$~denote the
fraction representing such ratio for the time~$T$. Then, in
the time~$T$, the number which pass out never to return will
bear the ratio~$fF$ to the whole number in the ensemble, and
in a time exceeding~$T/(fF)$ the number which pass out of
the limits never to return would exceed the total number
of systems in the ensemble. The proposition is therefore
proved.

This proof will apply to the cases before considered, and
may be regarded as more simple than that which was given.
It may also be applied to any true case of statistical equilibrium.
By a true case of statistical equilibrium is meant such
as may be described by giving the general value of the probability
that an unspecified system of the ensemble is contained
within any given limits of phase.\footnote
  {An ensemble in which the systems are material points constrained to
  move in vertical circles, with just enough energy to carry them to the
  highest points, cannot afford a true example of statistical equilibrium. For
  any other value of the energy than the critical value mentioned, we might
  in various ways describe an ensemble in statistical equilibrium, while the
  same language applied to the critical value of the energy would fail to do
  so. Thus, if we should say that the ensemble is so distributed that the
  probability that a system is in any given part of the circle is proportioned
  to the time which a single system spends in that part, motion in either direction
  being equally probable, we should perfectly define a distribution in statistical
  equilibrium for any value of the energy except the critical value
  mentioned above, but for this value of the energy all the probabilities in
  question would vanish unless the highest point is included in the part of the
  circle considered, in which case the probability is unity, or forms one of its
  limits, in which case the probability is indeterminate. Compare the foot-note
  on page~\Pageref{118}.

  A still more simple example is afforded by the uniform motion of a
  material point in a straight line. Here the impossibility of statistical equilibrium
  is not limited to any particular energy, and the canonical distribution
  as well as the microcanonical is impossible.

  These examples are mentioned here in order to show the necessity of
  caution in the application of the above principle, with respect to the question
  whether we have to do with a true case of statistical equilibrium.

  Another point in respect to which caution must be exercised is that the
  \emph{part} of an ensemble of which the theorem of the return of systems is asserted
  should be entirely defined by \emph{limits} within which it is contained, and not by
  any such condition as that a certain function of phase shall have a given
  value. This is necessary in order that the part of the ensemble which is
  considered should be any assignable fraction of the whole. Thus, if we have
  a canonical ensemble consisting of material points in vertical circles, the
  theorem of the return of systems may be applied to a part of the ensemble
  defined as contained in a given part of the circle. But it may not be applied
  in all cases to a part of the ensemble defined as contained in a given part
  of the circle and having a given energy. It would, in fact, express the exact
  opposite of the truth when the given energy is the critical value mentioned
  above.}%
\PageSep{143}

Let us next consider whether an ensemble of isolated
systems has any tendency in the course of time toward a
state of statistical equilibrium.

There are certain functions of phase which are constant in
time. The distribution of the ensemble with respect to the
values of these functions is necessarily invariable, that is,
the number of systems within any limits which can be
specified in terms of these functions cannot vary in the course
of time. The distribution in phase which without violating
this condition gives the least value of the average index of
probability of phase~($\bar{\eta}$) is unique, and is that in which the
\PageSep{144}
index of probability~($\eta$) is a function of the functions mentioned.\footnote
  {See Chapter~XI, Theorem~IV\@.}
It is therefore a permanent distribution,\footnote
  {See Chapter~IV, \textit{sub init}.}
and the
only permanent distribution consistent with the invariability
of the distribution with respect to the functions of phase
which are constant in time.

It would seem, therefore, that we might find a sort of measure
of the deviation of an ensemble from statistical equilibrium
in the excess of the average index above the minimum which is
consistent with the condition of the invariability of the distribution
with respect to the constant functions of phase. But
we have seen that the index of probability is constant in time
for each system of the ensemble. The average index is therefore
constant, and we find by this method no approach toward
statistical equilibrium in the course of time.

Yet we must here exercise great caution. One function
may approach indefinitely near to another function, while
some quantity determined by the first does not approach the
corresponding quantity determined by the second. A line
joining two points may approach indefinitely near to the
straight line joining them, while its length remains constant.
We may find a closer analogy with the case under consideration
in the effect of stirring an incompressible liquid.\footnote
  {By \emph{liquid} is here meant the continuous body of theoretical hydrodynamics,
  and not anything of the molecular structure and molecular motions
  of real liquids.}
In
space of $2n$~dimensions the case might be made analytically
identical with that of an ensemble of systems of $n$~degrees
of freedom, but the analogy is perfect in ordinary
space. Let us suppose the liquid to contain a certain amount
of coloring matter which does not affect its hydrodynamic
properties. Now the state in which the density of the coloring
matter is uniform, \ie, the state of perfect mixture, which is
a sort of state of equilibrium in this respect that the distribution
of the coloring matter in space is not affected by the
internal motions of the liquid, is characterized by a minimum
\PageSep{145}
value of the average square of the density of the coloring
matter. Let us suppose, however, that the coloring matter is
distributed with a variable density. If we give the liquid any
motion whatever, subject only to the hydrodynamic law of
incompressibility,---it may be a steady flux, or it may vary
with the time,---the density of the coloring matter at any
same point of the liquid will be unchanged, and the average
square of this density will therefore be unchanged. Yet no
fact is more familiar to us than that stirring tends to bring a
liquid to a state of uniform mixture, or uniform densities of
its components, which is characterized by minimum values
of the average squares of these densities. It is quite true that
in the physical experiment the result is hastened by the
process of diffusion, but the result is evidently not dependent
on that process.

The contradiction is to be traced to the notion of the \emph{density}
of the coloring matter, and the process by which this quantity
is evaluated. This quantity is the limiting ratio of the
quantity of the coloring matter in an element of space to the
volume of that element. Now if we should take for our elements
of volume, after any amount of stirring, the spaces
occupied by the same portions of the liquid which originally
occupied any given system of elements of volume, the densities
of the coloring matter, thus estimated, would be identical
with the original densities as determined by the given system
of elements of volume. Moreover, if at the end of any finite
amount of stirring we should take our elements of volume in
any ordinary form but sufficiently small, the average square
of the density of the coloring matter, as determined by such
element of volume, would approximate to any required degree
to its value before the stirring. But if we take any element
of space of fixed position and dimensions, we may continue
the stirring so long that the densities of the colored liquid
estimated for these fixed elements will approach a uniform
limit, viz., that of perfect mixture.

The case is evidently one of those in which the limit of a
limit has different values, according to the order in which we
\PageSep{146}
apply the processes of taking a limit. If treating the elements
of volume as constant, we continue the stirring indefinitely,
we get a uniform density, a result not affected by making the
elements as small as we choose; but if treating the amount of
stirring as finite, we diminish indefinitely the elements of
volume, we get exactly the same distribution in density as
before the stirring, a result which is not affected by continuing
the stirring as long as we choose. The question is
largely one of language and definition. One may perhaps be
allowed to say that a finite amount of stirring will not affect
the mean square of the density of the coloring matter, but an
infinite amount of stirring may be regarded as producing a
condition in which the mean square of the density has its
minimum value, and the density is uniform. We may certainly
say that a sensibly uniform density of the colored component
may be produced by stirring. Whether the time
required for this result would be long or short depends upon
the nature of the motion given to the liquid, and the fineness
of our method of evaluating the density.

All this may appear more distinctly if we consider a special
case of liquid motion. Let us imagine a cylindrical mass of
liquid of which one sector of~$90°$ is black and the rest white.
Let it have a motion of rotation about the axis of the cylinder
in which the angular velocity is a function of the distance
from the axis. In the course of time the black and the white
parts would become drawn out into thin ribbons, which would
be wound spirally about the axis. The thickness of these ribbons
would diminish without limit, and the liquid would therefore
tend toward a state of perfect mixture of the black and
white portions. That is, in any given element of space, the
proportion of the black and white would approach $1:3$ as a limit.
Yet after any finite time, the total volume would be divided
into two parts, one of which would consist of the white liquid
exclusively, and the other of the black exclusively. If the
coloring matter, instead of being distributed initially with a
uniform density throughout a section of the cylinder, were
distributed with a density represented by any arbitrary function
\PageSep{147}
of the cylindrical coördinates $r$,~$\theta$ and~$z$, the effect of the
same motion continued indefinitely would be an approach to
a condition in which the density is a function of $r$~and $z$ alone.
In this limiting condition, the average square of the density
would be less than in the original condition, when the density
was supposed to vary with~$\theta$, although after any finite time
the average square of the density would be the same as at
first.

If we limit our attention to the motion in a single plane
perpendicular to the axis of the cylinder, we have something
which is almost identical with a diagrammatic representation
of the changes in distribution in phase of an ensemble of
systems of one degree of freedom, in which the motion is
periodic, the period varying with the energy, as in the case of
a pendulum swinging in a circular arc. If the coördinates
and momenta of the systems are represented by rectangular
coördinates in the diagram, the points in the diagram
representing the changing phases of moving systems, will
move about the origin in closed curves of constant energy.
The motion will be such that areas bounded by points representing
moving systems will be preserved. The only difference
between the motion of the liquid and the motion in the
diagram is that in one case the paths are circular, and in the
other they differ more or less from that form.

When the energy is proportional to $p^{2} + q^{2}$ the curves of
constant energy are circles, and the period is independent of
the energy. There is then no tendency toward a state of statistical
equilibrium. The diagram turns about the origin without
change of form. This corresponds to the case of liquid
motion, when the liquid revolves with a uniform angular
velocity like a rigid solid.

The analogy between the motion of an ensemble of systems
in an extension-in-phase and a steady current in an incompressible
liquid, and the diagrammatic representation of the case
of one degree of freedom, which appeals to our geometrical intuitions,
may be sufficient to show how the conservation of
\Typo{density in phase}{density-in-phase}, which involves the conservation of the
\PageSep{148}
average value of the index of probability of phase, is consistent
with an approach to a limiting condition in which that
average value is less. We might perhaps fairly infer from
such considerations as have been adduced that an approach
to a limiting condition of statistical equilibrium is the general
rule, when the initial condition is not of that character. But
the subject is of such importance that it seems desirable to
give it farther consideration.

Let us suppose that the total extension-in-phase for the
kind of system considered to be divided into equal elements~($DV$)
which are very small but not infinitely small. Let us
imagine an ensemble of systems distributed in this extension
in a manner represented by the index of probability~$\eta$, which
is an arbitrary function of the phase subject only to the restriction
expressed by equation~\Eq{(46)} of Chapter~I\@. We shall
suppose the elements~$DV$ to be so small that $\eta$~may in general
be regarded as sensibly constant within any one of them
at the initial moment. Let the path of a system be defined as
the series of phases through which it passes.

At the initial moment~($t'$) a certain system is in an element
of extension~$DV'$. Subsequently, at the time~$t''$, the same
system is in the element~$DV''$. Other systems which were
at first in~$DV'$ will at the time~$t''$ be in~$DV''$, but not all,
probably. The systems which were at first in~$DV'$ will at
the time~$t''$ occupy an extension-in-phase exactly as large as at
first. But it will probably be distributed among a very great
number of the elements~($DV$) into which we have divided
the total extension-in-phase. If it is not so, we can generally
take a later time at which it will be so. There will be exceptions
to this for particular laws of motion, but we will confine
ourselves to what may fairly be called the general case.
Only a very small part of the systems initially in~$DV'$ will
be found in~$DV''$ at the time~$t''$, and those which are found in~$DV''$
at that time were at the initial moment distributed
among a very large number of elements~$DV$.

What is important for our purpose is the value of~$\eta$, the
index of probability of phase in the element~$DV''$ at the time~$t''$.
\PageSep{149}
In the part of~$DV''$ occupied by systems which at the
time~$t'$ were in~$DV'$ the value of~$\eta$ will be the same as its
value in~$DV'$ at the time~$t'$, which we shall call~$\eta'$. In the
parts of~$DV''$ occupied by systems which at~$t'$ were in elements
very near to~$DV'$ we may suppose the value of~$\eta$ to
vary little from~$\eta'$. We cannot assume this in regard to parts
of~$DV''$ occupied by systems which at~$t'$ were in elements
remote from~$DV'$. We want, therefore, some idea of the
nature of the extension-in-phase occupied at~$t'$ by the systems
which at~$t''$ will occupy~$DV''$. Analytically, the problem
is identical with finding the extension occupied at~$t''$
by the systems which at~$t'$ occupied~$DV'$. Now the systems
in~$DV''$ which lie on the same path as the system first considered,
evidently arrived at~$DV''$ at nearly the same time,
and must have left~$DV'$ at nearly the same time, and therefore
at~$t'$ were in or near~$DV'$. We may therefore take~$\eta'$ as
the value for these systems. The same essentially is true of
systems in~$DV''$ which lie on paths very close to the path
already considered. But with respect to paths passing through
$DV'$ and~$DV''$, but not so close to the first path, we cannot
assume that the time required to pass from $DV'$ to~$DV''$ is
nearly the same as for the first path. The difference of the
times required may be small in comparison with~$t'' - t'$, but as
this interval can be as large as we choose, the difference of the
times required in the different paths has no limit to its possible
value. Now if the case were one of statistical equilibrium,
the value of~$\eta$ would be constant in any path, and if all
the paths which pass through~$DV''$ also pass through or near~$DV'$,
the value of~$\eta$ throughout~$DV''$ will vary little from~$\eta'$.
But when the case is not one of statistical equilibrium,
we cannot draw any such conclusion. The only conclusion
which we can draw with respect to the phase at~$t'$ of the systems
which at~$t''$ are in~$DV''$ is that they are nearly on the
same path.

Now if we should make a new estimate of indices of probability
of phase at the time~$t''$, using for this purpose the
elements~$DV$,---that is, if we should divide the number of
\PageSep{150}
systems in~$DV''$, for example, by the total number of systems,
and also by the extension-in-phase of the element, and take
the logarithm of the quotient, we would get a number which
would be less than the average value of~$\eta$ for the systems
within~$DV''$ based on the distribution in phase at the time~$t'$.\footnote
  {See Chapter~XI, Theorem~IX\@.}
Hence the average value of~$\eta$ for the whole ensemble of
systems based on the distribution at~$t''$ will be less than the
average value based on the distribution at~$t'$.

We must not forget that there are exceptions to this general
rule. These exceptions are in cases in which the laws
of motion are such that systems having small differences
of phase will continue always to have small differences of
phase.

It is to be observed that if the average index of probability in
an ensemble may be said in some sense to have a less value at
one time than at another, it is not necessarily priority in time
which determines the greater average index. If a distribution,
which is not one of statistical equilibrium, should be given
for a time~$t'$, and the distribution at an earlier time~$t''$ should
be defined as that given by the corresponding phases, if we
increase the interval leaving $t'$ fixed and taking~$t''$ at an earlier
and earlier date, the distribution at~$t''$ will in general approach
a limiting distribution which is in statistical equilibrium. The
determining difference in such cases is that between a definite
distribution at a definite time and the limit of a varying distribution
when the moment considered is carried either forward
or backward indefinitely.\footnote
  {One may compare the kinematical truism that when two points are
  moving with uniform velocities, (with the single exception of the case where
  the relative motion is zero,) their mutual distance at any definite time is less
  than for $t = \infty$, or $t = -\infty$.}%

But while the distinction of prior and subsequent events
may be immaterial with respect to mathematical fictions, it is
quite otherwise with respect to the events of the real world.
It should not be forgotten, when our ensembles are chosen to
illustrate the probabilities of events in the real world, that
\PageSep{151}
while the probabilities of subsequent events may often be
determined from the probabilities of prior events, it is rarely
the case that probabilities of prior events can be determined
from those of subsequent events, for we are rarely justified in
excluding the consideration of the antecedent probability of
the prior events.

It is worthy of notice that to take a system at random from
an ensemble at a date chosen at random from several given
dates, $t'$, $t''$, etc., is practically the same thing as to take a system
at random from the ensemble composed of all the systems
of the given ensemble in their phases at the time~$t'$, together
with the same systems in their phases at the time~$t''$, etc. By
Theorem~VIII of Chapter~XI this will give an ensemble in
which the average index of probability will be less than in
the given ensemble, except in the case when the distribution
in the given ensemble is the same at the times $t'$, $t''$, etc.
Consequently, any indefiniteness in the time in which we take
a system at random from an ensemble has the practical effect
of diminishing the average index of the ensemble from which
the system may be supposed to be drawn, except when the
given ensemble is in statistical equilibrium.
\PageSep{152}


\Chapter{XIII.}{Effect of Various Processes on an Ensemble of
Systems.}

\First{In} the last chapter and in Chapter~I we have considered the
changes which take place in the course of time in an ensemble
of isolated systems. Let us now proceed to consider the
changes which will take place in an ensemble of systems under
external influences. These external influences will be of two
kinds, the variation of the coördinates which we have called
\emph{external}, and the action of other ensembles of systems. The
essential difference of the two kinds of influence consists in
this, that the bodies to which the external coördinates relate
are not distributed in phase, while in the case of interaction
of the systems of two ensembles, we have to regard the fact
that both are distributed in phase. To find the effect produced
on the ensemble with which we are principally concerned,
we have therefore to consider single values of what
we have called external coördinates, but an infinity of values
of the internal coördinates of any other ensemble with which
there is interaction.

Or,---to regard the subject from another point of view,---the
action between an unspecified system of an ensemble and
the bodies represented by the external coördinates, is the
action between a system imperfectly determined with respect
to phase and one which is perfectly determined; while the
interaction between two unspecified systems belonging to
different ensembles is the action between two systems both of
which are imperfectly determined with respect to phase.\footnote
  {In the development of the subject, we shall find that this distinction
  corresponds to the distinction in thermodynamics between mechanical and
  thermal action.}%

We shall suppose the ensembles which we consider to be
distributed in phase in the manner described in Chapter~I, and
\PageSep{153}
represented by the notations of that chapter, especially by the
index of probability of phase~($\eta$). There are therefore $2n$~independent
variations in the phases which constitute the
ensembles considered. This excludes ensembles like the
microcanonical, in which, as energy is constant, there are
only $2n - 1$ independent variations of phase. This seems
necessary for the purposes of a general discussion. For
although we may imagine a microcanonical ensemble to have
a permanent existence when isolated from external influences,
the effect of such influences would generally be to destroy the
uniformity of energy in the ensemble. Moreover, since the
microcanonical ensemble may be regarded as a limiting case of
such ensembles as are described in Chapter~I, (and that in
more than one way, as shown in Chapter~X,) the exclusion is
rather formal than real, since any properties which belong to
the microcanonical ensemble could easily be derived from those
of the ensembles of Chapter~I, which in a certain sense may
be regarded as representing the general case.

Let us first consider the effect of variation of the external
coördinates. We have already had occasion to regard these
quantities as variable in the differentiation of certain equations
relating to ensembles distributed according to certain laws
called canonical or microcanonical. That variation of the
external coördinates was, however, only carrying the attention
of the mind from an ensemble with certain values of the
external coördinates, and distributed in phase according to
some general law depending upon those values, to another
ensemble with different values of the external coördinates, and
with the distribution changed to conform to these new values.

What we have now to consider is the effect which would
actually result in the course of time in an ensemble of systems
in which the external coördinates should be varied in any
arbitrary manner. Let us suppose, in the first place, that
these coördinates are varied abruptly at a given instant, being
constant both before and after that instant. By the definition
of the external coördinates it appears that this variation does
not affect the phase of any system of the ensemble at the time
\PageSep{154}
when it takes place. Therefore it does not affect the index of
probability of phase~($\eta$) of any system, or the average value
of the index~($\bar{\eta}$) at that time. And if these quantities are
constant in time before the variation of the external coördinates,
and after that variation, their constancy in time is not
interrupted by that variation. In fact, in the demonstration
of the conservation of probability of phase in Chapter~I, the
variation of the external coördinates was not excluded.

But a variation of the external coördinates will in general
disturb a previously existing state of statistical equilibrium.
For, although it does not affect (at the first instant) the
distribution-in-phase, it does affect the condition necessary for
equilibrium. This condition, as we have seen in Chapter~IV,
is that the index of probability of phase shall be a function of
phase which is constant in time for moving systems. Now
a change in the external coördinates, by changing the forces
which act on the systems, will change the nature of the
functions of phase which are constant in time. Therefore,
the distribution in phase which was one of statistical equilibrium
for the old values of the external coördinates, will not
be such for the new values.

Now we have seen, in the last chapter, that when the dis\-tri\-bu\-tion-in-phase
is not one of statistical equilibrium, an
ensemble of systems may, and in general will, after a longer or
shorter time, come to a state which may be regarded, if very
small differences of phase are neglected, as one of statistical
equilibrium, and in which consequently the average value of
the index~($\bar{\eta}$) is less than at first. It is evident, therefore,
that a variation of the external coördinates, by disturbing a
state of statistical equilibrium, may indirectly cause a diminution,
(in a certain sense at least,) of the value of~$\bar{\eta}$.

But if the change in the external coördinates is very small,
the change in the distribution necessary for equilibrium will
in general be correspondingly small. Hence, the original distribution
in phase, since it differs little from one which would
be in statistical equilibrium with the new values of the external
coördinates, may be supposed to have a value of~$\bar{\eta}$
\PageSep{155}
which differs by a small quantity of the second order from
the minimum value which characterizes the state of statistical
equilibrium. And the diminution in the average index resulting
in the course of time from the very small change in the
external coördinates, cannot exceed this small quantity of
the second order.

Hence also, if the change in the external coördinates of an
ensemble initially in statistical equilibrium consists in successive
very small changes separated by very long intervals of
time in which the disturbance of statistical equilibrium becomes
sensibly effaced, the final diminution in the average
index of probability will in general be negligible, although the
total change in the external coördinates is large. The result
will be the same if the change in the external coördinates
takes place continuously but sufficiently slowly.

Even in cases in which there is no tendency toward the
restoration of statistical equilibrium in the lapse of time, a variation
of external coördinates which would cause, if it took
place in a short time, a great disturbance of a previous state
of equilibrium, may, if sufficiently distributed in time, produce
no sensible disturbance of the statistical equilibrium.

Thus, in the case of three degrees of freedom, let the systems
be heavy points suspended by elastic massless cords, and let the
ensemble be distributed in phase with a density proportioned
to some function of the energy, and therefore in statistical equilibrium.
For a change in the external coördinates, we may
take a horizontal motion of the point of suspension. If this
is moved a given distance, the resulting disturbance of the
statistical equilibrium may evidently be diminished indefinitely
by diminishing the velocity of the point of suspension.
This will be true if the law of elasticity of the string is such
that the period of vibration is independent of the energy, in
which case there is no tendency in the course of time toward
a state of statistical equilibrium, as well as in the more general
case, in which there is a tendency toward statistical equilibrium.

That something of this kind will be true in general, the
following considerations will tend to show.
\PageSep{156}

We define a path as the series of phases through which a
system passes in the course of time when the external coördinates
have fixed values. When the external coördinates
are varied, paths are changed. The path of a phase is the
path to which that phase belongs. With reference to any
ensemble of systems we shall denote by~$\Avg[p]{D}$ the average value
of the density-in-phase in a path. This implies that we have
a measure for comparing different portions of the path. We
shall suppose the time required to traverse any portion of a
path to be its measure for the purpose of determining this
average.

With this understanding, let us suppose that a certain ensemble
is in statistical equilibrium. In every element of
extension-in-phase, therefore, the density-in-phase~$D$ is equal
to its path-average~$\Avg[p]{D}$. Let a sudden small change be made
in the external coördinates. The statistical equilibrium will be
disturbed and we shall no longer have $D = \Avg[p]{D}$ everywhere.
This is not because $D$~is changed, but because $\Avg[p]{D}$~is changed,
the paths being changed. It is evident that if $D > \Avg[p]{D}$ in
a part of a path, we shall have $D < \Avg[p]{D}$ in other parts of the
same path.

Now, if we should imagine a further change in the external
coördinates of the same kind, we should expect it to produce
an effect of the same kind. But the manner in which the
second effect will be superposed on the first will be different,
according as it occurs immediately after the first change or
after an interval of time. If it occurs immediately after the
first change, then in any element of phase in which the first
change produced a positive value of $D - \Avg[p]{D}$ the second change
will add a positive value to the first positive value, and where
$D - \Avg[p]{D}$ was negative, the second change will add a negative
value to the first negative value.

But if we wait a sufficient time before making the second
change in the external coördinates, so that systems have
passed from elements of phase in which $D - \Avg[p]{D}$ was originally
positive to elements in which it was originally negative,
and \textit{vice versa}, (the systems carrying with them the values
\PageSep{157}
of $D - \Avg[p]{D}$,) the positive values of $D - \Avg[p]{D}$ caused by the
second change will be in part superposed on negative values
due to the first change, and \textit{vice versa}.

The disturbance of statistical equilibrium, therefore, produced
by a given change in the values of the external coördinates
may be very much diminished by dividing the
change into two parts separated by a sufficient interval of
time, and a sufficient interval of time for this purpose is one
in which the phases of the individual systems are entirely
unlike the first, so that any individual system is differently
affected by the change, although the whole ensemble is affected
in nearly the same way. Since there is no limit to the
diminution of the disturbance of equilibrium by division of
the change in the external coördinates, we may suppose as
a general rule that by diminishing the velocity of the changes
in the external coördinates, a given change may be made to
produce a very small disturbance of statistical equilibrium.

If we write~$\bar{\eta}'$ for the value of the average index of probability
before the variation of the external coördinates, and $\bar{\eta}''$~for the
value after this variation, we shall have in any case
\[
\bar{\eta}'' \leq \bar{\eta}'
\]
as the simple result of the variation of the external coördinates.
This may be compared with the thermodynamic theorem
that the entropy of a body cannot be diminished by
mechanical (as distinguished from thermal) action.\footnote
  {The correspondences to which the reader's attention is called are between
  $-\eta$~and entropy, and between $\Theta$~and temperature.}%

If we have (approximate) statistical equilibrium between
the times $t'$ and~$t''$ (corresponding to $\bar{\eta}'$ and~$\bar{\eta}''$), we shall have
approximately
\[
\bar{\eta}' = \bar{\eta}'',
\]
which may be compared with the thermodynamic theorem that
the entropy of a body is not (sensibly) affected by mechanical
action, during which the body is at each instant (sensibly) in
a state of thermodynamic equilibrium.

Approximate statistical equilibrium may usually be attained
\PageSep{158}
by a sufficiently slow variation of the external coördinates,
just as approximate thermodynamic equilibrium may usually
be attained by sufficient slowness in the mechanical operations
to which the body is subject.

We now pass to the consideration of the effect on an ensemble
of systems which is produced by the action of other
ensembles with which it is brought into dynamical connection.
In a previous chapter\footnote
  {See Chapter~IV, page~\Pageref{37}.}
we have imagined a dynamical
connection arbitrarily created between the systems of two
ensembles. We shall now regard the action between the
systems of the two ensembles as a result of the variation
of the external coördinates, which causes such variations
of the internal coördinates as to bring the systems of the
two ensembles within the range of each other's action.

Initially, we suppose that we have two separate ensembles
of systems, $E_{1}$ and~$E_{2}$. The numbers of degrees of freedom
of the systems in the two ensembles will be denoted by $n_{1}$~and
$n_{2}$ respectively, and the probability-coefficients by $e^{\eta_{1}}$ and $e^{\eta_{2}}$.
Now we may regard any system of the first ensemble combined
with any system of the second as forming a single
system of $n_{1} + n_{2}$ degrees of freedom. Let us consider the
ensemble~($E_{12}$) obtained by thus combining each system of the
first ensemble with each of the second.

At the initial moment, which may be specified by a single
accent, the probability-coefficient of any phase of the combined
systems is evidently the product of the probability-coefficients
of the phases of which it is made up. This may be expressed
by the equation,
\[
e^{\eta_{12}'} = e^{\eta_{1}'}\, e^{\eta_{2}'},
\Tag{(455)}
\]
or
\[
\eta_{12}' = \eta_{1}' + \eta_{2}',
\Tag{(456)}
\]
which gives
\[
\bar{\eta}_{12}' = \bar{\eta}_{1}' + \bar{\eta}_{2}',
\Tag{(457)}
\]

The forces tending to vary the internal coördinates of the
combined systems, together with those exerted by either
system upon the bodies represented by the coördinates called
\PageSep{159}
external, may be derived from a single force-function, which,
taken negatively, we shall call the potential energy of the
combined systems and denote by~$\eps_{12}$. But we suppose that
initially none of the systems of the two ensembles $E_{1}$ and
$E_{2}$ come within range of each other's action, so that the
potential energy of the combined system falls into two parts
relating separately to the systems which are combined. The
same is obviously true of the kinetic energy of the combined
compound system, and therefore of its total energy. This
may be expressed by the equation
\[
\eps_{12}' = \eps_{1}' + \eps_{2}',
\Tag{(458)}
\]
which gives
\[
\bar{\eps}_{12}' = \bar{\eps}_{1}' + \bar{\eps}_{2}'.
\Tag{(459)}
\]

Let us now suppose that in the course of time, owing to the
motion of the bodies represented by the coördinates called
external, the forces acting on the systems and consequently
their positions are so altered, that the systems of the ensembles
$E_{1}$ and $E_{2}$ are brought within range of each other's action,
and after such mutual influence has lasted for a time, by a
further change in the external coördinates, perhaps a return
to their original values, the systems of the two original ensembles
are brought again out of range of each other's action.
Finally, then, at a time specified by double accents, we shall
have as at first
\[
\bar{\eps}_{12}'' = \bar{\eps}_{1}'' + \bar{\eps}_{2}''.
\Tag{(460)}
\]
But for the indices of probability we must write\footnote
  {See Chapter~XI, Theorem~VII\@.}%
\[
\bar{\eta}_{1}'' + \bar{\eta}_{2}'' \leq \bar{\eta}_{12}''.
\Tag{(461)}
\]

The considerations adduced in the last chapter show that it
is safe to write
\[
\bar{\eta}_{12}'' \leq \bar{\eta}_{12}'.
\Tag{(462)}
\]
We have therefore
\[
\bar{\eta}_{1}'' + \bar{\eta}_{2}'' \leq \bar{\eta}_{1}' + \bar{\eta}_{2}',
\Tag{(463)}
\]
which may be compared with the thermodynamic theorem that
\PageSep{160}
the thermal contact of two bodies may increase but cannot
diminish the sum of their entropies.

Let us especially consider the case in which the two original
ensembles were both canonically distributed in phase with the
respective moduli $\Theta_{1}$ and~$\Theta_{2}$. We have then, by Theorem~III
of Chapter~XI,
\begin{gather*}
\bar{\eta}_{1}' + \frac{\eps_{1}'}{\Theta_{1}}
  \leq \bar{\eta}_{1}'' + \frac{\eps_{1}''}{\Theta_{1}}\Add{,}
\Tag{(464)}\displaybreak[0] \\
\bar{\eta}_{2}' + \frac{\eps_{2}'}{\Theta_{2}}
  \leq \bar{\eta}_{2}'' + \frac{\eps_{2}''}{\Theta_{2}}\Add{.}
\Tag{(465)}
\end{gather*}
Whence with~\Eq{(463)} we have
\[
\frac{\eps_{1}'}{\Theta_{1}} + \frac{\eps_{2}'}{\Theta_{2}}
  \leq \frac{\eps_{1}''}{\Theta_{1}} + \frac{\eps_{2}''}{\Theta_{2}}
\Tag{(466)}
\]
or
\[
\frac{\bar{\eps}_{1}'' - \bar{\eps}_{1}'}{\Theta_{1}}
  + \frac{\bar{\eps}_{2}'' - \bar{\eps}_{2}'}{\Theta_{2}} \geq 0.
\Tag{(467)}
\]
If we write $\bar{W}$ for the average work done by the combined
systems on the external bodies, we have by the principle of
the conservation of energy
\[
\bar{W} = \bar{\eps}_{12}' - \bar{\eps}_{12}''
  = \bar{\eps}_{1}' - \bar{\eps}_{1}'' + \bar{\eps}_{2}' - \bar{\eps}_{2}''.
\Tag{(468)}
\]
Now if $\bar{W}$~is negligible, we have
\[
\bar{\eps}_{1}'' - \bar{\eps}_{1}' = -(\bar{\eps}_{2}'' - \bar{\eps}_{2}')\Add{,}
\Tag{(469)}
\]
and \Eq{(467)}~shows that the ensemble which has the greater
modulus must lose energy. This result may be compared to
the thermodynamic principle, that when two bodies of different
temperatures are brought together, that which has the
higher temperature will lose energy.

Let us next suppose that the ensemble~$E_{2}$ is originally
canonically distributed with the modulus~$\Theta_{2}$, but leave the
distribution of the other arbitrary. We have, to determine
the result of a similar process,
\begin{gather*}
\bar{\eta}_{1}'' + \bar{\eta}_{2}'' \leq \bar{\eta}_{1}' + \bar{\eta}_{2}'\Add{,} \\
\bar{\eta}_{2}' + \frac{\bar{\eps}_{2}'}{\Theta_{2}}
  \leq \bar{\eta}_{2}'' + \frac{\bar{\eps}_{2}''}{\Theta_{2}}\Add{.}
\end{gather*}
\PageSep{161}
Hence
\[
\bar{\eta}_{1}'' + \frac{\bar{\eps}_{2}'}{\Theta_{2}}
  \leq \bar{\eta}_{1}' + \frac{\bar{\eps}_{2}''}{\Theta_{2}}\Add{,}
\Tag{(470)}
\]
which may be written
\[
\bar{\eta}_{1}' - \bar{\eta}_{1}'' \geq \frac{\bar{\eps}_{2}' - \bar{\eps}_{2}''}{\Theta_{2}}\Add{.}
\Tag{(471)}
\]
This may be compared with the thermodynamic principle that
when a body (which need not be in thermal equilibrium) is
brought into thermal contact with another of a given temperature,
the increase of entropy of the first cannot be less (algebraically)
than the loss of heat by the second divided by its
temperature. Where $\bar{W}$~is negligible, we may write
\[
\bar{\eta}_{1}'' + \frac{\bar{\eps}_{1}''}{\Theta_{2}}
  \leq \bar{\eta}_{1}' + \frac{\bar{\eps}_{1}'}{\Theta_{2}}\Add{.}
\Tag{(472)}
\]

Now, by Theorem~III of Chapter~XI, the quantity
\[
\bar{\eta}_{1} + \frac{\bar{\eps}_{1}}{\Theta_{2}}
\Tag{(473)}
\]
has a minimum value when the ensemble to which $\bar{\eta}_{1}$~and $\bar{\eps}_{1}$
relate is distributed canonically with the modulus~$\Theta_{2}$. If the
ensemble had originally this distribution, the sign~$<$ in~\Eq{(472)}
would be impossible. In fact, in this case, it would be easy to
show that the preceding formulae on which \Eq{(472)}~is founded
would all have the sign~$=$. But when the two ensembles are
not both originally distributed canonically with the same
modulus, the formulae indicate that the quantity~\Eq{(473)} may
be diminished by bringing the ensemble to which $\eps_{1}$~and $\eta_{1}$
relate into connection with another which is canonically distributed
with modulus~$\Theta_{2}$, and therefore, that by repeated
operations of this kind the ensemble of which the original distribution
was entirely arbitrary might be brought approximately
into a state of canonical distribution with the modulus~$\Theta_{2}$.
We may compare this with the thermodynamic principle
that a body of which the original thermal state may be entirely
arbitrary, may be brought approximately into a state of thermal
equilibrium with any given temperature by repeated connections
with other bodies of that temperature.
\PageSep{162}

Let us now suppose that we have a certain number of
ensembles, $E_{0}$, $E_{1}$, $E_{2}$, etc., distributed canonically with the
respective moduli $\Theta_{0}$, $\Theta_{1}$, $\Theta_{2}$, etc. By variation of the external
coördinates of the ensemble~$E_{0}$, let it be brought into
connection with~$E_{1}$, and then let the connection be broken.
Let it then be brought into connection with~$E_{2}$, and then let
that connection be broken. Let this process be continued
with respect to the remaining ensembles. We do not make
the assumption, as in some cases before, that the work connected
with the variation of the external coördinates is a negligible
quantity. On the contrary, we wish especially to consider
the case in which it is large. In the final state of the ensemble~$E_{0}$,
let us suppose that the external coördinates have been
brought back to their original values, and that the average
energy~($\bar{\eps}_{0}$) is the same as at first.

In our usual notations, using one and two accents to distinguish
original and final values, we get by repeated applications
of the principle expressed in~\Eq{(463)}
\[
\bar{\eta}_{0}' + \bar{\eta}_{1}' + \bar{\eta}_{2}' + \text{etc.}
  \geq \bar{\eta}_{0}'' + \bar{\eta}_{1}'' + \bar{\eta}_{2}'' + \text{etc.}
\Tag{(474)}
\]
But by Theorem~III of Chapter~XI,
\begin{gather*}
\bar{\eta}_{0}'' + \frac{\bar{\eps}_{0}''}{\Theta_{0}}
  \geq \bar{\eta}_{0}' + \frac{\bar{\eps}_{0}'}{\Theta_{0}},
\Tag{(475)}\displaybreak[0] \\
%
\bar{\eta}_{1}'' + \frac{\bar{\eps}_{1}''}{\Theta_{1}}
  \geq \bar{\eta}_{1}' + \frac{\bar{\eps}_{1}'}{\Theta_{1}},
\Tag{(476)}\displaybreak[0] \\
%
\bar{\eta}_{2}'' + \frac{\bar{\eps}_{2}''}{\Theta_{2}}
  \geq \bar{\eta}_{2}' + \frac{\bar{\eps}_{2}'}{\Theta_{2}}\Typo{,}{.}
\Tag{(477)}
\end{gather*}
Hence
\[
\frac{\bar{\eps}_{0}''}{\Theta_{0}}
  + \frac{\bar{\eps}_{1}''}{\Theta_{1}}
  + \frac{\bar{\eps}_{2}''}{\Theta_{2}} + \text{etc.}
  \geq \frac{\bar{\eps}_{0}'}{\Theta_{0}}
  + \frac{\bar{\eps}_{1}'}{\Theta_{1}}
  + \frac{\bar{\eps}_{2}'}{\Theta_{2}} + \text{etc.}\Add{,}
\Tag{(478)}
\]
or, since
\begin{gather*}
\bar{\eps}_{0}' = \bar{\eps}_{0}'', \\
0 \geq \frac{\bar{\eps}_{1}' - \bar{\eps}_{1}''}{\Theta_{1}}
  + \frac{\bar{\eps}_{2}' - \bar{\eps}_{2}''}{\Theta_{2}} + \text{etc.}
\Tag{(479)}
\end{gather*}
If we write~$\bar{W}$ for the average work done on the bodies represented
by the external coördinates, we have
\PageSep{163}
\[
\bar{\eps}_{1}' - \bar{\eps}_{1}'' + \bar{\eps}_{2}' - \bar{\eps}_{2}'' + \text{etc.} = \bar{W}.
\Tag{(480)}
\]
If $E_{0}$, $E_{1}$, and $E_{2}$ are the only ensembles, we have
\[
W \leq \frac{\Theta_{1} - \Theta_{2}}{\Theta_{1}} (\bar{\eps}_{1}' - \bar{\eps}_{1}'').
\Tag{(481)}
\]
It will be observed that the relations expressed in the last
three formulae between $\bar{W}$, $\bar{\eps}_{1}' - \bar{\eps}_{1}''$, $\bar{\eps}_{2}' - \bar{\eps}_{2}''$, etc., and $\Theta_{1}$,
$\Theta_{2}$, etc.\ are precisely those which hold in a Carnot's cycle for
the work obtained, the energy lost by the several bodies which
serve as heaters or coolers, and their initial temperatures.

It will not escape the reader's notice, that while from one
point of view the operations which are here described are quite
beyond our powers of actual performance, on account of the
impossibility of handling the immense number of systems
which are involved, yet from another point of view the operations
described are the most simple and accurate means of
representing what actually takes place in our simplest experiments
in thermodynamics. The states of the bodies which
we handle are certainly not known to us exactly. What we
know about a body can generally be described most accurately
and most simply by saying that it is one taken at random
from a great number (ensemble) of bodies which are completely
described. If we bring it into connection with another
body concerning which we have a similar limited knowledge,
the state of the two bodies is properly described as that of a
pair of bodies taken from a great number (ensemble) of pairs
which are formed by combining each body of the first ensemble
with each of the second.

Again, when we bring one body into thermal contact with
another, for example, in a Carnot's cycle, when we bring a
mass of fluid into thermal contact with some other body from
which we wish it to receive heat, we may do it by moving the
vessel containing the fluid. This motion is mathematically
expressed by the variation of the coördinates which determine
the position of the vessel. We allow ourselves for the purposes
of a theoretical discussion to suppose that the walls of
this vessel are incapable of absorbing heat from the fluid.
\PageSep{164}
Yet while we exclude the kind of action which we call thermal
between the fluid and the containing vessel, we allow the
kind which we call work in the narrower sense, which takes
place when the volume of the fluid is changed by the motion
of a piston. This agrees with what we have supposed in
regard to the external coördinates, which we may vary in
any arbitrary manner, and are in this entirely unlike the coördinates
of the second ensemble with which we bring the
first into connection.

When heat passes in any thermodynamic experiment between
the fluid principally considered and some other body, it is
actually absorbed and given out by the walls of the vessel,
which will retain a varying quantity. This is, however, a
disturbing circumstance, which we suppose in some way made
negligible, and actually neglect in a theoretical discussion.
In our case, we suppose the walls incapable of absorbing energy,
except through the motion of the external coördinates,
but that they allow the systems which they contain to act
directly on one another. Properties of this kind are mathematically
expressed by supposing that in the vicinity of a
certain surface, the position of which is determined by certain
(external) coördinates, particles belonging to the system in
question experience a repulsion from the surface increasing so
rapidly with nearness to the surface that an infinite expenditure
of energy would be required to carry them through it.
It is evident that two systems might be separated by a surface
or surfaces exerting the proper forces, and yet approach each
other closely enough to exert mechanical action on each other.
\PageSep{165}


\Chapter{XIV.}{Discussion of Thermodynamic Analogies.}

\First{If} we wish to find in rational mechanics an \textit{a~priori} foundation
for the principles of thermodynamics, we must seek
mechanical definitions of temperature and entropy. The
quantities thus defined must satisfy (under conditions and
with limitations which again must be specified in the language
of mechanics) the differential equation
\[
d\eps = T\, d\eta - A_{1}\, da_{1} - A_{2}\, da_{2} - \text{etc.},
\Tag{(482)}
\]
where $\eps$, $T$, and~$\eta$ denote the energy, temperature, and entropy
of the system considered, and $A_{1}\, da_{1}$, etc., the mechanical work
(in the narrower sense in which the term is used in thermodynamics,
\ie, with exclusion of thermal action) done upon
external bodies.

This implies that we are able to distinguish in mechanical
terms the thermal action of one system on another from that
which we call mechanical in the narrower sense, if not indeed
in every case in which the two may be combined, at least so as
to specify cases of thermal action and cases of mechanical
action.

Such a differential equation moreover implies a finite equation
between $\eps$,~$\eta$, and $a_{1}$, $a_{2}$, etc., which may be regarded
as fundamental in regard to those properties of the system
which we call thermodynamic, or which may be called so from
analogy. This fundamental thermodynamic equation is determined
by the fundamental mechanical equation which
expresses the energy of the system as function of its momenta
and coördinates with those external coördinates ($a_{1}$, $a_{2}$,
etc.)\ which appear in the differential expression of the work
done on external bodies. We have to show the mathematical
operations by which the fundamental thermodynamic equation,
\PageSep{166}
which in general is an equation of few variables, is derived
from the fundamental mechanical equation, which in the case
of the bodies of nature is one of an enormous number of
variables.

We have also to enunciate in mechanical terms, and to
prove, what we call the tendency of heat to pass from a system
of higher temperature to one of lower, and to show that
this tendency vanishes with respect to systems of the same
temperature.

At least, we have to show by \textit{a~priori} reasoning that for
such systems as the material bodies which nature presents to
us, these relations hold with such approximation that they
are sensibly true for human faculties of observation. This
indeed is all that is really necessary to establish the science of
thermodynamics on an \textit{a~priori} basis. Yet we will naturally
desire to find the exact expression of those principles of which
the laws of thermodynamics are the approximate expression.
A very little study of the statistical properties of conservative
systems of a finite number of degrees of freedom is sufficient
to make it appear, more or less distinctly, that the general
laws of thermodynamics are the limit toward which the exact
laws of such systems approximate, when their number of
degrees of freedom is indefinitely increased. And the problem
of finding the exact relations, as distinguished from the approximate,
for systems of a great number of degrees of freedom,
is practically the same as that of finding the relations
which hold for any number of degrees of freedom, as distinguished
from those which have been established on an empirical
basis for systems of a great number of degrees of
freedom.

The enunciation and proof of these exact laws, for systems
of any finite number of degrees of freedom, has been a principal
object of the preceding discussion. But it should be distinctly
stated that, if the results obtained when the numbers
of degrees of freedom are enormous coincide sensibly with
the general laws of thermodynamics, however interesting and
significant this coincidence may be, we are still far from
\PageSep{167}
having explained the phenomena of nature with respect to
these laws. For, as compared with the case of nature, the
systems which we have considered are of an ideal simplicity.
Although our only assumption is that we are considering
conservative systems of a finite number of degrees of freedom,
it would seem that this is assuming far too much, so far as the
bodies of nature are concerned. The phenomena of radiant
heat, which certainly should not be neglected in any complete
system of thermodynamics, and the electrical phenomena
associated with the combination of atoms, seem to show that
the hypothesis of systems of a finite number of degrees of
freedom is inadequate for the explanation of the properties of
bodies.

Nor do the results of such assumptions in every detail
appear to agree with experience. We should expect, for
example, that a diatomic gas, so far as it could be treated
independently of the phenomena of radiation, or of any sort of
electrical manifestations, would have six degrees of freedom
for each molecule. But the behavior of such a gas seems to
indicate not more than five.

But although these difficulties, long recognized by physicists,\footnote
  {See Boltzmann, Sitzb.\ der Wiener Akad., Bd.~LXIII, S.~418, (1871).}
seem to prevent, in the present state of science, any
satisfactory explanation of the phenomena of thermodynamics
as presented to us in nature, the ideal case of systems of a
finite number of degrees of freedom remains as a subject
which is certainly not devoid of a theoretical interest, and
which may serve to point the way to the solution of the far
more difficult problems presented to us by nature. And if
the study of the statistical properties of such systems gives
us an exact expression of laws which in the limiting case take
the form of the received laws of thermodynamics, its interest
is so much the greater.

Now we have defined what we have called the \emph{modulus}~($\Theta$)
of an ensemble of systems canonically distributed in phase,
and what we have called the index of probability~($\eta$) of any
phase in such an ensemble. It has been shown that between
\PageSep{168}
the modulus~($\Theta$), the external coördinates ($a_{1}$, etc.), and the
average values in the ensemble of the energy~($\eps$), the index
of probability~($\eta$), and the external forces ($A_{1}$, etc.)\ exerted
by the systems, the following differential equation will hold:
\[
d\bar{\eps} = -\Theta\, d\bar{\eta} - \Bar{A}_{1}\, da_{1} - \Bar{A}_{2}\, da_{2} - \text{etc.}
\Tag{(483)}
\]
This equation, if we neglect the sign of averages, is identical
in form with the thermodynamic equation~\Eq{(482)}, the modulus~($\Theta$)
corresponding to temperature, and the index of probability
of phase with its sign reversed corresponding to entropy.\footnote
  {See Chapter~IV, pages \Pageref{44}, \Pageref{45}.}

We have also shown that the average square of the anomalies
of~$\eps$, that is, of the deviations of the individual values from
the average, is in general of the same order of magnitude as
the reciprocal of the number of degrees of freedom, and therefore
to human observation the individual values are indistinguishable
from the average values when the number of degrees
of freedom is very great.\footnote
  {See Chapter~VII, pages \Pageref{73}--\Pageref{75}.}
In this case also the anomalies of~$\eta$
are practically insensible. The same is true of the anomalies of
the external forces ($A_{1}$, etc.), so far as these are the result of
the anomalies of energy, so that when these forces are sensibly
determined by the energy and the external coördinates, and
the number of degrees of freedom is very great, the anomalies
of these forces are insensible.

The mathematical operations by which the finite equation
between $\bar{\eps}$, $\bar{\eta}$, and $a_{1}$, etc., is deduced from that which gives
the energy~($\eps$) of a system in terms of the momenta ($p_{1}$\Add{,}~\dots $p_{n}$)
and coördinates both internal ($q_{1}$\Add{,}~\dots $q_{n}$) and external ($a_{1}$, etc.),
are indicated by the equation
\[
e^{-\efrac{\psi}{\Theta}} = \intap e^{-\efrac{\eps}{\Theta}}\, dq_{1} \dots dq_{n}\, dp_{1} \dots dp_{n},
\Tag{(484)}
\]
where
\[
\psi = \Theta \bar{\eta} + \bar{\eps}.
\]

We have also shown that when systems of different ensembles
are brought into conditions analogous to thermal contact,
the average result is a passage of energy from the ensemble
\PageSep{169}
of the greater modulus to that of the less,\footnote
  {See Chapter~XIII, page~\Pageref{160}.}
or in case of equal
moduli, that we have a condition of statistical equilibrium in
regard to the distribution of energy.\footnote
  {See Chapter~IV, pages \Pageref{35}--\Pageref{37}.}%

Propositions have also been demonstrated analogous to
those in thermodynamics relating to a Carnot's cycle,\footnote
  {See Chapter~XIII, pages \Pageref{162}, \Pageref{163}.}
or to
the tendency of entropy to increase,\footnote
  {See Chapter~XII, pages \Pageref{143}--\Pageref{151}.}
especially when bodies
of different temperature are brought into contact.\footnote
  {See Chapter~XIII, page~\Pageref{159}.}%

We have thus precisely defined quantities, and rigorously
demonstrated propositions, which hold for any number of
degrees of freedom, and which, when the number of degrees
of freedom~($n$) is enormously great, would appear to human
faculties as the quantities and propositions of empirical thermodynamics.

It is evident, however, that there may be more than one
quantity defined for finite values of~$n$, which approach the
same limit, when $n$~is increased indefinitely, and more than one
proposition relating to finite values of~$n$, which approach the
same limiting form for $n = \infty$. There may be therefore,
and there are, other quantities which may be thought to have
some claim to be regarded as temperature and entropy with
respect to systems of a finite number of degrees of freedom.

The definitions and propositions which we have been considering
relate essentially to what we have called a canonical
ensemble of systems. This may appear a less natural and
simple conception than what we have called a microcanonical
ensemble of systems, in which all have the same energy, and
which in many cases represents simply the \emph{time-ensemble}, or
ensemble of phases through which a single system passes in
the course of time.

It may therefore seem desirable to find definitions and
propositions relating to these microcanonical ensembles, which
shall correspond to what in thermodynamics are based on
experience. Now the differential equation
\[
d\eps = e^{-\phi} V\, d\log V - \Avg{A_{1}}\, da_{1} - \Avg{A_{2}}\, da_{2} - \text{etc.},
\Tag{(485)}
\]
\PageSep{170}
which has been demonstrated in Chapter~X, and which relates to
a microcanonical ensemble, $\Avg{A_{1}}$~denoting the average value of~$A_{1}$
in such an ensemble, corresponds precisely to the thermodynamic
equation, except for the sign of average applied to the
external forces. But as these forces are not entirely determined
by the energy with the external coördinates, the use of
average values is entirely germane to the subject, and affords
the readiest means of getting perfectly determined quantities.
These averages, which are taken for a microcanonical ensemble,
may seem from some points of view a more simple and natural
conception than those which relate to a canonical ensemble.
Moreover, the energy, and the quantity corresponding to entropy,
are free from the sign of average in this equation.

The quantity in the equation which corresponds to entropy
is~$\log V$, the quantity~$V$ being defined as the extension-in-phase
within which the energy is less than a certain limiting
value~($\eps$). This is certainly a more simple conception than the
average value in a canonical ensemble of the index of probability
of phase. $\operatorname{Log} V$~has the property that when it is constant
\[
d\eps = -\Avg{A_{1}}\, da_{1} - \Avg{A_{2}}\, da_{2} + \text{etc.},
\Tag{(486)}
\]
which closely corresponds to the thermodynamic property of
entropy, that when it is constant
\[
d\eps = -A_{1}\, da_{1} - A_{2}\, da_{2} + \text{etc.}
\Tag{(487)}
\]
The quantity in the equation which corresponds to temperature
is $e^{-\phi} V$, or $d\eps/d\log V$. In a canonical ensemble, the
average value of this quantity is equal to the modulus, as has
been shown by different methods in Chapters IX and~X\@.

In Chapter~X it has also been shown that if the systems
of a microcanonical ensemble consist of parts with separate
energies, the average value of~$e^{-\phi} V$ or any part is equal to its
average value for any other part, and to the uniform value
of the same expression for the whole ensemble. This corresponds
to the theorem in the theory of heat that in case of
thermal equilibrium the temperatures of the parts of a body
are equal to one another and to that of the whole body.
\PageSep{171}
Since the energies of the parts of a body cannot be supposed
to remain absolutely constant, even where this is the case
with respect to the whole body, it is evident that if we regard
the temperature as a function of the energy, the taking of
average or of probable values, or some other statistical process,
must be used with reference to the parts, in order to get a
perfectly definite value corresponding to the notion of temperature.

It is worthy of notice in this connection that the average
value of the kinetic energy, either in a microcanonical ensemble,
or in a canonical, divided by one half the number of
degrees of freedom, is equal to~$e^{-\phi} V$, or to its average value,
and that this is true not only of the whole system which is
distributed either microcanonically or canonically, but also
of any part, although the corresponding theorem relating to
temperature hardly belongs to empirical thermodynamics, since
neither the (inner) kinetic energy of a body, nor its number
of degrees of freedom is immediately cognizable to our faculties,
and we meet the gravest difficulties when we endeavor
to apply the theorem to the theory of gases, except in the
simplest case, that of the gases known as monatomic.

But the correspondence between $e^{-\phi} V$ or $d\eps/d\log V$ and
temperature is imperfect. If two isolated systems have such
energies that
\[
\frac{d\eps_{1}}{d\log V_{1}} = \frac{d\eps_{2}}{d\log V_{2}},
\]
and the two systems are regarded as combined to form a third
system with energy
\[
\eps_{12} = \eps_{1} + \eps_{2},
\]
we shall not have in general
\[
\frac{d\eps_{12}}{d\log V_{12}}
  = \frac{d\eps_{1}}{d\log V_{1}}
  = \frac{d\eps_{2}}{d\log V_{2}},
\]
as analogy with temperature would require. In fact, we have
seen that
\[
\frac{d\eps_{12}}{d\log V_{12}}
  = \Avg[\eps_{12}]{\frac{d\eps_{1}}{d\log V_{1}}}
  = \Avg[\eps_{12}]{\frac{d\eps_{2}}{d\log V_{2}}},
\]
\PageSep{172}
where the second and third members of the equation denote
average values in an ensemble in which the compound system
is microcanonically distributed in phase. Let us suppose the
two original systems to be identical in nature. Then
\[
\eps_{1} = \eps_{2} = \Avg[\eps_{12}]{\eps_{1}} = \Avg[\eps_{12}]{\eps_{2}}.
\]
The equation in question would require that
\[
\frac{d\eps_{1}}{d\log V_{1}} = \Avg[\eps_{12}]{\frac{d\eps_{1}}{d\log V_{1}}},
\]
\ie, that we get the same result, whether we take the value
of $d\eps_{1}/d\log V_{1}$ determined for the average value of~$\eps_{1}$ in the
ensemble, or take the average value of $d\eps_{1}/d\log V_{1}$. This
will be the case where $d\eps_{1}/d\log V_{1}$ is a linear function of~$\eps_{1}$.
Evidently this does not constitute the most general case.
Therefore the equation in question cannot be true in general.
It is true, however, in some very important particular cases, as
when the energy is a quadratic function of the $p$'s and~$q$'s, or
of the $p$'s alone.\footnote
  {This last case is important on account of its relation to the theory of
  gases, although it must in strictness be regarded as a limit of possible cases,
  rather than as a case which is itself possible.}
When the equation holds, the case is analogous
to that of bodies in thermodynamics for which the
specific heat for constant volume is constant.

Another quantity which is closely related to temperature is
$d\phi/d\eps$. It has been shown in Chapter~IX that in a canonical
ensemble, if $n > 2$, the average value of $d\phi/d\eps$ is~$1/\Theta$, and
that the most common value of the energy in the ensemble is
that for which $d\phi/d\eps = 1/\Theta$. The first of these properties
may be compared with that of $d\eps/d\log V$, which has been
seen to have the average value~$\Theta$ in a canonical ensemble,
without restriction in regard to the number of degrees of
freedom.

With respect to microcanonical ensembles also, $d\phi/d\eps$ has
a property similar to what has been mentioned with respect to
$d\eps/d\log V$. That is, if a system microcanonically distributed
in phase consists of two parts with separate energies, and each
\PageSep{173}
with more than two degrees of freedom, the average values in
the ensemble of~$d\phi/d\eps$ for the two parts are equal to one
another and to the value of same expression for the whole.
In our usual notations
\[
\Avg[\eps_{12}]{\frac{d\phi_{1}}{d\eps_{1}}}
  = \Avg[\eps_{12}]{\frac{d\phi_{2}}{d\eps_{2}}}
  = \frac{d\phi_{12}}{d\eps_{12}}
\]
if $n_{1} > 2$, and $n_{2} > 2$.

This analogy with temperature has the same incompleteness
which was noticed with respect to $d\eps/d\log V$, viz., if two systems
have such energies ($\eps_{1}$ and~$\eps_{2}$) that
\[
\frac{d\phi_{1}}{d\eps_{1}} = \frac{d\phi_{2}}{d\eps_{2}},
\]
and they are combined to form a third system with energy
\[
\eps_{12} = \eps_{1} + \eps_{2},
\]
we shall not have in general
\[
\frac{d\phi_{12}}{d\eps_{12}}
  = \frac{d\phi_{1}}{d\eps_{1}}
  = \frac{d\phi_{2}}{d\eps_{2}}.
\]
Thus, if the energy is a quadratic function of the $p$'s and~$q$'s,
we have\footnote
  {See foot-note on page~\Pageref{93}. We have here made the least value of the
  energy consistent with the values of the external coördinates zero instead
  of~$\eps_{\alpha}$, as is evidently allowable when the external coördinates are supposed
  invariable.}%
\begin{gather*}
\frac{d\phi_{1}}{d\eps_{1}} = \frac{n_{1} - 1}{\eps_{1}},\qquad
\frac{d\phi_{2}}{d\eps_{2}} = \frac{n_{2} - 1}{\eps_{2}}, \\
%
\frac{d\phi_{12}}{d\eps_{12}} = \frac{n_{12} - 1}{\eps_{12}}
   = \frac{n_{1} + n_{2} - 1}{\eps_{1} + \eps_{2}},
\end{gather*}
where $n_{1}$, $n_{2}$, $n_{12}$, are the numbers of degrees of freedom of the
separate and combined systems. But
\[
\frac{d\phi_{1}}{d\eps_{1}} = \frac{d\phi_{2}}{d\eps_{2}}
  = \frac{n_{1} + n_{2} - 2}{\eps_{1} + \eps_{2}}.
\]
If the energy is a quadratic function of the $p$'s alone, the case
would be the same except that we should have $\frac{1}{2} n_{1}$, $\frac{1}{2} n_{2}$, $\frac{1}{2} n_{12}$,
instead of $n_{1}$, $n_{2}$, $n_{12}$. In these particular cases, the analogy
\PageSep{174}
between $d\eps/d\log V$ and temperature would be complete, as has
already been remarked. We should have
\begin{gather*}
\frac{d\eps_{1}}{d\log V_{1}} = \frac{\eps_{1}}{n_{1}},\qquad
\frac{d\eps_{2}}{d\log V_{2}} = \frac{\eps_{2}}{n_{2}}, \\
%
\frac{d\eps_{12}}{d\log V_{12}} = \frac{\eps_{12}}{n_{12}}
  = \frac{d\eps_{1}}{d\log V_{1}}
  = \frac{d\eps_{2}}{d\log V_{2}},
\end{gather*}
when the energy is a quadratic function of the $p$'s and~$q$'s, and
similar equations with $\frac{1}{2} n_{1}$, $\frac{1}{2} n_{2}$, $\frac{1}{2} n_{12}$, instead of $n_{1}$, $n_{2}$, $n_{12}$,
when the energy is a quadratic function of the $p$'s alone.

More characteristic of $d\phi/d\eps$ are its properties relating to
most probable values of energy. If a system having two parts
with separate energies and each with more than two degrees
of freedom is microcanonically distributed in phase, the most
probable division of energy between the parts, in a system
taken at random from the ensemble, satisfies the equation
\[
\frac{d\phi_{1}}{d\eps_{1}} = \frac{d\phi_{2}}{d\eps_{2}},
\Tag{(488)}
\]
which corresponds to the thermodynamic theorem that the
distribution of energy between the parts of a system, in case of
thermal equilibrium, is such that the temperatures of the parts
are equal.

To prove the theorem, we observe that the fractional part
of the whole number of systems which have the energy of one
part~($\eps_{1}$) between the limits $\eps_{1}'$ and $\eps_{1}''$ is expressed by
\[
e^{-\phi_{12}} \int_{\eps_{1}'}^{\eps_{1}''} e^{\phi_{1} + \phi_{2}}\, d\eps_{1},
\]
where the variables are connected by the equation
\[
\eps_{1} + \eps_{2} = \constant = e_{12}.
\]
The greatest value of this expression, for a constant infinitesimal
value of the difference $\eps_{1}'' - \eps_{1}'$, determines a value of~$\eps_{1}$,
which we may call its most probable value. This depends on
the greatest possible value of $\phi_{1} + \phi_{2}$. Now if $n_{1} > 2$, and
$n_{2} > 2$, we shall have $\phi_{1} = -\infty$ for the least possible value of~$\eps_{1}$,
\PageSep{175}
and $\phi_{2} = -\infty$ for the least possible value of~$\eps_{2}$. Between
these limits $\phi_{1}$~and $\phi_{2}$ will be finite and continuous. Hence
$\phi_{1} + \phi_{2}$ will have a maximum satisfying the equation~\Eq{(488)}.

But if $n_{1} \leq 2$, or $n_{2} \leq 2$, $d\phi_{1}/d\eps_{1}$~or $d\phi_{2}/2\eps_{2}$ may be negative,
or zero, for all values of $\eps_{1}$ or~$\eps_{2}$, and can hardly be
regarded as having properties analogous to temperature.

It is also worthy of notice that if a system which is microcanonically
distributed in phase has three parts with separate
energies, and each with more than two degrees of freedom, the
most probable division of energy between these parts satisfies
the equation
\[
\frac{d\phi_{1}}{d\eps_{1}} = \frac{d\phi_{2}}{d\eps_{2}} = \frac{d\phi_{3}}{d\eps_{3}}.
\]
That is, this equation gives the most probable set of values
of $\eps_{1}$,~$\eps_{2}$, and~$\eps_{3}$. But it does not give the most probable
value of~$\eps_{1}$, or of~$\eps_{2}$, or of~$\eps_{3}$. Thus, if the energies are quadratic
functions of the $p$'s and~$q$'s, the most probable division
of energy is given by the equation
\[
\frac{n_{1} - 1}{\eps_{1}} = \frac{n_{2} - 1}{\eps_{2}} = \frac{n_{3} - 1}{\eps_{3}}.
\]
But the most probable value of~$\eps_{1}$ is given by
\[
\frac{n_{1} - 1}{\eps_{1}} = \frac{n_{2} + n_{3} - 1}{\eps_{2} + \eps_{3}},
\]
while the preceding equations give
\[
\frac{n_{1} - 1}{\eps_{1}} = \frac{n_{2} + n_{3} - 2}{\eps_{2} + \eps_{3}}.
\]

These distinctions vanish for very great values of $n_{1}$, $n_{2}$, $n_{3}$.
For small values of these numbers, they are important. Such
facts seem to indicate that the consideration of the most
probable division of energy among the parts of a system does
not afford a convenient foundation for the study of thermodynamic
analogies in the case of systems of a small number of
degrees of freedom. The fact that a certain division of energy
is the most probable has really no especial physical importance,
except when the ensemble of possible divisions are grouped so
\PageSep{176}
closely together that the most probable division may fairly
represent the whole. This is in general the case, to a very
close approximation, when $n$~is enormously great; it entirely
fails when $n$~is small.

If we regard $d\phi/d\eps$ as corresponding to the reciprocal of
temperature, or, in other words, $d\eps/d\phi$~as corresponding to
temperature, $\phi$~will correspond to entropy. It has been defined
as $\log(dV/d\eps)$. In the considerations on which its definition
is founded, it is therefore very similar to~$\log V$. We have
seen that $d\phi/d\log V$ approaches the value unity when $n$~is
very great.\footnote
  {See Chapter~X, pages \Pageref{120},~\Pageref{121}.}%

To form a differential equation on the model of the thermodynamic
equation~\Eq{(482)}, in which $d\eps/d\phi$~shall take the place
of temperature, and $\phi$~of entropy, we may write
\[
d\eps = \left(\frac{d\eps}{d\phi}\right)_{\alpha} d\phi
  + \left(\frac{d\eps}{da_{1}}\right)_{\phi,\alpha} da_{1}
  + \left(\frac{d\eps}{da_{2}}\right)_{\phi,\alpha} da_{2} + \text{etc.},
\Tag{(489)}
\]
or
\[
d\phi = \frac{d\phi}{d\eps}\, d\eps
  + \frac{d\phi}{da_{1}}\, da_{1}
  + \frac{d\phi}{da_{2}}\, da_{2} + \text{etc.}
\Tag{(490)}
\]
With respect to the differential coefficients in the last equation,
which corresponds exactly to~\Eq{(482)} solved with respect
to~$d\eta$, we have seen that their average values in a canonical
ensemble are equal to~$1/\Theta$, and the averages of $A_{1}/\Theta$, $A_{2}/\Theta$,
etc.\footnote
  {See Chapter~IX, equations \Eq{(321)}, \Eq{(327)}.}
We have also seen that $d\eps/d\phi$ (or $d\phi/d\eps$) has relations
to the most probable values of energy in parts of a microcanonical
ensemble. That $(d\eps/da_{1})_{\phi,\alpha}$, etc., have properties
somewhat analogous, may be shown as follows.

In a physical experiment, we measure a force by balancing it
against another. If we should ask what force applied to increase
or diminish~$a_{1}$ would balance the action of the systems,
it would be one which varies with the different systems. But
we may ask what single force will make a given value of~$a_{1}$
the most probable, and we shall find that under certain conditions
$(d\eps/da_{1})_{\phi,\alpha}$ represents that force.
\PageSep{177}

To make the problem definite, let us consider a system consisting
of the original system together with another having
the coördinates $a_{1}$, $a_{2}$, etc., and forces $A_{1}'$, $A_{2}'$, etc., tending
to increase those coördinates. These are in addition to the
forces $A_{1}$, $A_{2}$, etc., exerted by the original system, and are derived
from a force-function~($-\eps_{q}'$) by the equations
\[
A_{1}' = -\frac{d\eps_{q}'}{da_{1}},\quad
A_{2}' = -\frac{d\eps_{q}'}{da_{2}},\quad \text{etc.}
\]
For the energy of the whole system we may write
\[
E = \eps + \eps_{q}' + \tfrac{1}{2} m_{1} \dot{a}_{1}^{2} + \tfrac{1}{2} m_{2} \dot{a}_{2}^{2} + \text{etc.},
\]
and for the extension-in-phase of the whole system within any
limits
\[
\ints dp_{1} \dots dq_{n}\, da_{1}\, m_{1}\, d\dot{a}_{1}\, da_{2}\, m_{2}\, d\dot{a}_{2} \dots\Add{,}
\]
or
\[
\ints e^{\phi}\, d\eps\, da_{1}\, m_{1}\, d\dot{a}_{1}\, da_{2}\, m_{2}\, d\dot{a}_{2} \dots,
\]
or again
\[
\ints e^{\phi}\, dE\, da_{1}\, m_{1}\, d\dot{a}_{1}\, da_{2}\, m_{2}\, d\dot{a}_{2} \dots,
\]
since $d\eps = dE$, when $a_{1}$, $\dot{a}_{1}$, $a_{2}$, $\dot{a}_{2}$, etc., are constant. If the
limits are expressed by $E$~and $E + dE$, $a_{1}$~and $a_{1} + da_{1}$, $\dot{a}_{1}$~and
$\Typo{a_{1}}{\dot{a}_{1}} + d\dot{a}_{1}$, etc., the integral reduces to
\[
e^{\phi}\, dE\, da_{1}\, m_{1}\, d\dot{a}_{1}\, da_{2}\, m_{2}\, d\dot{a}_{2} \dots\Add{.}
\]
The values of $a_{1}$, $\dot{a}_{1}$, $a_{2}$, $\dot{a}_{2}$, etc., which make this expression
a maximum for constant values of the energy of the whole
system and of the differentials $dE$, $da_{1}$, $d\dot{a}_{1}$, etc., are what may
be called the most probable values of $a_{1}$, $\dot{a}_{1}$, etc., in an ensemble
in which the whole system is distributed microcanonically.

% [** Newline but no indent in the original]
To determine these values we have
\[
de^{\phi} = 0,
\]
when
\[
d(\eps + \eps_{q}' + \tfrac{1}{2} m_{1} \dot{a}_{1}^{2} + \tfrac{1}{2} m_{2} \dot{a}_{2}^{2} + \text{etc.}) = 0.
\]
That is,
\[
d\phi = 0,
\]
\PageSep{178}
when
\[
\left(\frac{d\eps}{d\phi}\right)_{\alpha} d\phi
  + \left(\frac{d\eps}{da_{1}}\right)_{\phi,\alpha} da_{1}
  - A_{1}'\, da_{1} + \text{etc.}
  + m_{1} \dot{a}_{1}\, d\dot{a}_{1} + \text{etc.}
  = 0.
\]
This requires
\[
\dot{a}_{1} = 0,\quad
\dot{a}_{2} = 0,\quad \text{etc.}
\]
and
\[
\left(\frac{d\eps}{da_{1}}\right)_{\phi,\alpha} = A_{1}',\quad
\left(\frac{d\eps}{da_{2}}\right)_{\phi,\alpha} = A_{2}',\quad \text{etc.}
\]
This shows that for any given values of $E$, $a_{1}$, $a_{2}$, etc.\
$\left(\dfrac{d\eps}{da_{1}}\right)_{\phi,\alpha}$, $\left(\dfrac{d\eps}{da_{2}}\right)_{\phi,\alpha}$, etc., represent the forces (in the generalized
sense) which the external bodies would have to exert
to make these values of $a_{1}$, $a_{2}$, etc., the most probable under
the conditions specified. When the differences of the external
forces which are exerted by the different systems are negligible,
$-(d\eps/da_{1})_{\phi,\alpha}$ etc., represent these forces.

It is certainly in the quantities relating to a canonical
ensemble, $\bar{\eps}$, $\Theta$, $\bar{\eta}$, $\bar{A}_{1}$, etc., $a_{1}$, etc., that we find the most
complete correspondence with the quantities of the thermodynamic
equation~\Eq{(482)}. Yet the conception itself of the canonical
ensemble may seem to some artificial, and hardly germane
to a natural exposition of the subject; and the quantities
$\eps$, $\dfrac{d\eps}{d\log V}$, $\log V$, $\Avg{A_{1}}$, etc., $a_{1}$, etc., or $\eps$, $\dfrac{d\eps}{d\phi}$, $\phi$, $\left(\dfrac{d\eps}{da_{1}}\right)_{\phi,\alpha}$,
etc., $a_{1}$, etc., which are closely related to ensembles of constant
energy, and to average and most probable values in such
ensembles, and most of which are defined without reference
to any ensemble, may appear the most natural analogues of
the thermodynamic quantities.

In regard to the naturalness of seeking analogies with the
thermodynamic behavior of bodies in canonical or microcanonical
ensembles of systems, much will depend upon how we
approach the subject, especially upon the question whether we
regard energy or temperature as an independent variable.

It is very natural to take energy for an independent variable
rather than temperature, because ordinary mechanics furnishes
us with a perfectly defined conception of energy, whereas the
idea of something relating to a mechanical system and corresponding
\PageSep{179}
to temperature is a notion but vaguely defined. Now
if the state of a system is given by its energy and the external
coördinates, it is incompletely defined, although its partial definition
is perfectly clear as far as it goes. The ensemble of
phases microcanonically distributed, with the given values of
the energy and the external coördinates, will represent the imperfectly
defined system better than any other ensemble or
single phase. When we approach the subject from this side,
our theorems will naturally relate to average values, or most
probable values, in such ensembles.

In this case, the choice between the variables of~\Eq{(485)} or of~\Eq{(489)}
will be determined partly by the relative importance
which is attached to average and probable values. It would
seem that in general average values are the most important, and
that they lend themselves better to analytical transformations.
This consideration would give the preference to the system of
variables in which $\log V$ is the analogue of entropy. Moreover,
if we make~$\phi$ the analogue of entropy, we are embarrassed by
the necessity of making numerous exceptions for systems of
one or two degrees of freedom.

On the other hand, the definition of~$\phi$ may be regarded as a
little more simple than that of~$\log V$, and if our choice is determined
by the simplicity of the definitions of the analogues of
entropy and temperature, it would seem that the $\phi$~system
should have the preference. In our definition of these quantities,
$V$~was defined first, and $e^{\phi}$~derived from~$V$ by differentiation.
This gives the relation of the quantities in the most
simple analytical form. Yet so far as the notions are concerned,
it is perhaps more natural to regard~$v$ as derived from~$e^{\phi}$
by integration. At all events, $e^{\phi}$~may be defined independently
of~$V$, and its definition may be regarded as more
simple as not requiring the determination of the zero from
which $V$~is measured, which sometimes involves questions
of a delicate nature. In fact, the quantity~$e^{\phi}$ may exist,
when the definition of~$V$ becomes illusory for practical purposes,
as the integral by which it is determined becomes infinite.

The case is entirely different, when we regard the temperature
\PageSep{180}
as an independent variable, and we have to consider a
system which is described as having a certain temperature and
certain values for the external coördinates. Here also the
state of the system is not completely defined, and will be
better represented by an ensemble of phases than by any single
phase. What is the nature of such an ensemble as will best
represent the imperfectly defined state?

When we wish to give a body a certain temperature, we
place it in a bath of the proper temperature, and when we
regard what we call thermal equilibrium as established, we say
that the body has the same temperature as the bath. Perhaps
we place a second body of standard character, which we
call a thermometer, in the bath, and say that the first body,
the bath, and the thermometer, have all the same temperature.

But the body under such circumstances, as well as the
bath, and the thermometer, even if they were entirely isolated
from external influences (which it is convenient to suppose
in a theoretical discussion), would be continually changing in
phase, and in energy as well as in other respects, although
our means of observation are not fine enough to perceive
these variations.

The series of phases through which the whole system runs
in the course of time may not be entirely determined by the
energy, but may depend on the initial phase in other respects.
In such cases the ensemble obtained by the microcanonical
distribution of the whole system, which includes all possible
time-ensembles combined in the proportion which seems least
arbitrary, will represent better than any one time-ensemble
the effect of the bath. Indeed a single time-ensemble, when
it is not also a microcanonical ensemble, is too ill-defined a
notion to serve the purposes of a general discussion. We
will therefore direct our attention, when we suppose the body
placed in a bath, to the microcanonical ensemble of phases
thus obtained.

If we now suppose the quantity of the substance forming
the bath to be increased, the anomalies of the separate energies
of the body and of the thermometer in the microcanonical
\PageSep{181}
ensemble will be increased, but not without limit. The anomalies
of the energy of the bath, considered in comparison with
its whole energy, diminish indefinitely as the quantity of the
bath is increased, and become in a sense negligible, when
the quantity of the bath is sufficiently increased. The
ensemble of phases of the body, and of the thermometer,
approach a standard form as the quantity of the bath is indefinitely
increased. This limiting form is easily shown to be
what we have described as the canonical distribution.

Let us write~$\eps$ for the energy of the whole system consisting
of the body first mentioned, the bath, and the thermometer
(if any), and let us first suppose this system to be distributed
canonically with the modulus~$\Theta$. We have by~\Eq{(205)}
\[
\Bar{(\eps - \bar{\eps})^{2}} = \Theta^{2} \frac{d\bar{\eps}}{d\Theta},
\]
and since
\begin{gather*}
\bar{\eps}_{p} = \frac{n}{2} \Theta, \\
\frac{d\bar{\eps}}{d\Theta} = \frac{n}{2}\, \frac{d\bar{\eps}}{d\bar{\eps}_{p}}.
\end{gather*}
If we write $\Delta\eps$ for the anomaly of mean square, we have
\[
(\Delta\eps)^{2} = \Bar{(\eps - \bar{\eps})^{2}}.
\]
If we set
\[
\Delta\Theta = \frac{d\Theta}{d\eps}\, d\eps,
\]
$\Delta\Theta$~will represent approximately the increase of~$\Theta$ which
would produce an increase in the average value of the energy
equal to its anomaly of mean square. Now these equations
give
\[
(\Delta\Theta)^{2} = \frac{2\Theta^{2}}{n}\, \frac{d\bar{\eps}_{p}}{d\bar{\eps}},
\]
which shows that we may diminish~$\Delta\Theta$ indefinitely by increasing
the quantity of the bath.

Now our canonical ensemble consists of an infinity of microcanonical
ensembles, which differ only in consequence of the
different values of the energy which is constant in each. If
we consider separately the phases of the first body which
\PageSep{182}
occur in the canonical ensemble of the whole system, these
phases will form a canonical ensemble of the same modulus.
This canonical ensemble of phases of the first body will consist
of parts which belong to the different microcanonical
ensembles into which the canonical ensemble of the whole
system is divided.

Let us now imagine that the modulus of the principal canonical
ensemble is increased by~$2\, \Delta\Theta$, and its average energy
by~$2\, \Delta\eps$. The modulus of the canonical ensemble of the
phases of the first body considered separately will be increased
by~$2\, \Delta\Theta$. We may regard the infinity of microcanonical ensembles
into which we have divided the principal canonical
ensemble as each having its energy increased by~$2\, \Delta\eps$. Let
us see how the ensembles of phases of the first body contained
in these microcanonical ensembles are affected. We
may assume that they will all be affected in about the same
way, as all the differences which come into account may be
treated as small. Therefore, the canonical ensemble formed by
taking them together will also be affected in the same way.
But we know how this is affected. It is by the increase of
its modulus by~$2\, \Delta\Theta$, a quantity which vanishes when the
quantity of the bath is indefinitely increased.

In the case of an infinite bath, therefore, the increase of the
energy of one of the microcanonical ensembles by~$2\, \Delta\eps$, produces
a vanishing effect on the distribution in energy of the
phases of the first body which it contains. But $2\, \Delta\eps$~is more
than the average difference of energy between the microcanonical
ensembles. The distribution in energy of these
phases is therefore the same in the different microcanonical
ensembles, and must therefore be canonical, like that of the
ensemble which they form when taken together.\footnote
  {In order to appreciate the above reasoning, it should be understood that
  the differences of energy which occur in the canonical ensemble of phases of
  the first body are not here regarded as vanishing quantities. To fix one's
  ideas, one may imagine that he has the fineness of perception to make these
  differences seem large. The difference between the part of these phases
  which belong to one microcanonical ensemble of the whole system and the
  part which belongs to another would still be imperceptible, when the quantity
  of the bath is sufficiently increased.}%
\PageSep{183}

As a general theorem, the conclusion may be expressed in
the words:---If a system of a great number of degrees of
freedom is microcanonically distributed in phase, any very
small part of it may be regarded as canonically distributed.\footnote
  {It is assumed---and without this assumption the theorem would have
  no distinct meaning---that the part of the ensemble considered may be
  regarded as having separate energy.}%

It would seem, therefore, that a canonical ensemble of
phases is what best represents, with the precision necessary
for exact mathematical reasoning, the notion of a body with
a given temperature, if we conceive of the temperature as the
state produced by such processes as we actually use in physics
to produce a given temperature. Since the anomalies of the
body increase with the quantity of the bath, we can only get
rid of all that is arbitrary in the ensemble of phases which is
to represent the notion of a body of a given temperature by
making the bath infinite, which brings us to the canonical
distribution.

A comparison of temperature and entropy with their analogues
in statistical mechanics would be incomplete without a
consideration of their differences with respect to units and
zeros, and the numbers used for their numerical specification.
If we apply the notions of statistical mechanics to such bodies
as we usually consider in thermodynamics, for which the
kinetic energy is of the same order of magnitude as the unit
of energy, but the number of degrees of freedom is enormous,
the values of $\Theta$, $d\eps/d\log V$, and $d\eps/d\phi$ will be of the same
order of magnitude as~$1/n$, and the variable part of $\bar{\eta}$, $\log V$,
and~$\phi$ will be of the same order of magnitude as~$n$.\footnote
  {See equations \Eq{(124)}, \Eq{(288)}, \Eq{(289)}, and~\Eq{(314)}; also page~\Pageref{106}.}
If these
quantities, therefore, represent in any sense the notions of temperature
and entropy, they will nevertheless not be measured
in units of the usual order of magnitude,---a fact which must
be borne in mind in determining what magnitudes may be
regarded as insensible to human observation.

Now nothing prevents our supposing energy and time in
our statistical formulae to be measured in such units as may
\PageSep{184}
be convenient for physical purposes. But when these units
have been chosen, the numerical values of $\Theta$, $d\eps/d\log V$,
$d\eps/d\phi$, $\bar{\eta}$, $\log V$, $\phi$, are entirely determined,\footnote
  {The unit of time only affects the last three quantities, and these only
  by an additive constant, which disappears (with the additive constant of
  entropy), when differences of entropy are compared with their statistical
  analogues. See page~\Pageref{19}.}
and in order to
compare them with temperature and entropy, the numerical
values of which depend upon an arbitrary unit, we must multiply
all values of $\Theta$, $d\eps/d\log V$, $d\eps/d\phi$ by a constant~($K$),
and divide all values of $\bar{\eta}$, $\log V$, and~$\phi$ by the same constant.
This constant is the same for all bodies, and depends only on
the units of temperature and energy which we employ. For
ordinary units it is of the same order of magnitude as the
numbers of atoms in ordinary bodies.

We are not able to determine the numerical value of~$K$,
as it depends on the number of molecules in the bodies with
which we experiment. To fix our ideas, however, we may
seek an expression for this value, based upon very probable
assumptions, which will show how we would naturally proceed
to its evaluation, if our powers of observation were fine
enough to take cognizance of individual molecules.

If the unit of mass of a monatomic gas contains $\nu$~atoms,
and it may be treated as a system of $3\nu$~degrees of freedom,
which seems to be the case, we have for canonical
distribution
\begin{gather*}
\bar{\eps}_{p} = \tfrac{3}{2} \nu\Theta, \\
\frac{d\bar{\eps}_{p}}{d\Theta} = \tfrac{3}{2} \nu.
\Tag{(491)}
\end{gather*}
If we write $T$ for temperature, and $c_{v}$~for the specific heat of
the gas for constant volume (or rather the limit toward
which this specific heat tends, as rarefaction is indefinitely
increased), we have
\[
\frac{d\eps_{p}}{dT} = c_{v},
\]
since we may regard the energy as entirely kinetic. We may
set the $\eps_{p}$ of this equation equal to the $\bar{\eps}_{p}$ of the preceding,
\PageSep{185}
where indeed the individual values of which the average is
taken would appear to human observation as identical. This
gives
\[
\frac{d\Theta}{dT} = \frac{2c_{v}}{3\nu},
\]
whence
\[
\frac{1}{K} = \frac{2c_{v}}{3\nu}.
\Tag{(493)}
\]
a value recognized by physicists as a constant independent of
the kind of monatomic gas considered.

We may also express the value of~$K$ in a somewhat different
form, which corresponds to the indirect method by which
physicists are accustomed to determine the quantity~$c_{v}$. The
kinetic energy due to the motions of the centers of mass of
the molecules of a mass of gas sufficiently expanded is easily
shown to be equal to
\[
\tfrac{3}{2}pv,
\]
where $p$~and $v$ denote the pressure and volume. The average
value of the same energy in a canonical ensemble of such
a mass of gas is
\[
\tfrac{3}{2}\Theta\nu,
\]
where $\nu$~denotes the number of molecules in the gas. Equating
these values, we have
\[
pv = \Theta\nu,
\Tag{(494)}
\]
whence
\[
\frac{1}{K} = \frac{\Theta}{T} = \frac{pv}{\nu T}.
\Tag{(495)}
\]
Now the laws of Boyle, Charles, and Avogadro may be expressed
by the equation
\[
pv = A\nu T,
\Tag{(496)}
\]
where $A$~is a constant depending only on the units in which
energy and temperature are measured. $1/K$,~therefore, might
be called the constant of the law of Boyle, Charles, and
Avogadro as expressed with reference to the true number of
molecules in a gaseous body.

Since such numbers are unknown to us, it is more convenient
to express the law with reference to relative values. If
we denote by~$M$ the so-called molecular weight of a gas, that
\PageSep{186}
is, a number taken from a table of numbers proportional to
the weights of various molecules and atoms, but having one
of the values, perhaps the atomic weight of hydrogen, arbitrarily
made unity, the law of Boyle, Charles, and Avogadro
may be written in the more practical form
\[
pv = A'T\frac{m}{M},
\Tag{(497)}
\]
where $A'$~is a constant and $m$~the weight of gas considered.
It is evident that $\Typo{1\ K}{1/K}$~is equal to the product of the constant
of the law in this form and the (true) weight of an atom of
hydrogen, or such other atom or molecule as may be given
the value unity in the table of molecular weights.

In the following chapter we shall consider the necessary
modifications in the theory of equilibrium, when the quantity
of matter contained in a system is to be regarded as variable,
or, if the system contains more than one kind of matter,
when the quantities of the several kinds of matter in the
system are to be regarded as independently variable. This
will give us yet another set of variables in the statistical
equation, corresponding to those of the amplified form of
the thermodynamic equation.
\PageSep{187}


\Chapter{XV.}{Systems Composed of Molecules.}

\First{The} nature of material bodies is such, that especial interest
attaches to the dynamics of systems composed of a great
number of entirely similar particles, or, it may be, of a great
number of particles of several kinds, all of each kind being
entirely similar to each other. We shall therefore proceed to
consider systems composed of such particles, whether in great
numbers or otherwise, and especially to consider the statistical
equilibrium of ensembles of such systems. One of the variations
to be considered in regard to such systems is a variation
in the numbers of the particles of the various kinds which it
contains, and the question of statistical equilibrium between
two ensembles of such systems relates in part to the tendencies
of the various kinds of particles to pass from the one to the
other.

First of all, we must define precisely what is meant by
statistical equilibrium of such an ensemble of systems. The
essence of statistical equilibrium is the permanence of the
number of systems which fall within any given limits with
respect to phase. We have therefore to define how the term
``phase'' is to be understood in such cases. If two phases differ
only in that certain entirely similar particles have changed
places with one another, are they to be regarded as identical
or different phases? If the particles are regarded as indistinguishable,
it seems in accordance with the spirit of the
statistical method to regard the phases as identical. In fact,
it might be urged that in such an ensemble of systems as we
are considering no identity is possible between the particles
of different systems except that of qualities, and if $\nu$~particles
of one system are described as entirely similar to one another
and to $\nu$~of another system, nothing remains on which to base
\PageSep{188}
the \Typo{indentification}{identification} of any particular particle of the first system
with any particular particle of the second. And this would
be true, if the ensemble of systems had a simultaneous
objective existence. But it hardly applies to the creations
of the imagination. In the cases which we have been considering,
and in those which we shall consider, it is not only
possible to conceive of the motion of an ensemble of similar
systems simply as possible cases of the motion of a single
system, but it is actually in large measure for the sake of
representing more clearly the possible cases of the motion of
a single system that we use the conception of an ensemble
of systems. The perfect similarity of several particles of a
system will not in the least interfere with the identification
of a particular particle in one case with a particular particle
in another. The question is one to be decided in accordance
with the requirements of practical convenience in the discussion
of the problems with which we are engaged.

Our present purpose will often require us to use the terms
\emph{phase}, \emph{density-in-phase}, \emph{statistical equilibrium}, and other connected
terms on the supposition that phases are \emph{not} altered
by the exchange of places between similar particles. Some
of the most important questions with which we are concerned
have reference to phases thus defined. We shall call them
phases determined by generic definitions, or briefly, generic
phases. But we shall also be obliged to discuss phases defined
by the narrower definition (so that exchange of position
between similar particles is regarded as changing the phase),
which will be called phases determined by specific definitions,
or briefly, specific phases. For the analytical description of
a specific phase is more simple than that of a generic phase.
And it is a more simple matter to make a multiple integral
extend over all possible specific phases than to make one extend
without repetition over all possible generic phases.

It is evident that if $\nu_{1}$, $\nu_{2}$\Add{,}~\dots $\nu_{h}$, are the numbers of the different
kinds of molecules in any system, the number of specific
phases embraced in one generic phase is represented by the
continued product $\Fact[]{\nu_{1}}\, \Fact[]{\nu_{2}} \cdots \Fact[]{\nu_{h}}$ and the coefficient of probability
\PageSep{189}
of a generic phase is the sum of the probability-coefficients
of the specific phases which it represents. When these are
equal among themselves, the probability-coefficient of the generic
phase is equal to that of the specific phase multiplied by
$\Fact[]{\nu_{1}}\, \Fact[]{\nu_{2}} \cdots \Fact[]{\nu_{h}}$. It is also evident that statistical equilibrium
may subsist with respect to generic phases without statistical
equilibrium with respect to specific phases, but not \textit{vice versa}.

Similar questions arise where one particle is capable of
several equivalent positions. Does the change from one of
these positions to another change the phase? It would be
most natural and logical to make it affect the specific phase,
but not the generic. The number of specific phases contained
in a generic phase would then be $\Fact[]{\nu_{1}} \kappa_{1}^{\nu_{1}} \cdots \Fact[]{\nu_{h}} \kappa_{h}^{\nu_{h}}$, where
$\kappa_{1}$,~\dots $\kappa_{h}$ denote the numbers of equivalent positions belonging
to the several kinds of particles. The case in which a~$\kappa$ is
infinite would then require especial attention. It does not
appear that the resulting complications in the formulae would
be compensated by any real advantage. The reason of this is
that in problems of real interest equivalent positions of a
particle will always be equally probable. In this respect,
equivalent positions of the same particle are entirely unlike
the $\Fact[]{\nu}$~different ways in which $\nu$~particles may be distributed
in $\nu$~different positions. Let it therefore be understood that
in spite of the physical equivalence of different positions of
the same particle they are to be considered as constituting a
difference of generic phase as well as of specific. The number
of specific phases contained in a generic phase is therefore
always given by the product $\Fact[]{\nu_{1}}\, \Fact[]{\nu_{2}} \cdots \Fact[]{\nu_{h}}$.

Instead of considering, as in the preceding chapters, ensembles
of systems differing only in phase, we shall now
suppose that the systems constituting an ensemble are composed
of particles of various kinds, and that they differ not
only in phase but also in the numbers of these particles which
they contain. The external coördinates of all the systems in
the ensemble are supposed, as heretofore, to have the same
value, and when they vary, to vary together. For distinction,
we may call such an ensemble a \emph{grand ensemble}, and one in
\PageSep{190}
which the systems differ only in phase a \emph{petit ensemble}. A
grand ensemble is therefore composed of a multitude of petit
ensembles. The ensembles which we have hitherto discussed
are petit ensembles.

Let $\nu_{1}$,~\dots $\nu_{h}$, etc., denote the numbers of the different
kinds of particles in a system, $\eps$~its energy, and $q_{1}$,~\dots $q_{n}$,
$p_{1}$,~\dots $p_{n}$ its coördinates and momenta. If the particles are of
the nature of material points, the number of coördinates~($n$)
of the system will be equal to $3\nu_{1} \Add{+} \cdots + 3\nu_{h}$. But if the particles
are less simple in their nature, if they are to be treated
as rigid solids, the orientation of which must be regarded, or
if they consist each of several atoms, so as to have more than
three degrees of freedom, the number of coördinates of the
system will be equal to the sum of $\nu_{1}$, $\nu_{2}$, etc., multiplied
each by the number of degrees of freedom of the kind of
particle to which it relates.

Let us consider an ensemble in which the number of
systems having $\nu_{1}$,~\dots $\nu_{h}$ particles of the several kinds, and
having values of their coördinates and momenta lying between
the limits $q_{1}$~and $q_{1} + dq_{1}$, $p_{1}$~and $p_{1} + dp_{1}$, etc., is represented
by the expression
\[
\frac{Ne^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n},
\Tag{(498)}
\]
where $N$, $\Omega$, $\mu_{1}$,~\dots $\mu_{h}$ are constants, $N$~denoting the total
number of systems in the ensemble. The expression
\[
\frac{Ne^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}
\Tag{(499)}
\]
evidently represents the density-in-phase of the ensemble
within the limits described, that is, for a phase specifically
defined. The expression
\[
\frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n},
\Tag{(500)}
\]
\PageSep{191}
is therefore the probability-coefficient for a phase specifically
defined. This has evidently the same value for all the
$\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}$ phases obtained by interchanging the phases of
particles of the same kind. The probability-coefficient for a
generic phase will be $\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}$ times as great, viz.,
\[
e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}.
\Tag{(501)}
\]

We shall say that such an ensemble as has been described
is \emph{canonically distributed}, and shall call the constant~$\Theta$ its
modulus. It is evidently what we have called a grand ensemble.
The petit ensembles of which it is composed are
canonically distributed, according to the definitions of Chapter~IV,
since the expression
\[
\frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h}}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}
\Tag{(502)}
\]
is constant for each petit ensemble. The grand ensemble,
therefore, is in statistical equilibrium with respect to specific
phases.

If an ensemble, whether grand or petit, is identical so far
as generic phases are concerned with one canonically distributed,
we shall say that its distribution is canonical with
respect to generic phases. Such an ensemble is evidently in
statistical equilibrium with respect to generic phases, although
it may not be so with respect to specific phases.

If we write~$\Eta$ for the index of probability of a generic phase
in a grand ensemble, we have for the case of canonical
distribution
\[
\Eta = \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}.
\Tag{(503)}
\]
It will be observed that the~$\Eta$ is a linear function of $\eps$~and
$\nu_{1}$,~\dots $\nu_{h}$; also that whenever the index of probability of
generic phases in a grand ensemble is a linear function of
$\eps$, $\nu_{1}$,~\dots $\nu_{h}$, the ensemble is canonically distributed with
respect to generic phases.
\PageSep{192}

The constant~$\Omega$ we may regard as determined by the
equation
\[
N = \sum_{\nu_{1}} \dots \sum_{\nu_{h}} \intap
  \frac{Ne^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n},
\Tag{(504)}
\]
or
\[
e^{-\efrac{\Omega}{\Theta}}
  = \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \frac{e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h}}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}
  \intap e^{-\efrac{\eps}{\Theta}}\, dp_{1} \dots dq_{n},
\Tag{(505)}
\]
where the multiple sum indicated by $\sum_{\nu_{1}} \dots \sum_{\nu_{h}}$ includes all
terms obtained by giving to each of the symbols $\nu_{1}$\Add{,}~\dots $\nu_{h}$ all
integral values from zero upward, and the multiple integral
(which is to be evaluated separately for each term of the
multiple sum) is to be extended over all the (specific) phases
of the system having the specified numbers of particles of the
various kinds. The multiple integral in the last equation is
what we have represented by~$e^{-\efrac{\psi}{\Theta}}$. See equation\Eq{(92)}. We
may therefore write
\[
e^{-\efrac{\Omega}{\Theta}}
  = \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \frac{e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}.
\Tag{(506)}
\]

It should be observed that the summation includes a term
in which all the symbols $\nu_{1}$\Add{,}~\dots $\nu_{h}$ have the value zero. We
must therefore recognize in a certain sense a system consisting
of no particles, which, although a barren subject of study in
itself, cannot well be excluded as a particular case of a system
of a variable number of particles. In this case $\eps$~is constant,
and there are no integrations to be performed. We have
therefore\footnote
  {This conclusion may appear a little strained. The original definition
  of~$\psi$ may not be regarded as fairly applying to systems of no degrees of
  freedom. We may therefore prefer to regard these equations as defining~$\psi$
  in this case.}%
\[
e^{-\efrac{\psi}{\Theta}} = e^{-\efrac{\eps}{\Theta}},\quad\text{\ie,}\quad \psi = \eps.
\]
\PageSep{193}
The value of~$\eps_{p}$ is of course zero in this case. But the
value of~$\eps_{q}$ contains an arbitrary constant, which is generally
determined by considerations of convenience, so that $\eps_{q}$~and $\eps$
do not necessarily vanish with $\nu_{1}$,~\dots $\nu_{h}$.

Unless $-\Omega$~has a finite value, our formulae become illusory.
We have already, in considering petit ensembles canonically
distributed, found it necessary to exclude cases in which $-\psi$~has
not a finite value.\footnote
  {See Chapter~IV, page~\Pageref{35}.}
The same exclusion would here
make $-\psi$ finite for any finite values of $\nu_{1}$\Add{,}~\dots $\nu_{h}$. This does
not necessarily make a multiple series of the form~\Eq{(506)} finite.
We may observe, however, that if for all values of $\nu_{1}$\Add{,}~\dots $\nu_{h}$
\[
-\psi \leq c_{0} + c_{1} \nu_{1}\Typo{,}\Add{+} \cdots + c_{h}\nu_{h},
\Tag{(507)}
\]
where $c_{0}$, $c_{1}$,~\dots $c_{h}$ are constants or functions of~$\Theta$,
\begin{gather*}
e^{-\efrac{\Omega}{\Theta}}
  \leq \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \frac{e^{\efrac{c_{0} + (\mu_{1} + c_{1})\nu_{1} \Add{+} \cdots + (\mu_{h} + c_{h})\nu_{h}}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\Add{,}
%
\intertext{\ie}
e^{-\efrac{\Omega}{\Theta}}
  \leq e^{\efrac{c_{0}}{\Theta}}
    \sum_{\nu_{1}} \frac{e^{\efrac{\mu_{1} + c_{1}}{\Theta}\nu_{1}}}{\Fact[]{\nu_{1}}} \cdots
    \sum_{\nu_{h}} \frac{e^{\efrac{\mu_{h} + c_{h}}{\Theta}\nu_{h}}}{\Fact[]{\nu_{h}}}\Add{,}
%
\intertext{\ie}
e^{-\efrac{\Omega}{\Theta}}
  \leq e^{\efrac{c_{0}}{\Theta}}
    e^{e^{\frac{\mu_{1} + c_{1}}{\Theta}\nu_{1}}} \cdots
    e^{e^{\frac{\mu_{h} + c_{h}}{\Theta}\nu_{h}}}\Add{,}
%
\intertext{\ie}
-\frac{\Omega}{\Theta}
  \leq \frac{c_{0}}{\Theta}
    + e^{\efrac{\mu_{1} + c_{1}}{\Theta}\nu_{1}} \Add{+} \cdots
    + e^{\efrac{\mu_{h} + c_{h}}{\Theta}\nu_{h}}.
\Tag{(508)}
\end{gather*}
The value of~$-\Omega$ will therefore be finite, when the condition~\Eq{(507)}
is satisfied. If therefore we assume that $-\Omega$~is finite,
we do not appear to exclude any cases which are analogous to
those of nature.\footnote
  {If the external coördinates determine a certain volume within which the
  system is confined, the contrary of~\Eq{(507)} would imply that we could obtain
  an infinite amount of work by crowding an infinite quantity of matter into a
  finite volume.}%

The interest of the ensemble which has been described lies
in the fact that it may be in statistical \Typo{equilbrium}{equilibrium}, both in
\PageSep{194}
respect to exchange of energy and exchange of particles, with
other grand ensembles canonically distributed and having the
same values of~$\Theta$ and of the coefficients $\mu_{1}$, $\mu_{2}$, etc., when the
circumstances are such that exchange of energy and of
particles are possible, and when equilibrium would not subsist,
were it not for equal values of these constants in the two
ensembles.

With respect to the exchange of energy, the case is exactly
the same as that of the petit ensembles considered in Chapter~IV,
and needs no especial discussion. The question of exchange
of particles is to a certain extent analogous, and may
be treated in a somewhat similar manner. Let us suppose
that we have two grand ensembles canonically distributed
with respect to specific phases, with the same value of the
modulus and of the coefficients $\mu_{1}$\Add{,}~\dots $\mu_{h}$, and let us consider
the ensemble of all the systems obtained by combining each
system of the first ensemble with each of the second.

The probability-coefficient of a generic phase in the first
ensemble may be expressed by
\[
e^{\efrac{\Omega' + \mu_{1}\nu_{1}' \Add{+} \cdots + \mu_{h}\nu_{h}' - \eps'}{\Theta}}\Add{.}
\Tag{(509)}
\]
The probability-coefficient of a specific phase will then be
expressed by
\[
\frac{e^{\efrac{\Omega' + \mu_{1}\nu_{1}' \Add{+} \cdots + \mu_{h}\nu_{h}' - \eps'}{\Theta}}}
     {\Fact[]{\nu_{1}'} \cdots \Fact[]{\nu_{h}'}},
\Tag{(510)}
\]
since each generic phase comprises $\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}$ specific phases.
In the second ensemble the probability-coefficients of the
generic and specific phases will be
\[
e^{\efrac{\Omega'' + \mu_{1}\nu_{1}'' \Add{+} \cdots + \mu_{h}\nu_{h}'' - \eps''}{\Theta}},
\Tag{(511)}
\]
and
\[
\frac{e^{\efrac{\Omega'' + \mu_{1}\nu_{1}'' \Add{+} \cdots + \mu_{h}\nu_{h}'' - \eps''}{\Theta}}}
     {\Fact[]{\nu_{1}''} \cdots \Fact[]{\nu_{h}''}}.
\Tag{(512)}
\]
\PageSep{195}

The probability-coefficient of a generic phase in the third
ensemble, which consists of systems obtained by regarding
each system of the first ensemble combined with each of the
second as forming a system, will be the product of the probability-coefficients
of the generic phases of the systems combined,
and will therefore be represented by the formula
\[
e^{\efrac{\Omega''' + \mu_{1}\nu_{1}''' \Add{+} \cdots + \mu_{h}\nu_{h}''' - \eps'''}{\Theta}}
\Tag{(513)}
\]
where $\Omega''' = \Omega' + \Omega''$, $\eps''' = \eps' + \eps''$, $\nu_{1}''' = \nu_{1}' + \nu_{1}''$, etc. It
will be observed that $\nu_{1}'''$, etc., represent the numbers of
particles of the various kinds in the third ensemble, and $\eps'''$~its
energy; also that $\Omega'''$~is a constant. The third ensemble
is therefore canonically distributed with respect to generic
phases.

If all the systems in the same generic phase in the third
ensemble were equably distributed among the $\Fact[]{\nu_{1}'''} \cdots \Fact[]{\nu_{h}'''}$ specific phases which are comprised in the generic phase, the probability-coefficient
of a specific phase would be
\[
\frac{e^{\efrac{\Omega''' + \mu_{1}\nu_{1}''' \Add{+} \cdots + \mu_{h}\nu_{h}''' - \eps'''}{\Theta}}}
     {\Fact[]{\nu_{1}'''} \cdots \Fact[]{\nu_{h}'''}}.
\Tag{(514)}
\]
In fact, however, the probability-coefficient of any specific
phase which occurs in the third ensemble is
\[
\frac{e^{\efrac{\Omega''' + \mu_{1}\nu_{1}''' \Add{+} \cdots + \mu_{h}\nu_{h}''' - \eps'''}{\Theta}}}
     {\Fact[]{\nu_{1}'} \cdots \Fact[]{\nu_{h}'}\, \Fact[]{\nu_{1}''} \cdots \Fact[]{\nu_{h}''}},
\Tag{(515)}
\]
which we get by multiplying the probability-coefficients of
specific phases in the first and second ensembles. The difference
between the formulae \Eq{(514)} and~\Eq{(515)} is due to the fact
that the generic phases to which \Eq{(513)}~relates include not
only the specific phases occurring in the third ensemble and
having the probability-coefficient \Eq{(515)}, but also all the
specifier phases obtained from these by interchange of similar
particles between two combined systems. Of these the probability-coefficient
\PageSep{196}
is evidently zero, as they do not occur in the
ensemble.

Now this third ensemble is in statistical equilibrium, with
respect both to specific and generic phases, since the ensembles
from which it is formed are so. This statistical equilibrium
is not dependent on the equality of the modulus and the \Chg{co-efficients}{coefficients}
$\mu_{1}$,~\dots $\mu_{h}$ in the first and second ensembles. It depends
only on the fact that the two original ensembles were separately
in statistical equilibrium, and that there is no interaction
between them, the combining of the two ensembles to form a
third being purely nominal, and involving no physical connection.
This independence of the systems, determined physically
by forces which prevent particles from passing from one system
to the other, or coming within range of each other's action,
is represented mathematically by infinite values of the energy
for particles in a space dividing the systems. Such a space
may be called a diaphragm.

If we now suppose that, when we combine the systems of
the two original ensembles, the forces are so modified that the
energy is no longer infinite for particles in all the space forming
the diaphragm, but is diminished in a part of this space,
so that it is possible for particles to pass from one system
to the other, this will involve a change in the function~$\eps'''$
which represents the energy of the combined systems, and the
equation $\eps''' = \eps' + \eps''$ will no longer hold. Now if the coefficient
of probability in the third ensemble were represented
by~\Eq{(513)} with this new function~$\eps'''$, we should have statistical
equilibrium, with respect to generic phases, although not to
specific. But this need involve only a trifling change in the
distribution of the third ensemble,\footnote
  {It will be observed that, so far as the distribution is concerned, very
  large and infinite values of~$\eps$ (for certain phases) amount to nearly the same
  thing,---one representing the total and the other the nearly total exclusion
  of the phases in question. An infinite change, therefore, in the value of~$\eps$
  (for certain phases) may represent a vanishing change in the distribution.}
a change represented by
the addition of comparatively few systems in which the transference
of particles is taking place to the immense number
\PageSep{197}
obtained by combining the two original ensembles. The
difference between the ensemble which would be in statistical
equilibrium, and that obtained by combining the two original
ensembles may be diminished without limit, while it is still
possible for particles to pass from one system to another. In
this sense we may say that the ensemble formed by combining
the two given ensembles may still be regarded as in a state of
(approximate) statistical equilibrium with respect to generic
phases, when it has been made possible for particles to pass
between the systems combined, and when statistical equilibrium
for specific phases has therefore entirely ceased to exist, and
when the equilibrium for generic phases would also have
entirely ceased to exist, if the given ensembles had not been
canonically distributed, with respect to generic phases, with
the same values of~$\Theta$ and $\mu_{1}$,~\dots $\mu_{h}$.

It is evident also that considerations of this kind will apply
separately to the several kinds of particles. We may diminish
the energy in the space forming the diaphragm for one kind of
particle and not for another. This is the mathematical expression
for a ``semipermeable'' diaphragm. The condition
necessary for statistical equilibrium where the diaphragm is
permeable only to particles to which the suffix~$(\ )_{1}$ relates
will be fulfilled when $\mu_{1}$~and $\Theta$ have the same values in the
two ensembles, although the other coefficients $\mu_{2}$, etc., may be
different.

This important property of grand ensembles with canonical
distribution will supply the motive for a more particular examination
of the nature of such ensembles, and especially of
the comparative numbers of systems in the several petit ensembles
which make up a grand ensemble, and of the average
values in the grand ensemble of some of the most important
quantities, and of the average squares of the deviations from
these average values.

The probability that a system taken at random from a
grand ensemble canonically distributed will have exactly
$\nu_{1}$,~\dots $\nu_{h}$ particles of the various kinds is expressed by the
multiple integral
\PageSep{198}
\[
\intap \frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n},
\Tag{(516)}
\]
or
\[
\frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}
\Tag{(517)}
\]
This may be called the probability of the petit ensemble
$(\nu_{1}, \dots \nu_{h})$. The sum of all such probabilities is evidently
unity. That is,
\[
\sum_{\nu_{1}} \dots \sum_{\nu_{h}}
\frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
     {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}} = 1,
\Tag{(518)}
\]
which agrees with~\Eq{(506)}.

The average value in the grand ensemble of any quantity~$u$,
is given by the formula
\[
\bar{u} = \sum_{\nu_{1}} \dots \sum_{\nu_{h}} \intap
  \frac{u\, e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n}.
\Tag{(519)}
\]
If $u$~is a function of $\nu_{1}$,~\dots $\nu_{h}$ alone, \ie, if it has the same
value in all systems of any same petit ensemble, the formula
reduces to
\[
\bar{u} = \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \frac{u\, e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}.
\Tag{(520)}
\]
Again, if we write $\Avg[\text{grand}]{u}$ and $\Avg[\text{petit}]{u}$ to distinguish averages in
the grand and petit ensembles, we shall have
\[
\Avg[\text{grand}]{u} = \sum_{\nu_{1}} \dots \sum_{\nu_{h}} \Avg[\text{petit}]{u}\,
  \frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}.
\Tag{(521)}
\]

In this chapter, in which we are treating of grand ensembles,
$\bar{u}$~will always denote the average for a grand ensemble.
In the preceding chapters, $\bar{u}$~has always denoted
the average for a petit ensemble.
\PageSep{199}

Equation~\Eq{(505)}, which we repeat in a slightly different
form, viz.,
\[
e^{-\efrac{\Omega}{\Theta}}
  = \sum_{\nu_{1}} \dots \sum_{\nu_{h}} \intap
  \frac{e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n},
\Tag{(522)}
\]
shows that $\Omega$~is a function of~$\Theta$ and $\mu_{1}$,~\dots $\mu_{h}$; also of the
external coördinates $a_{1}$, $a_{2}$, etc., which are involved implicitly
in~$\eps$. If we differentiate the equation regarding all these
quantities as variable, we have
\begin{multline*}
e^{-\efrac{\Omega}{\Theta}} \left(-\frac{d\Omega}{\Theta} + \frac{\Omega}{\Theta^{2}}\, d\Theta\right) \\
%
\begin{aligned}[b]
  &= -\frac{d\Theta}{\Theta} \sum_{\nu_{1}} \dots \sum_{\nu_{h}} \intap \\
  &\qquad\qquad
   \frac{(\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps)\, e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}
        {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n} \\
%
  &+ \frac{d\mu_{1}}{\Theta} \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \intap \frac{\nu_{1}\, e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}{\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n} \\
  &+ \text{etc.} \\
%
  &- \frac{da_{1}}{\Theta} \sum_{\nu_{1}} \dots \sum_{\nu_{h}}
  \intap \frac{d\eps}{da_{1}}\, \frac{e^{\efrac{\mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}}}{\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}}\, dp_{1} \dots dq_{n}\Typo{,}{} \\
  &- \text{etc.}
\end{aligned}
  \Tag{(523)}
\end{multline*}
If we multiply this equation by~$e^{\efrac{\Omega}{\Theta}}$, and set as usual $A_{1}$, $A_{2}$,
etc., for $-d\eps/da_{1}$, $-d\eps/da_{2}$, etc., we get in virtue of the law
expressed by equation~\Eq{(519)},
\begin{align*}
-\frac{d\Omega}{\Theta} + \frac{\Omega}{\Theta^{2}}
  &= -\frac{d\Theta}{\Theta^{2}}\, (\mu_{1}\bar{\nu}_{1} \Add{+} \cdots \mu_{h}\bar{\nu}_{h} - \bar{\eps}) \\
  &+ \frac{d\mu_{1}}{\Theta}\, \bar{\nu}_{1} + \frac{d\mu_{2}}{\Theta}\, \bar{\nu}_{2} + \text{etc.} \\
  &+ \frac{da_{1}}{\Theta}\, \bar{A}_{1} + \frac{da_{2}}{\Theta}\, \bar{A}_{2} + \text{etc.};
\Tag{(524)}
\end{align*}
\PageSep{200}
that is,
\[
d\Omega = \frac{\Omega + \mu_{1}\bar{\nu}_{1} \Add{+} \cdots \Add{+} \mu_{h}\bar{\nu}_{h} - \bar{\eps}}{\Theta}\, d\Theta
  - \sum \bar{\nu}_{1}\, d\mu_{1} - \sum \bar{A}_{1}\, da_{1}.
\Tag{(525)}
\]
Since equation~\Eq{(503)} gives
\[
\frac{\Omega + \mu_{1}\bar{\nu}_{1} \Add{+} \cdots \Add{+} \mu_{h}\bar{\nu}_{h} - \bar{\eps}}{\Theta} = \bar{\Eta},
\Tag{(526)}
\]
the preceding equation may be written
\[
d\Omega = \bar{\Eta}\, d\Theta - \sum \bar{\nu}_{1}\, d\mu_{1} - \sum \bar{A}_{1}\, da_{1}.
\Tag{(527)}
\]
Again, equation~\Eq{(526)} gives
\[
d\Omega + \sum \mu_{1}\, d\bar{\nu}_{1} + \sum \bar{\nu}_{1}\, d\mu_{1} - d\bar{\eps}
  = \Theta\, d\bar{\Eta} + \bar{\Eta}\, d\Theta.
\Tag{(528)}
\]
Eliminating~$d\Omega$ from these equations, we get
\[
d\bar{\eps} = -\Theta\, d\bar{\Eta} + \sum \mu_{1}\, d\bar{\nu}_{1} - \sum \bar{A}_{1}\, da_{1}.
\Tag{(529)}
\]
If we set
\begin{gather*}
\Psi = \bar{\eps} + \Theta \bar{\Eta},
\Tag{(530)} \\
d\Psi = d\eps + \Theta\, d\bar{\Eta} + \bar{\Eta}\, d\Theta,
\Tag{(531)}
\end{gather*}
we have
\[
d\Psi = \bar{\Eta}\, d\Theta + \sum \mu_{1}\, d\bar{\nu}_{1} - \sum \bar{A}_{1}\, da_{1}.
\Tag{(532)}
\]

The corresponding thermodynamic equations are
\begin{align*}
d\eps &= T\, d\eta + \sum \mu_{1}\, dm_{1} - \sum A_{1}\, da_{1},
\Tag{(533)} \\
\psi &= \eps - T\eta,
\Tag{(534)} \\
d\psi &= -\eta\, dT + \sum \mu_{1}\, dm_{1} - \sum A_{1}\, da_{1}.
\Tag{(535)}
\end{align*}
These are derived from the thermodynamic equations \Eq{(114)}
and \Eq{(117)} by the addition of the terms necessary to take account
of variation in the quantities ($m_{1}$, $m_{2}$, etc.)\ of the
several substances of which a body is composed. The correspondence
of the equations is most perfect when the component
substances are measured in such units that $m_{1}$, $m_{2}$,
etc., are proportional to the numbers of the different kinds
of molecules or atoms. The quantities $\mu_{1}$, $\mu_{2}$, etc., in these
thermodynamic equations may be defined as differential coefficients
by either of the equations in which they occur.\footnote
  {Compare Transactions Connecticut Academy, Vol.~III, pages~\Pageref{116}~ff.}%
\PageSep{201}

If we compare the statistical equations \Eq{(529)} and~\Eq{(532)}
with \Eq{(114)} and~\Eq{(112)}, which are given in Chapter~IV, and
discussed in Chapter~XIV, as analogues of thermodynamic
equations, we find considerable difference. Beside the
terms corresponding to the additional terms in the thermodynamic
equations of this chapter, and beside the fact that
the averages are taken in a grand ensemble in one case
and in a petit in the other, the analogues of entropy, $\Eta$
and~$\eta$, are quite different in definition and value. We shall
return to this point after we have determined the order
of magnitude of the usual anomalies of $\nu_{1}$,~\dots $\nu_{h}$.

If we differentiate equation~\Eq{(518)} with respect to~$\mu_{1}$, and
multiply by~$\Theta$, we get
\[
\sum_{\nu_{1}} \dots \sum_{\nu_{h}} \left(\frac{d\Omega}{d\mu_{1}} + \nu_{1}\right)
  \frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}} = 0,
\Tag{(536)}
\]
whence $d\Omega/d\mu_{1} = -\bar{\nu}_{1}$, which agrees with~\Eq{(527)}. Differentiating
again with respect to~$\mu_{1}$ and to~$\mu_{2}$, and setting
\[
\frac{d\Omega}{d\mu_{1}} = -\bar{\nu}_{1},\quad
\frac{d\Omega}{d\mu_{2}} = -\bar{\nu}_{2},
\]
we get
\begin{gather*}
\sum_{\nu_{1}} \dots \sum_{\nu_{h}} \left(\frac{d^{2}\Omega}{d\mu_{1}^{2}} + \frac{(\nu_{1} - \bar{\nu}_{1})^{2}}{\Theta}\right)
  \frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}} = 0,
\Tag{(537)}\displaybreak[0] \\
%
\sum_{\nu_{1}} \dots \sum_{\nu_{h}} \left(\frac{d^{2}\Omega}{d\mu_{1}\, d\mu_{2}} + \frac{(\nu_{1} - \bar{\nu}_{1})(\nu_{2} - \bar{\nu}_{2})}{\Theta}\right)
  \frac{e^{\efrac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}}}
       {\Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}} = 0.
\Tag{(538)}
\end{gather*}
The first members of these equations represent the average
values of the quantities in the principal parentheses. We
have therefore
\begin{gather*}
\Bar{(\nu_{1} - \bar{\nu}_{1})^{2}} = \Bar{\nu_{1}^{2}} - \bar{\nu}_{1}^{2}
  = -\Theta\, \frac{d^{2}\Omega}{d\mu_{1}^{2}}
  = \Theta\, \frac{d\bar{\nu}_{1}}{d\mu_{1}},
\Tag{(539)}\displaybreak[0] \\
%
\Bar{(\nu_{1} - \bar{\nu}_{1})(\nu_{2} - \bar{\nu}_{2})} = \Bar{\nu_{1} \nu_{2}} - \bar{\nu}_{1} \bar{\nu}_{2}
  = -\Theta\, \frac{d^{2}\Omega}{d\mu_{1}\, d\mu_{2}}
  = \Theta\, \frac{d\bar{\nu}_{1}}{d\mu_{2}}
  = \Theta\, \frac{d\bar{\nu}_{2}}{d\mu_{1}}.
\Tag{(540)}
\end{gather*}
\PageSep{202}

From equation~\Eq{(539)} we may get an idea of the order of
magnitude of the divergences of~$\nu_{1}$ from its average value
in the ensemble, when that average value is great. The
equation may be written
\[
\frac{(\nu_{1} - \bar{\nu}_{1})^{2}}{\bar{\nu}_{1}^{2}}
  = \frac{\Theta}{\bar{\nu}_{1}^{2}}\, \frac{d\bar{\nu}_{1}}{d\mu_{1}},
\Tag{(541)}
\]
The second member of this equation will in general be small
when $\bar{\nu}_{1}$~is great. Large values are not necessarily excluded,
but they must be confined within very small limits with respect
to~$\mu$. For if
\[
\frac{\Bar{(\nu_{1} - \bar{\nu}_{1})^{2}}}{\bar{\nu}_{1}^{2}}
  > \frac{1}{\bar{\nu}_{1}^{\efrac{1}{2}}},
\Tag{(542)}
\]
for all values of~$\mu_{1}$ between the limits $\mu_{1}'$ and~$\mu_{1}''$, we shall
have between the same limits
\[
\frac{\Theta}{\bar{\nu}_{1}^{\frac{3}{2}}}\, d\bar{\nu}_{1} > d\mu_{1},
\Tag{(543)}
\]
and therefore
\[
\tfrac{1}{2} \Theta \left(\frac{1}{\bar{\nu}_{1}'^{\efrac{1}{2}}} - \frac{1}{\bar{\nu}_{1}''^{\efrac{1}{2}}}\right) > \mu_{1}'' - \mu_{1}'.
\Tag{(544)}
\]
The difference $\mu_{1}'' - \mu_{1}'$ is therefore numerically a very small
quantity. To form an idea of the importance of such a
difference, we should observe that in formula~\Eq{(498)} $\mu_{1}$~is
multiplied by~$\nu_{1}$ and the product subtracted from the energy.
A very small difference in the value of~$\mu_{1}$ may therefore be important.
But since $\nu\Theta$~is always less than the kinetic energy
of the system, our formula shows that $\mu_{1}'' - \mu_{1}'$, even when
multiplied by $\bar{\nu}_{1}'$ or $\bar{\nu}_{1}''$, may still be regarded as an insensible
quantity.

We can now perceive the leading characteristics with respect
to properties sensible to human faculties of such an ensemble
as we are considering (a grand ensemble canonically
distributed), when the average numbers of particles of the various
kinds are of the same order of magnitude as the number
of molecules in the bodies which are the subject of physical
\PageSep{203}
experiment. Although the ensemble contains systems having
the widest possible variations in respect to the numbers of
the particles which they contain, these variations are practically
contained within such narrow limits as to be insensible,
except for particular values of the constants of the ensemble.
This exception corresponds precisely to the case of nature,
when certain thermodynamic quantities corresponding to $\Theta$,
$\mu_{1}$, $\mu_{2}$, etc., which in general determine the separate densities
of various components of a body, have certain values which
make these densities indeterminate, in other words, when the
conditions are such as determine coexistent phases of matter.
Except in the case of these particular values, the grand ensemble
would not differ to human faculties of perception from
a petit ensemble, viz., any one of the petit ensembles which it
contains in which $\bar{\nu}_{1}$, $\bar{\nu}_{2}$, etc., do not sensibly differ from their
average values.

Let us now compare the quantities $\Eta$ and~$\eta$, the average
values of which (in a grand and a petit ensemble respectively)
we have seen to correspond to entropy. Since
\begin{align*}
\Eta &= \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \eps}{\Theta}, \\
\intertext{and}
\eta &= \frac{\psi - \eps}{\Theta}, \\
\Eta - \eta &= \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta}.
\Tag{(545)}
\end{align*}
A part of this difference is due to the fact that $\Eta$~relates to
generic phases and $\eta$~to specific. If we write $\eta_{\text{gen}}$ for the
index of probability for generic phases in a petit ensemble,
we have
\begin{gather*}
\eta_{\text{gen}} = \eta + \log \Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}},
\Tag{(546)} \\
\Eta - \eta = \Eta - \eta_{\text{gen}} + \log \Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}},
\Tag{(547)} \\
\Eta - \eta_{\text{gen}} = \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi}{\Theta} - \log \Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}}.
\Tag{(548)}
\end{gather*}
This is the logarithm of the probability of the petit ensemble
$(\nu_{1}\Add{,} \dots \nu_{h})$.\footnote
  {See formula~\Eq{(517)}.}
If we set
\PageSep{204}
\[
\frac{\psi_{\text{gen}} - \eps}{\Theta} = \eta_{\text{gen}},
\Tag{(549)}
\]
which corresponds to the equation
\[
\frac{\psi - \eps}{\Theta} = \eta,
\Tag{(550)}
\]
we have
\[
\psi_{\text{gen}} = \psi + \Theta \log \Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}},
\]
and
\[
\Eta - \eta_{\text{gen}} = \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi_{\text{gen}}}{\Theta}.
\Tag{(551)}
\]
This will have a maximum when\footnote
  {Strictly speaking, $\psi_{\text{gen}}$~is not determined as function of $\nu_{1}$,~\dots $\nu_{h}$, except
  for integral values of these variables. Yet we may suppose it to be determined
  as a continuous function by any suitable process of interpolation.}%
\[
\frac{d\psi_{\text{gen}}}{d\nu_{1}} = \mu_{1},\quad
\frac{d\psi_{\text{gen}}}{d\nu_{2}} = \mu_{2},\quad \text{etc.}
\Tag{(552)}
\]

Distinguishing values corresponding to this maximum by
accents, we have approximately, when $\nu_{1}$,~\dots $\nu_{h}$ are of the
same order of magnitude as the numbers of molecules in ordinary
bodies,
\begin{multline*}
\begin{aligned}
\Eta - \eta_{\text{gen}}
  &= \frac{\Omega + \mu_{1}\nu_{1} \Add{+} \cdots + \mu_{h}\nu_{h} - \psi_{\text{gen}}}{\Theta} \\
  &= \frac{\Omega + \mu_{1}\nu_{1}' \Add{+} \cdots + \mu_{h}\nu_{h}' - \psi_{\text{gen}}'}{\Theta}
\end{aligned} \\
  - \left(\frac{d^{2} \psi_{\text{gen}}}{d\nu_{1}^{2}}\right)'\!\! \frac{(\Delta\nu_{1})^{2}}{2\Theta}
  - \left(\frac{d^{2} \psi_{\text{gen}}}{d\nu_{1}\, d\nu_{2}}\right)'\!\! \frac{\Delta\nu_{1}\, \Delta\nu_{2}}{\Theta} \Add{-} \cdots
  - \left(\frac{d^{2} \psi_{\text{gen}}}{d\nu_{h}^{2}}\right)'\!\! \frac{(\Delta\nu_{h})^{2}}{2\Theta},
\Tag{(553)}
\end{multline*}
\[
e^{\Eta - \eta_{\text{gen}}}
  = e^{C} e^{-\left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{1}^{2}}\right)' \efrac{(\Delta\nu_{1})^{2}}{2\Theta}
  - \left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{1}\, d\nu_{2}}\right)' \efrac{\Delta\nu_{1}\, \Delta\nu_{2}}{\Theta} \Add{-} \cdots
  - \left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{h}^{2}}\right)' \efrac{(\Delta\nu_{h})^{2}}{2\Theta}},
\Tag{(554)}
\]
where
\[
C = \frac{\Omega + \mu_{1}\nu_{1}' \Add{+} \cdots + \mu_{h}\nu_{h}' - \psi_{\text{gen}}'}{\Theta},
\Tag{(555)}
\]
and
\[
\Delta\nu_{1} = \nu_{1} - \nu_{1}',\quad
\Delta\nu_{2} = \nu_{2} - \nu_{2}',\quad \text{etc.}
\Tag{(556)}
\]
This is the probability of the system $(\nu_{1}\Add{,} \dots \nu_{h})$. The \Typo{probabilty}{probability}
that the values of $\nu_{1}$,~\dots $\nu_{h}$ lie within given limits is
given by the multiple integral
\PageSep{205}
{\footnotesize\begin{multline*}
\ints e^{C} e^{-\left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{1}^{2}}\right)'\!\! \efrac{(\Delta\nu_{1})^{2}}{2\Theta}
  - \left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{1}\, d\nu_{2}}\right)'\!\! \efrac{\Delta\nu_{1}\, \Delta\nu_{2}}{\Theta} \cdots
  - \left(\efrac{d^{2} \psi_{\text{gen}}}{d\nu_{h}^{2}}\right)'\!\! \efrac{(\Delta\nu_{h})^{2}}{2\Theta}}\!\! d\nu_{1} \dots d\nu_{h}.
\Tag{(557)}
\end{multline*}}

This shows that the distribution of the grand ensemble with
respect to the values of $\nu_{1}$,~\dots $\nu_{h}$ follows the ``law of errors''
when $\nu_{1}'$,~\dots $\nu_{h}'$ are very great. The value of this integral
for the limits~$\pm\infty$ should be unity. This gives
\[
e^{C}\, \frac{(2\pi\Theta)^{\efrac{h}{2}}}{D^{\efrac{1}{2}}} = 1,
\Tag{(558)}
\]
or
\[
C = \tfrac{1}{2} \log D - \frac{h}{2} \log(2\pi\Theta),
\Tag{(559)}
\]
where
\[
D = \left|\begin{array}{@{}cccc@{}}
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{1}^{2}}\right)' &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{1}\, d\nu_{2}}\right)' & \cdots &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{1}\, d\nu_{h}}\right)' \\
%
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{2}\, d\nu_{1}}\right)' &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{2}^{2}}\right)' & \cdots &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{2}\, d\nu_{h}}\right)' \\
%
%\cdots & \cdots & \cdots & \cdots \\
\cdots & \cdots & \cdots & \cdots \\
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{h}\, d\nu_{1}}\right)' &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{h}\, d\nu_{2}}\right)' & \cdots &
\left(\dfrac{d^{2}\psi_{\text{gen}}}{d\nu_{h}^{2}}\right)' \\
\end{array}\right|\Add{;}
\Tag{(560)}
\]
that is,
\[
D = \left|\begin{array}{@{}cccc@{}}
\left(\dfrac{d\mu_{1}}{d\nu_{1}}\right)' &
\left(\dfrac{d\mu_{1}}{d\nu_{2}}\right)' & \cdots &
\left(\dfrac{d\mu_{1}}{d\nu_{h}}\right)' \\
%
\left(\dfrac{d\mu_{2}}{d\nu_{1}}\right)' &
\left(\dfrac{d\mu_{2}}{d\nu_{2}}\right)' & \cdots &
\left(\dfrac{d\mu_{2}}{d\nu_{h}}\right)' \\
%\cdots & \cdots & \cdots & \cdots \\
\cdots & \cdots & \cdots & \cdots \\
%
\left(\dfrac{d\mu_{h}}{d\nu_{1}}\right)' &
\left(\dfrac{d\mu_{h}}{d\nu_{2}}\right)' & \cdots &
\left(\dfrac{d\mu_{h}}{d\nu_{h}}\right)' \\
\end{array}\right|\Add{.}
\Tag{(561)}
\]

Now, by~\Eq{(553)}, we have for the first approximation
\[
\bar{\Eta} - \bar{\eta}_{\text{gen}}
  = C - \tfrac{1}{2} \log D - \frac{h}{2} \log(2\pi\Theta),
\Tag{(562)}
\]
and if we divide by the constant~$K$,\footnote
  {See \Typo{page}{pages} \Pageref{184}--\Pageref{186}.}
to reduce these quantities
to the usual unit of entropy,
\[
\frac{\bar{\Eta} - \bar{\eta}_{\text{gen}}}{K}
  = \frac{\log D - h \log(2\pi\Theta)}{2K}\Add{.}
\Tag{(563)}
\]
\PageSep{206}
This is evidently a negligible quantity, since $K$~is of the same
order of magnitude as the number of molecules in ordinary
bodies. It is to be observed that $\bar{\eta}_{\text{gen}}$~is here the average in
the grand ensemble, whereas the quantity which we wish to
compare with~$\bar{\Eta}$ is the average in a petit ensemble. But as we
have seen that in the case considered the grand ensemble would
appear to human observation as a petit ensemble, this distinction
may be neglected.

The differences therefore, in the case considered, between the
quantities which may be represented by the notations\footnote
  {In this paragraph, for greater distinctness, $\Avg[\text{grand}]{\Eta_{\text{gen}}}$ and $\Avg[\text{petit}]{\eta_{\text{spec}}}$ have
  been written for the quantities which elsewhere are denoted by $\bar{\Eta}$ and~$\bar{\eta}$.}%
\[
\Avg[\text{grand}]{\Eta_{\text{gen}}},\quad
\Avg[\text{grand}]{\eta_{\text{gen}}},\quad
\Avg[\text{petit}]{\eta_{\text{gen}}}
\]
are not sensible to human faculties. The difference
\[
\Avg[\text{petit}]{\eta_{\text{gen}}} - \Avg[\text{petit}]{\eta_{\text{spec}}}
  = \Fact[]{\nu_{1}} \cdots \Fact[]{\nu_{h}},
\]
and is therefore constant, so long as the numbers $\nu_{1}$,~\dots $\nu_{h}$
are constant. For constant values of these numbers, therefore,
it is immaterial whether we use the average of~$\eta_{\text{gen}}$ or of~$\eta$ for
entropy, since this only affects the arbitrary constant of integration
which is added to entropy. But when the numbers
$\nu_{1}$,~\dots $\nu_{h}$ are varied, it is no longer possible to use the index
for specific phases. For the principle that the entropy of any
body has an arbitrary additive constant is subject to limitation,
when different quantities of the same substance are
concerned. In this case, the constant being determined for
one quantity of a substance, is thereby determined for all
quantities of the same substance.

To fix our ideas, let us suppose that we have two identical
fluid masses in contiguous chambers. The entropy of the
whole is equal to the sum of the entropies of the parts, and
double that of one part. Suppose a valve is now opened,
making a communication between the chambers. We do not
regard this as making any change in the entropy, although
the masses of gas or liquid diffuse into one another, and although
the same process of diffusion would increase the
\PageSep{207}
entropy, if the masses of fluid were different. It is evident,
therefore, that it is equilibrium with respect to generic phases,
and not with respect to specific, with which we have to do in
the evaluation of entropy, and therefore, that we must use
the average of~$\Eta$ or of~$\eta_{\text{gen}}$, and not that of~$\eta$, as the equivalent
of entropy, except in the thermodynamics of bodies in
which the number of molecules of the various kinds is
constant.

%%%%%%%%%%%%%%%%%%%%%%%%% GUTENBERG LICENSE %%%%%%%%%%%%%%%%%%%%%%%%%%
\PGLicense
\begin{PGtext}
End of the Project Gutenberg EBook of Elementary Principles of Statistical
Mechanics, by Josiah Willard Gibbs

*** END OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPLES STATISTICAL MECHANICS ***

***** This file should be named 50992-t.tex or 50992-t.zip *****
This and all associated files of various formats will be found in:
        http://www.gutenberg.org/5/0/9/9/50992/

Produced by Andrew D. Hwang
Updated editions will replace the previous one--the old editions will
be renamed.

Creating the works from print editions not protected by U.S. copyright
law means that no one owns a United States copyright in these works,
so the Foundation (and you!) can copy and distribute it in the United
States without permission and without paying copyright
royalties. Special rules, set forth in the General Terms of Use part
of this license, apply to copying and distributing Project
Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm
concept and trademark. Project Gutenberg is a registered trademark,
and may not be used if you charge for the eBooks, unless you receive
specific permission. If you do not charge anything for copies of this
eBook, complying with the rules is very easy. You may use this eBook
for nearly any purpose such as creation of derivative works, reports,
performances and research. They may be modified and printed and given
away--you may do practically ANYTHING in the United States with eBooks
not protected by U.S. copyright law. Redistribution is subject to the
trademark license, especially commercial redistribution.

START: FULL LICENSE

THE FULL PROJECT GUTENBERG LICENSE
PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK

To protect the Project Gutenberg-tm mission of promoting the free
distribution of electronic works, by using or distributing this work
(or any other work associated in any way with the phrase "Project
Gutenberg"), you agree to comply with all the terms of the Full
Project Gutenberg-tm License available with this file or online at
www.gutenberg.org/license.

Section 1. General Terms of Use and Redistributing Project
Gutenberg-tm electronic works

1.A. By reading or using any part of this Project Gutenberg-tm
electronic work, you indicate that you have read, understand, agree to
and accept all the terms of this license and intellectual property
(trademark/copyright) agreement. If you do not agree to abide by all
the terms of this agreement, you must cease using and return or
destroy all copies of Project Gutenberg-tm electronic works in your
possession. If you paid a fee for obtaining a copy of or access to a
Project Gutenberg-tm electronic work and you do not agree to be bound
by the terms of this agreement, you may obtain a refund from the
person or entity to whom you paid the fee as set forth in paragraph
1.E.8.

1.B. "Project Gutenberg" is a registered trademark. It may only be
used on or associated in any way with an electronic work by people who
agree to be bound by the terms of this agreement. There are a few
things that you can do with most Project Gutenberg-tm electronic works
even without complying with the full terms of this agreement. See
paragraph 1.C below. There are a lot of things you can do with Project
Gutenberg-tm electronic works if you follow the terms of this
agreement and help preserve free future access to Project Gutenberg-tm
electronic works. See paragraph 1.E below.

1.C. The Project Gutenberg Literary Archive Foundation ("the
Foundation" or PGLAF), owns a compilation copyright in the collection
of Project Gutenberg-tm electronic works. Nearly all the individual
works in the collection are in the public domain in the United
States. If an individual work is unprotected by copyright law in the
United States and you are located in the United States, we do not
claim a right to prevent you from copying, distributing, performing,
displaying or creating derivative works based on the work as long as
all references to Project Gutenberg are removed. Of course, we hope
that you will support the Project Gutenberg-tm mission of promoting
free access to electronic works by freely sharing Project Gutenberg-tm
works in compliance with the terms of this agreement for keeping the
Project Gutenberg-tm name associated with the work. You can easily
comply with the terms of this agreement by keeping this work in the
same format with its attached full Project Gutenberg-tm License when
you share it without charge with others.

1.D. The copyright laws of the place where you are located also govern
what you can do with this work. Copyright laws in most countries are
in a constant state of change. If you are outside the United States,
check the laws of your country in addition to the terms of this
agreement before downloading, copying, displaying, performing,
distributing or creating derivative works based on this work or any
other Project Gutenberg-tm work. The Foundation makes no
representations concerning the copyright status of any work in any
country outside the United States.

1.E. Unless you have removed all references to Project Gutenberg:

1.E.1. The following sentence, with active links to, or other
immediate access to, the full Project Gutenberg-tm License must appear
prominently whenever any copy of a Project Gutenberg-tm work (any work
on which the phrase "Project Gutenberg" appears, or with which the
phrase "Project Gutenberg" is associated) is accessed, displayed,
performed, viewed, copied or distributed:

  This eBook is for the use of anyone anywhere in the United States and
  most other parts of the world at no cost and with almost no
  restrictions whatsoever. You may copy it, give it away or re-use it
  under the terms of the Project Gutenberg License included with this
  eBook or online at www.gutenberg.org. If you are not located in the
  United States, you'll have to check the laws of the country where you
  are located before using this ebook.

1.E.2. If an individual Project Gutenberg-tm electronic work is
derived from texts not protected by U.S. copyright law (does not
contain a notice indicating that it is posted with permission of the
copyright holder), the work can be copied and distributed to anyone in
the United States without paying any fees or charges. If you are
redistributing or providing access to a work with the phrase "Project
Gutenberg" associated with or appearing on the work, you must comply
either with the requirements of paragraphs 1.E.1 through 1.E.7 or
obtain permission for the use of the work and the Project Gutenberg-tm
trademark as set forth in paragraphs 1.E.8 or 1.E.9.

1.E.3. If an individual Project Gutenberg-tm electronic work is posted
with the permission of the copyright holder, your use and distribution
must comply with both paragraphs 1.E.1 through 1.E.7 and any
additional terms imposed by the copyright holder. Additional terms
will be linked to the Project Gutenberg-tm License for all works
posted with the permission of the copyright holder found at the
beginning of this work.

1.E.4. Do not unlink or detach or remove the full Project Gutenberg-tm
License terms from this work, or any files containing a part of this
work or any other work associated with Project Gutenberg-tm.

1.E.5. Do not copy, display, perform, distribute or redistribute this
electronic work, or any part of this electronic work, without
prominently displaying the sentence set forth in paragraph 1.E.1 with
active links or immediate access to the full terms of the Project
Gutenberg-tm License.

1.E.6. You may convert to and distribute this work in any binary,
compressed, marked up, nonproprietary or proprietary form, including
any word processing or hypertext form. However, if you provide access
to or distribute copies of a Project Gutenberg-tm work in a format
other than "Plain Vanilla ASCII" or other format used in the official
version posted on the official Project Gutenberg-tm web site
(www.gutenberg.org), you must, at no additional cost, fee or expense
to the user, provide a copy, a means of exporting a copy, or a means
of obtaining a copy upon request, of the work in its original "Plain
Vanilla ASCII" or other form. Any alternate format must include the
full Project Gutenberg-tm License as specified in paragraph 1.E.1.

1.E.7. Do not charge a fee for access to, viewing, displaying,
performing, copying or distributing any Project Gutenberg-tm works
unless you comply with paragraph 1.E.8 or 1.E.9.

1.E.8. You may charge a reasonable fee for copies of or providing
access to or distributing Project Gutenberg-tm electronic works
provided that

* You pay a royalty fee of 20% of the gross profits you derive from
  the use of Project Gutenberg-tm works calculated using the method
  you already use to calculate your applicable taxes. The fee is owed
  to the owner of the Project Gutenberg-tm trademark, but he has
  agreed to donate royalties under this paragraph to the Project
  Gutenberg Literary Archive Foundation. Royalty payments must be paid
  within 60 days following each date on which you prepare (or are
  legally required to prepare) your periodic tax returns. Royalty
  payments should be clearly marked as such and sent to the Project
  Gutenberg Literary Archive Foundation at the address specified in
  Section 4, "Information about donations to the Project Gutenberg
  Literary Archive Foundation."

* You provide a full refund of any money paid by a user who notifies
  you in writing (or by e-mail) within 30 days of receipt that s/he
  does not agree to the terms of the full Project Gutenberg-tm
  License. You must require such a user to return or destroy all
  copies of the works possessed in a physical medium and discontinue
  all use of and all access to other copies of Project Gutenberg-tm
  works.

* You provide, in accordance with paragraph 1.F.3, a full refund of
  any money paid for a work or a replacement copy, if a defect in the
  electronic work is discovered and reported to you within 90 days of
  receipt of the work.

* You comply with all other terms of this agreement for free
  distribution of Project Gutenberg-tm works.

1.E.9. If you wish to charge a fee or distribute a Project
Gutenberg-tm electronic work or group of works on different terms than
are set forth in this agreement, you must obtain permission in writing
from both the Project Gutenberg Literary Archive Foundation and The
Project Gutenberg Trademark LLC, the owner of the Project Gutenberg-tm
trademark. Contact the Foundation as set forth in Section 3 below.

1.F.

1.F.1. Project Gutenberg volunteers and employees expend considerable
effort to identify, do copyright research on, transcribe and proofread
works not protected by U.S. copyright law in creating the Project
Gutenberg-tm collection. Despite these efforts, Project Gutenberg-tm
electronic works, and the medium on which they may be stored, may
contain "Defects," such as, but not limited to, incomplete, inaccurate
or corrupt data, transcription errors, a copyright or other
intellectual property infringement, a defective or damaged disk or
other medium, a computer virus, or computer codes that damage or
cannot be read by your equipment.

1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the "Right
of Replacement or Refund" described in paragraph 1.F.3, the Project
Gutenberg Literary Archive Foundation, the owner of the Project
Gutenberg-tm trademark, and any other party distributing a Project
Gutenberg-tm electronic work under this agreement, disclaim all
liability to you for damages, costs and expenses, including legal
fees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT
LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE
PROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE
TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE
LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR
INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH
DAMAGE.

1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a
defect in this electronic work within 90 days of receiving it, you can
receive a refund of the money (if any) you paid for it by sending a
written explanation to the person you received the work from. If you
received the work on a physical medium, you must return the medium
with your written explanation. The person or entity that provided you
with the defective work may elect to provide a replacement copy in
lieu of a refund. If you received the work electronically, the person
or entity providing it to you may choose to give you a second
opportunity to receive the work electronically in lieu of a refund. If
the second copy is also defective, you may demand a refund in writing
without further opportunities to fix the problem.

1.F.4. Except for the limited right of replacement or refund set forth
in paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO
OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.

1.F.5. Some states do not allow disclaimers of certain implied
warranties or the exclusion or limitation of certain types of
damages. If any disclaimer or limitation set forth in this agreement
violates the law of the state applicable to this agreement, the
agreement shall be interpreted to make the maximum disclaimer or
limitation permitted by the applicable state law. The invalidity or
unenforceability of any provision of this agreement shall not void the
remaining provisions.

1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the
trademark owner, any agent or employee of the Foundation, anyone
providing copies of Project Gutenberg-tm electronic works in
accordance with this agreement, and any volunteers associated with the
production, promotion and distribution of Project Gutenberg-tm
electronic works, harmless from all liability, costs and expenses,
including legal fees, that arise directly or indirectly from any of
the following which you do or cause to occur: (a) distribution of this
or any Project Gutenberg-tm work, (b) alteration, modification, or
additions or deletions to any Project Gutenberg-tm work, and (c) any
Defect you cause.

Section 2. Information about the Mission of Project Gutenberg-tm

Project Gutenberg-tm is synonymous with the free distribution of
electronic works in formats readable by the widest variety of
computers including obsolete, old, middle-aged and new computers. It
exists because of the efforts of hundreds of volunteers and donations
from people in all walks of life.

Volunteers and financial support to provide volunteers with the
assistance they need are critical to reaching Project Gutenberg-tm's
goals and ensuring that the Project Gutenberg-tm collection will
remain freely available for generations to come. In 2001, the Project
Gutenberg Literary Archive Foundation was created to provide a secure
and permanent future for Project Gutenberg-tm and future
generations. To learn more about the Project Gutenberg Literary
Archive Foundation and how your efforts and donations can help, see
Sections 3 and 4 and the Foundation information page at
www.gutenberg.org



Section 3. Information about the Project Gutenberg Literary Archive Foundation

The Project Gutenberg Literary Archive Foundation is a non profit
501(c)(3) educational corporation organized under the laws of the
state of Mississippi and granted tax exempt status by the Internal
Revenue Service. The Foundation's EIN or federal tax identification
number is 64-6221541. Contributions to the Project Gutenberg Literary
Archive Foundation are tax deductible to the full extent permitted by
U.S. federal laws and your state's laws.

The Foundation's principal office is in Fairbanks, Alaska, with the
mailing address: PO Box 750175, Fairbanks, AK 99775, but its
volunteers and employees are scattered throughout numerous
locations. Its business office is located at 809 North 1500 West, Salt
Lake City, UT 84116, (801) 596-1887. Email contact links and up to
date contact information can be found at the Foundation's web site and
official page at www.gutenberg.org/contact

For additional contact information:

    Dr. Gregory B. Newby
    Chief Executive and Director
    gbnewby@pglaf.org

Section 4. Information about Donations to the Project Gutenberg
Literary Archive Foundation

Project Gutenberg-tm depends upon and cannot survive without wide
spread public support and donations to carry out its mission of
increasing the number of public domain and licensed works that can be
freely distributed in machine readable form accessible by the widest
array of equipment including outdated equipment. Many small donations
($1 to $5,000) are particularly important to maintaining tax exempt
status with the IRS.

The Foundation is committed to complying with the laws regulating
charities and charitable donations in all 50 states of the United
States. Compliance requirements are not uniform and it takes a
considerable effort, much paperwork and many fees to meet and keep up
with these requirements. We do not solicit donations in locations
where we have not received written confirmation of compliance. To SEND
DONATIONS or determine the status of compliance for any particular
state visit www.gutenberg.org/donate

While we cannot and do not solicit contributions from states where we
have not met the solicitation requirements, we know of no prohibition
against accepting unsolicited donations from donors in such states who
approach us with offers to donate.

International donations are gratefully accepted, but we cannot make
any statements concerning tax treatment of donations received from
outside the United States. U.S. laws alone swamp our small staff.

Please check the Project Gutenberg Web pages for current donation
methods and addresses. Donations are accepted in a number of other
ways including checks, online payments and credit card donations. To
donate, please visit: www.gutenberg.org/donate

Section 5. General Information About Project Gutenberg-tm electronic works.

Professor Michael S. Hart was the originator of the Project
Gutenberg-tm concept of a library of electronic works that could be
freely shared with anyone. For forty years, he produced and
distributed Project Gutenberg-tm eBooks with only a loose network of
volunteer support.

Project Gutenberg-tm eBooks are often created from several printed
editions, all of which are confirmed as not protected by copyright in
the U.S. unless a copyright notice is included. Thus, we do not
necessarily keep eBooks in compliance with any particular paper
edition.

Most people start at our Web site which has the main PG search
facility: www.gutenberg.org

This Web site includes information about Project Gutenberg-tm,
including how to make donations to the Project Gutenberg Literary
Archive Foundation, how to help produce our new eBooks, and how to
subscribe to our email newsletter to hear about new eBooks.
\end{PGtext}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %
%                                                                         %
% End of the Project Gutenberg EBook of Elementary Principles of Statistical
% Mechanics, by Josiah Willard Gibbs                                      %
%                                                                         %
% *** END OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPLES STATISTICAL MECHANICS ***
%                                                                         %
% ***** This file should be named 50992-t.tex or 50992-t.zip *****        %
% This and all associated files of various formats will be found in:      %
%         http://www.gutenberg.org/5/0/9/9/50992/                         %
%                                                                         %
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %

\end{document}
###
@ControlwordReplace = (
  ['\\ie', 'i.e.'],
  ['\\dots', '... ']
  );

@ControlwordArguments = (
  ['\\Signature', 1, 1, '', ' ', 1, 1, '', ' ', 1, 1, '', ' ', 1, 1, '', ''],
  ['\\ToC', 1, 1, '', ' ', 1, 1, '', '', 1, 1, '', '', 1, 1, '', ''],
  ['\\ToCChap', 1, 1, 'Chapter ', ' ', 1, 1, '', ''],
  ['\\BookMark', 1, 0, '', '', 1, 0, '', ''],
  ['\\First', 1, 1, '', ''],
  ['\\Chapter', 0, 0, '', '', 1, 1, 'Chapter ', ' '],
  ['\\Pagelabel', 1, 0, '', ''],
  ['\\Pageref', 1, 1, '', ''],
  ['\\Eq', 0, 0, '', '', 1, 1, '', ''],
  ['\\Typo', 1, 0, '', '', 1, 1, '', ''],
  ['\\Add', 1, 1, '', ''],
  ['\\Chg', 1, 0, '', '', 1, 1, '', '']
  );
$PageSeparator = qr/^\\PageSep/;
$CustomClean = 'print "\\nCustom cleaning in progress...";
my $cline = 0;
 while ($cline <= $#file) {
   $file[$cline] =~ s/--------[^\n]*\n//; # strip page separators
   $cline++
 }
 print "done\\n";';
###
This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013/Debian) (format=pdflatex 2015.9.16)  21 JAN 2016 19:22
entering extended mode
 %&-line parsing enabled.
**50992-t.tex
(./50992-t.tex
LaTeX2e <2011/06/27>
Babel <3.9h> and hyphenation patterns for 78 languages loaded.
(/usr/share/texlive/texmf-dist/tex/latex/base/book.cls
Document Class: book 2007/10/19 v1.4h Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/bk12.clo
File: bk12.clo 2007/10/19 v1.4h Standard LaTeX file (size option)
)
\c@part=\count79
\c@chapter=\count80
\c@section=\count81
\c@subsection=\count82
\c@subsubsection=\count83
\c@paragraph=\count84
\c@subparagraph=\count85
\c@figure=\count86
\c@table=\count87
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
) (/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty
Package: inputenc 2008/03/30 v1.1d Input encoding file
\inpenc@prehook=\toks14
\inpenc@posthook=\toks15
(/usr/share/texlive/texmf-dist/tex/latex/base/latin1.def
File: latin1.def 2008/03/30 v1.1d Input encoding file
)) (/usr/share/texlive/texmf-dist/tex/latex/base/ifthen.sty
Package: ifthen 2001/05/26 v1.1c Standard LaTeX ifthen package (DPC)
) (/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
Package: amsmath 2013/01/14 v2.14 AMS math features
\@mathmargin=\skip43
For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
Package: amstext 2000/06/29 v2.01
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty
File: amsgen.sty 1999/11/30 v2.0
\@emptytoks=\toks16
\ex@=\dimen103
)) (/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty
Package: amsbsy 1999/11/29 v1.2d
\pmbraise@=\dimen104
) (/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty
Package: amsopn 1999/12/14 v2.01 operator names
)
\inf@bad=\count88
LaTeX Info: Redefining \frac on input line 210.
\uproot@=\count89
\leftroot@=\count90
LaTeX Info: Redefining \overline on input line 306.
\classnum@=\count91
\DOTSCASE@=\count92
LaTeX Info: Redefining \ldots on input line 378.
LaTeX Info: Redefining \dots on input line 381.
LaTeX Info: Redefining \cdots on input line 466.
\Mathstrutbox@=\box26
\strutbox@=\box27
\big@size=\dimen105
LaTeX Font Info:    Redeclaring font encoding OML on input line 566.
LaTeX Font Info:    Redeclaring font encoding OMS on input line 567.
\macc@depth=\count93
\c@MaxMatrixCols=\count94
\dotsspace@=\muskip10
\c@parentequation=\count95
\dspbrk@lvl=\count96
\tag@help=\toks17
\row@=\count97
\column@=\count98
\maxfields@=\count99
\andhelp@=\toks18
\eqnshift@=\dimen106
\alignsep@=\dimen107
\tagshift@=\dimen108
\tagwidth@=\dimen109
\totwidth@=\dimen110
\lineht@=\dimen111
\@envbody=\toks19
\multlinegap=\skip44
\multlinetaggap=\skip45
\mathdisplay@stack=\toks20
LaTeX Info: Redefining \[ on input line 2665.
LaTeX Info: Redefining \] on input line 2666.
) (/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
Package: amssymb 2013/01/14 v3.01 AMS font symbols
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty
Package: amsfonts 2013/01/14 v3.01 Basic AMSFonts support
\symAMSa=\mathgroup4
\symAMSb=\mathgroup5
LaTeX Font Info:    Overwriting math alphabet `\mathfrak' in version `bold'
(Font)                  U/euf/m/n --> U/euf/b/n on input line 106.
)) (/usr/share/texlive/texmf-dist/tex/latex/base/alltt.sty
Package: alltt 1997/06/16 v2.0g defines alltt environment
) (/usr/share/texlive/texmf-dist/tex/latex/footmisc/footmisc.sty
Package: footmisc 2011/06/06 v5.5b a miscellany of footnote facilities
\FN@temptoken=\toks21
\footnotemargin=\dimen112
\c@pp@next@reset=\count100
\c@@fnserial=\count101
Package footmisc Info: Declaring symbol style bringhurst on input line 855.
Package footmisc Info: Declaring symbol style chicago on input line 863.
Package footmisc Info: Declaring symbol style wiley on input line 872.
Package footmisc Info: Declaring symbol style lamport-robust on input line 883.

Package footmisc Info: Declaring symbol style lamport* on input line 903.
Package footmisc Info: Declaring symbol style lamport*-robust on input line 924
.
) (/usr/share/texlive/texmf-dist/tex/latex/tools/calc.sty
Package: calc 2007/08/22 v4.3 Infix arithmetic (KKT,FJ)
\calc@Acount=\count102
\calc@Bcount=\count103
\calc@Adimen=\dimen113
\calc@Bdimen=\dimen114
\calc@Askip=\skip46
\calc@Bskip=\skip47
LaTeX Info: Redefining \setlength on input line 76.
LaTeX Info: Redefining \addtolength on input line 77.
\calc@Ccount=\count104
\calc@Cskip=\skip48
) (/usr/share/texlive/texmf-dist/tex/latex/fancyhdr/fancyhdr.sty
\fancy@headwidth=\skip49
\f@ncyO@elh=\skip50
\f@ncyO@erh=\skip51
\f@ncyO@olh=\skip52
\f@ncyO@orh=\skip53
\f@ncyO@elf=\skip54
\f@ncyO@erf=\skip55
\f@ncyO@olf=\skip56
\f@ncyO@orf=\skip57
) (/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty
Package: geometry 2010/09/12 v5.6 Page Geometry
(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty
Package: keyval 1999/03/16 v1.13 key=value parser (DPC)
\KV@toks@=\toks22
) (/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty
Package: ifpdf 2011/01/30 v2.3 Provides the ifpdf switch (HO)
Package ifpdf Info: pdfTeX in PDF mode is detected.
) (/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty
Package: ifvtex 2010/03/01 v1.5 Detect VTeX and its facilities (HO)
Package ifvtex Info: VTeX not detected.
) (/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty
Package: ifxetex 2010/09/12 v0.6 Provides ifxetex conditional
)
\Gm@cnth=\count105
\Gm@cntv=\count106
\c@Gm@tempcnt=\count107
\Gm@bindingoffset=\dimen115
\Gm@wd@mp=\dimen116
\Gm@odd@mp=\dimen117
\Gm@even@mp=\dimen118
\Gm@layoutwidth=\dimen119
\Gm@layoutheight=\dimen120
\Gm@layouthoffset=\dimen121
\Gm@layoutvoffset=\dimen122
\Gm@dimlist=\toks23
) (/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty
Package: hyperref 2012/11/06 v6.83m Hypertext links for LaTeX
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty
Package: hobsub-hyperref 2012/05/28 v1.13 Bundle oberdiek, subset hyperref (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty
Package: hobsub-generic 2012/05/28 v1.13 Bundle oberdiek, subset generic (HO)
Package: hobsub 2012/05/28 v1.13 Construct package bundles (HO)
Package: infwarerr 2010/04/08 v1.3 Providing info/warning/error messages (HO)
Package: ltxcmds 2011/11/09 v1.22 LaTeX kernel commands for general use (HO)
Package: ifluatex 2010/03/01 v1.3 Provides the ifluatex switch (HO)
Package ifluatex Info: LuaTeX not detected.
Package hobsub Info: Skipping package `ifvtex' (already loaded).
Package: intcalc 2007/09/27 v1.1 Expandable calculations with integers (HO)
Package hobsub Info: Skipping package `ifpdf' (already loaded).
Package: etexcmds 2011/02/16 v1.5 Avoid name clashes with e-TeX commands (HO)
Package etexcmds Info: Could not find \expanded.
(etexcmds)             That can mean that you are not using pdfTeX 1.50 or
(etexcmds)             that some package has redefined \expanded.
(etexcmds)             In the latter case, load this package earlier.
Package: kvsetkeys 2012/04/25 v1.16 Key value parser (HO)
Package: kvdefinekeys 2011/04/07 v1.3 Define keys (HO)
Package: pdftexcmds 2011/11/29 v0.20 Utility functions of pdfTeX for LuaTeX (HO
)
Package pdftexcmds Info: LuaTeX not detected.
Package pdftexcmds Info: \pdf@primitive is available.
Package pdftexcmds Info: \pdf@ifprimitive is available.
Package pdftexcmds Info: \pdfdraftmode found.
Package: pdfescape 2011/11/25 v1.13 Implements pdfTeX's escape features (HO)
Package: bigintcalc 2012/04/08 v1.3 Expandable calculations on big integers (HO
)
Package: bitset 2011/01/30 v1.1 Handle bit-vector datatype (HO)
Package: uniquecounter 2011/01/30 v1.2 Provide unlimited unique counter (HO)
)
Package hobsub Info: Skipping package `hobsub' (already loaded).
Package: letltxmacro 2010/09/02 v1.4 Let assignment for LaTeX macros (HO)
Package: hopatch 2012/05/28 v1.2 Wrapper for package hooks (HO)
Package: xcolor-patch 2011/01/30 xcolor patch
Package: atveryend 2011/06/30 v1.8 Hooks at the very end of document (HO)
Package atveryend Info: \enddocument detected (standard20110627).
Package: atbegshi 2011/10/05 v1.16 At begin shipout hook (HO)
Package: refcount 2011/10/16 v3.4 Data extraction from label references (HO)
Package: hycolor 2011/01/30 v1.7 Color options for hyperref/bookmark (HO)
) (/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty
Package: auxhook 2011/03/04 v1.3 Hooks for auxiliary files (HO)
) (/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty
Package: kvoptions 2011/06/30 v3.11 Key value format for package options (HO)
)
\@linkdim=\dimen123
\Hy@linkcounter=\count108
\Hy@pagecounter=\count109
(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def
File: pd1enc.def 2012/11/06 v6.83m Hyperref: PDFDocEncoding definition (HO)
)
\Hy@SavedSpaceFactor=\count110
(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg
File: hyperref.cfg 2002/06/06 v1.2 hyperref configuration of TeXLive
)
Package hyperref Info: Option `hyperfootnotes' set `false' on input line 4319.
Package hyperref Info: Option `bookmarks' set `true' on input line 4319.
Package hyperref Info: Option `linktocpage' set `false' on input line 4319.
Package hyperref Info: Option `pdfdisplaydoctitle' set `true' on input line 431
9.
Package hyperref Info: Option `pdfpagelabels' set `true' on input line 4319.
Package hyperref Info: Option `bookmarksopen' set `true' on input line 4319.
Package hyperref Info: Option `colorlinks' set `true' on input line 4319.
Package hyperref Info: Hyper figures OFF on input line 4443.
Package hyperref Info: Link nesting OFF on input line 4448.
Package hyperref Info: Hyper index ON on input line 4451.
Package hyperref Info: Plain pages OFF on input line 4458.
Package hyperref Info: Backreferencing OFF on input line 4463.
Package hyperref Info: Implicit mode ON; LaTeX internals redefined.
Package hyperref Info: Bookmarks ON on input line 4688.
\c@Hy@tempcnt=\count111
(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty
\Urlmuskip=\muskip11
Package: url 2013/09/16  ver 3.4  Verb mode for urls, etc.
)
LaTeX Info: Redefining \url on input line 5041.
\XeTeXLinkMargin=\dimen124
\Fld@menulength=\count112
\Field@Width=\dimen125
\Fld@charsize=\dimen126
Package hyperref Info: Hyper figures OFF on input line 6295.
Package hyperref Info: Link nesting OFF on input line 6300.
Package hyperref Info: Hyper index ON on input line 6303.
Package hyperref Info: backreferencing OFF on input line 6310.
Package hyperref Info: Link coloring ON on input line 6313.
Package hyperref Info: Link coloring with OCG OFF on input line 6320.
Package hyperref Info: PDF/A mode OFF on input line 6325.
LaTeX Info: Redefining \ref on input line 6365.
LaTeX Info: Redefining \pageref on input line 6369.
\Hy@abspage=\count113
\c@Item=\count114
)

Package hyperref Message: Driver: hpdftex.

(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hpdftex.def
File: hpdftex.def 2012/11/06 v6.83m Hyperref driver for pdfTeX
\Fld@listcount=\count115
\c@bookmark@seq@number=\count116
(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/rerunfilecheck.sty
Package: rerunfilecheck 2011/04/15 v1.7 Rerun checks for auxiliary files (HO)
Package uniquecounter Info: New unique counter `rerunfilecheck' on input line 2
82.
)
\Hy@SectionHShift=\skip58
)
\TmpLen=\skip59
(./50992-t.aux)
\openout1 = `50992-t.aux'.

LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
LaTeX Font Info:    Checking defaults for PD1/pdf/m/n on input line 413.
LaTeX Font Info:    ... okay on input line 413.
*geometry* driver: auto-detecting
*geometry* detected driver: pdftex
*geometry* verbose mode - [ preamble ] result:
* driver: pdftex
* paper: <default>
* layout: <same size as paper>
* layoutoffset:(h,v)=(0.0pt,0.0pt)
* hratio: 1:1
* modes: includehead includefoot twoside 
* h-part:(L,W,R)=(9.03375pt, 343.28249pt, 9.03375pt)
* v-part:(T,H,B)=(0.54498pt, 504.52754pt, 0.81747pt)
* \paperwidth=361.34999pt
* \paperheight=505.89pt
* \textwidth=343.28249pt
* \textheight=442.65375pt
* \oddsidemargin=-63.23624pt
* \evensidemargin=-63.23624pt
* \topmargin=-71.725pt
* \headheight=12.0pt
* \headsep=19.8738pt
* \topskip=12.0pt
* \footskip=30.0pt
* \marginparwidth=98.0pt
* \marginparsep=7.0pt
* \columnsep=10.0pt
* \skip\footins=10.8pt plus 4.0pt minus 2.0pt
* \hoffset=0.0pt
* \voffset=0.0pt
* \mag=1000
* \@twocolumnfalse
* \@twosidetrue
* \@mparswitchtrue
* \@reversemarginfalse
* (1in=72.27pt=25.4mm, 1cm=28.453pt)

\AtBeginShipoutBox=\box28
(/usr/share/texlive/texmf-dist/tex/latex/graphics/color.sty
Package: color 2005/11/14 v1.0j Standard LaTeX Color (DPC)
(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/color.cfg
File: color.cfg 2007/01/18 v1.5 color configuration of teTeX/TeXLive
)
Package color Info: Driver file: pdftex.def on input line 130.
(/usr/share/texlive/texmf-dist/tex/latex/pdftex-def/pdftex.def
File: pdftex.def 2011/05/27 v0.06d Graphics/color for pdfTeX
\Gread@gobject=\count117
(/usr/share/texlive/texmf-dist/tex/context/base/supp-pdf.mkii
[Loading MPS to PDF converter (version 2006.09.02).]
\scratchcounter=\count118
\scratchdimen=\dimen127
\scratchbox=\box29
\nofMPsegments=\count119
\nofMParguments=\count120
\everyMPshowfont=\toks24
\MPscratchCnt=\count121
\MPscratchDim=\dimen128
\MPnumerator=\count122
\makeMPintoPDFobject=\count123
\everyMPtoPDFconversion=\toks25
)))
Package hyperref Info: Link coloring ON on input line 413.
(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty
Package: nameref 2012/10/27 v2.43 Cross-referencing by name of section
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/gettitlestring.sty
Package: gettitlestring 2010/12/03 v1.4 Cleanup title references (HO)
)
\c@section@level=\count124
)
LaTeX Info: Redefining \ref on input line 413.
LaTeX Info: Redefining \pageref on input line 413.
LaTeX Info: Redefining \nameref on input line 413.
(./50992-t.out) (./50992-t.out)
\@outlinefile=\write3
\openout3 = `50992-t.out'.


Overfull \hbox (34.97295pt too wide) in paragraph at lines 440--440
[]\OT1/cmtt/m/n/8 *** START OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPL
ES STATISTICAL MECHANICS ***[] 
 []

LaTeX Font Info:    Try loading font information for U+msa on input line 442.
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd
File: umsa.fd 2013/01/14 v3.01 AMS symbols A
)
LaTeX Font Info:    Try loading font information for U+msb on input line 442.
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd
File: umsb.fd 2013/01/14 v3.01 AMS symbols B
) [1

{/var/lib/texmf/fonts/map/pdftex/updmap/pdftex.map}] [2] [1


]
LaTeX Font Info:    Try loading font information for OMS+cmr on input line 495.

(/usr/share/texlive/texmf-dist/tex/latex/base/omscmr.fd
File: omscmr.fd 1999/05/25 v2.5h Standard LaTeX font definitions
)
LaTeX Font Info:    Font shape `OMS/cmr/m/n' in size <8> not available
(Font)              Font shape `OMS/cmsy/m/n' tried instead on input line 495.
[2

] [3

] [4] [5] [6] [7] [8] [9] [10

] [11] [12] [13] [14] [15] [16] [17] [18]
LaTeX Font Info:    Font shape `OMS/cmr/m/n' in size <7> not available
(Font)              Font shape `OMS/cmsy/m/n' tried instead on input line 1087.

[1



] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] 
[19] [20] [21] [22

] [23] [24] [25] [26] [27] [28] [29

] [30] [31] [32] [33] [34] [35] [36

] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [5
2

] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65

] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79

] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [9
5] [96]
Overfull \hbox (2.67761pt too wide) detected at line 4760
[] \OT1/cmr/m/n/12 = [] + 2[] + []\OML/cmm/m/it/12 :
 []

[97] [98] [99] [100] [101] [102

] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113]
Overfull \hbox (1.1806pt too wide) in paragraph at lines 5422--5425
\OT1/cmr/m/n/12 If the ap-prox-i-ma-tion is suf-fi-cient with-out go-ing be-yon
d the quadratic
 []

[114] [115] [116] [117] [118

] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131]
[132] [133] [134] [135] [136

] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149]
[150] [151] [152] [153

] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164

] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177]
[178

] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191]
[192] [193] [194

] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207]
[208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220

] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233]
[234] [235] [236] [237] [238] [239]
Overfull \hbox (3.75719pt too wide) detected at line 10239
 []$[] \OMS/cmsy/m/n/12 ^^@ [][]   [] ^^@ [][]   [] ^^@ [] ^^@ [][]   []\OML/cm
m/m/it/12 ; []$ []  
 []

[240]
Overfull \hbox (1.64584pt too wide) detected at line 10267
 []$[]   []   []   [] \OML/cmm/m/it/10 e[]e[]  d^^W[] [] d^^W[]:[]$ []  
 []

[241] [242] [243] [244]
Overfull \hbox (26.47282pt too wide) in paragraph at lines 10397--10397
[]\OT1/cmtt/m/n/8 *** END OF THIS PROJECT GUTENBERG EBOOK ELEMENTARY PRINCIPLES
 STATISTICAL MECHANICS ***[] 
 []

[1

] [2] [3] [4] [5] [6] [7] [8]
Package atveryend Info: Empty hook `BeforeClearDocument' on input line 10771.
[9]
Package atveryend Info: Empty hook `AfterLastShipout' on input line 10771.
(./50992-t.aux)
Package atveryend Info: Executing hook `AtVeryEndDocument' on input line 10771.


 *File List*
    book.cls    2007/10/19 v1.4h Standard LaTeX document class
    bk12.clo    2007/10/19 v1.4h Standard LaTeX file (size option)
inputenc.sty    2008/03/30 v1.1d Input encoding file
  latin1.def    2008/03/30 v1.1d Input encoding file
  ifthen.sty    2001/05/26 v1.1c Standard LaTeX ifthen package (DPC)
 amsmath.sty    2013/01/14 v2.14 AMS math features
 amstext.sty    2000/06/29 v2.01
  amsgen.sty    1999/11/30 v2.0
  amsbsy.sty    1999/11/29 v1.2d
  amsopn.sty    1999/12/14 v2.01 operator names
 amssymb.sty    2013/01/14 v3.01 AMS font symbols
amsfonts.sty    2013/01/14 v3.01 Basic AMSFonts support
   alltt.sty    1997/06/16 v2.0g defines alltt environment
footmisc.sty    2011/06/06 v5.5b a miscellany of footnote facilities
    calc.sty    2007/08/22 v4.3 Infix arithmetic (KKT,FJ)
fancyhdr.sty    
geometry.sty    2010/09/12 v5.6 Page Geometry
  keyval.sty    1999/03/16 v1.13 key=value parser (DPC)
   ifpdf.sty    2011/01/30 v2.3 Provides the ifpdf switch (HO)
  ifvtex.sty    2010/03/01 v1.5 Detect VTeX and its facilities (HO)
 ifxetex.sty    2010/09/12 v0.6 Provides ifxetex conditional
hyperref.sty    2012/11/06 v6.83m Hypertext links for LaTeX
hobsub-hyperref.sty    2012/05/28 v1.13 Bundle oberdiek, subset hyperref (HO)
hobsub-generic.sty    2012/05/28 v1.13 Bundle oberdiek, subset generic (HO)
  hobsub.sty    2012/05/28 v1.13 Construct package bundles (HO)
infwarerr.sty    2010/04/08 v1.3 Providing info/warning/error messages (HO)
 ltxcmds.sty    2011/11/09 v1.22 LaTeX kernel commands for general use (HO)
ifluatex.sty    2010/03/01 v1.3 Provides the ifluatex switch (HO)
 intcalc.sty    2007/09/27 v1.1 Expandable calculations with integers (HO)
etexcmds.sty    2011/02/16 v1.5 Avoid name clashes with e-TeX commands (HO)
kvsetkeys.sty    2012/04/25 v1.16 Key value parser (HO)
kvdefinekeys.sty    2011/04/07 v1.3 Define keys (HO)
pdftexcmds.sty    2011/11/29 v0.20 Utility functions of pdfTeX for LuaTeX (HO)
pdfescape.sty    2011/11/25 v1.13 Implements pdfTeX's escape features (HO)
bigintcalc.sty    2012/04/08 v1.3 Expandable calculations on big integers (HO)
  bitset.sty    2011/01/30 v1.1 Handle bit-vector datatype (HO)
uniquecounter.sty    2011/01/30 v1.2 Provide unlimited unique counter (HO)
letltxmacro.sty    2010/09/02 v1.4 Let assignment for LaTeX macros (HO)
 hopatch.sty    2012/05/28 v1.2 Wrapper for package hooks (HO)
xcolor-patch.sty    2011/01/30 xcolor patch
atveryend.sty    2011/06/30 v1.8 Hooks at the very end of document (HO)
atbegshi.sty    2011/10/05 v1.16 At begin shipout hook (HO)
refcount.sty    2011/10/16 v3.4 Data extraction from label references (HO)
 hycolor.sty    2011/01/30 v1.7 Color options for hyperref/bookmark (HO)
 auxhook.sty    2011/03/04 v1.3 Hooks for auxiliary files (HO)
kvoptions.sty    2011/06/30 v3.11 Key value format for package options (HO)
  pd1enc.def    2012/11/06 v6.83m Hyperref: PDFDocEncoding definition (HO)
hyperref.cfg    2002/06/06 v1.2 hyperref configuration of TeXLive
     url.sty    2013/09/16  ver 3.4  Verb mode for urls, etc.
 hpdftex.def    2012/11/06 v6.83m Hyperref driver for pdfTeX
rerunfilecheck.sty    2011/04/15 v1.7 Rerun checks for auxiliary files (HO)
   color.sty    2005/11/14 v1.0j Standard LaTeX Color (DPC)
   color.cfg    2007/01/18 v1.5 color configuration of teTeX/TeXLive
  pdftex.def    2011/05/27 v0.06d Graphics/color for pdfTeX
supp-pdf.mkii
 nameref.sty    2012/10/27 v2.43 Cross-referencing by name of section
gettitlestring.sty    2010/12/03 v1.4 Cleanup title references (HO)
 50992-t.out
 50992-t.out
    umsa.fd    2013/01/14 v3.01 AMS symbols A
    umsb.fd    2013/01/14 v3.01 AMS symbols B
  omscmr.fd    1999/05/25 v2.5h Standard LaTeX font definitions
 ***********

Package atveryend Info: Executing hook `AtEndAfterFileList' on input line 10771
.
Package rerunfilecheck Info: File `50992-t.out' has not changed.
(rerunfilecheck)             Checksum: C431FF98E60C65B7CED111EE828EFFC7;3271.
Package atveryend Info: Empty hook `AtVeryVeryEnd' on input line 10771.
 ) 
Here is how much of TeX's memory you used:
 9274 strings out of 493304
 123578 string characters out of 6139871
 275055 words of memory out of 5000000
 10937 multiletter control sequences out of 15000+600000
 16798 words of font info for 64 fonts, out of 8000000 for 9000
 957 hyphenation exceptions out of 8191
 28i,25n,44p,471b,483s stack positions out of 5000i,500n,10000p,200000b,80000s
</usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmcsc10.pfb></u
sr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmex10.pfb></usr/sha
re/texlive/texmf-dist/fonts/type1/public/amsfonts/cmextra/cmex7.pfb></usr/share
/texlive/texmf-dist/fonts/type1/public/amsfonts/cmextra/cmex8.pfb></usr/share/t
exlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmmi10.pfb></usr/share/texlive
/texmf-dist/fonts/type1/public/amsfonts/cm/cmmi12.pfb></usr/share/texlive/texmf
-dist/fonts/type1/public/amsfonts/cm/cmmi5.pfb></usr/share/texlive/texmf-dist/f
onts/type1/public/amsfonts/cm/cmmi6.pfb></usr/share/texlive/texmf-dist/fonts/ty
pe1/public/amsfonts/cm/cmmi7.pfb></usr/share/texlive/texmf-dist/fonts/type1/pub
lic/amsfonts/cm/cmmi8.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/ams
fonts/cm/cmr10.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/c
m/cmr12.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmr17
.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmr5.pfb></u
sr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmr6.pfb></usr/share
/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmr7.pfb></usr/share/texlive
/texmf-dist/fonts/type1/public/amsfonts/cm/cmr8.pfb></usr/share/texlive/texmf-d
ist/fonts/type1/public/amsfonts/cm/cmsy10.pfb></usr/share/texlive/texmf-dist/fo
nts/type1/public/amsfonts/cm/cmsy5.pfb></usr/share/texlive/texmf-dist/fonts/typ
e1/public/amsfonts/cm/cmsy6.pfb></usr/share/texlive/texmf-dist/fonts/type1/publ
ic/amsfonts/cm/cmsy7.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsf
onts/cm/cmsy8.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm
/cmti10.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmti1
2.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmti8.pfb><
/usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmtt10.pfb></usr/s
hare/texlive/texmf-dist/fonts/type1/public/amsfonts/cm/cmtt8.pfb>
Output written on 50992-t.pdf (273 pages, 975785 bytes).
PDF statistics:
 3832 PDF objects out of 4296 (max. 8388607)
 3324 compressed objects within 34 object streams
 1725 named destinations out of 1728 (max. 500000)
 177 words of extra memory for PDF output out of 10000 (max. 10000000)

